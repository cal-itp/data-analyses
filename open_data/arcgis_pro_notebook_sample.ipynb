{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import arcpy\n",
    "import json\n",
    "\n",
    "from arcpy import metadata as md\n",
    "S_NUMBER = \"s156485\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\s156485\\\\Documents\\\\ArcGIS'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "arcpy.env.workspace = os.path.join(\n",
    "    \"C:\\\\\", \"Users\", S_NUMBER,\n",
    "    \"Documents\", \"ArcGIS\"\n",
    ")\n",
    "working_dir = arcpy.env.workspace\n",
    "working_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'c:\\\\program files\\\\arcgis\\\\pro\\\\'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "directory = arcpy.GetInstallInfo(\"desktop\")[\"InstallDir\"] \n",
    "directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Set datasets to update...match to `update_vars.RUN_ME`\n",
    "in_features = [\n",
    "    'ca_hq_transit_areas',\n",
    "    'ca_hq_transit_stops',\n",
    "    'ca_transit_routes',\n",
    "    'ca_transit_stops',\n",
    "    'speeds_by_stop_segments',\n",
    "    'speeds_by_route_time_of_day'\n",
    "]\n",
    "\n",
    "staging_location = 'staging.gdb'\n",
    "out_location = 'open_data.gdb'\n",
    "\n",
    "def feature_class_in_gdb_path(my_gdb, file_name):\n",
    "    return os.path.join(my_gdb, file_name)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Unzip zipped shapefiles, download metadata.json into local path\n",
    "\n",
    "### Set FGDC field defs for each dataset and export XML (do once when new dataset added)\n",
    "\n",
    "Only the FGDC standard keeps fields.\n",
    "See if we can use this and combine it with our ISO 19139 standard later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in json with all the changes we need for each layer\n",
    "with open(f\"{working_dir}\\metadata.json\") as f:\n",
    "    meta_dict = json.load(f)\n",
    "\n",
    "    \n",
    "def update_metadata_class(this_feature_class, meta_dict_for_dataset: dict):\n",
    "    \"\"\"\n",
    "    Update the elements in the arcpy.metadata class.\n",
    "    \"\"\"\n",
    "    # Now update metadata class elements that are available\n",
    "    source_metadata = md.Metadata(this_feature_class)\n",
    "\n",
    "    source_metadata.title = meta_dict_for_dataset[\"dataset_name\"]\n",
    "    source_metadata.tags = meta_dict_for_dataset[\"theme_keywords\"]\n",
    "    source_metadata.summary = meta_dict_for_dataset[\"summary_purpose\"]\n",
    "    source_metadata.description = meta_dict_for_dataset[\"description\"]\n",
    "    source_metadata.accessConstraints = meta_dict_for_dataset[\"public_access\"]\n",
    "    source_metadata.save()\n",
    "    \n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def export_fgdc_metadata(one_feature_class):\n",
    "    \"\"\"\n",
    "    Export XML as FGDC format, \n",
    "    that's the only one that keeps field names and definitions\n",
    "    available.\n",
    "    \"\"\"\n",
    "    this_feature_class = feature_class_in_gdb_path(\n",
    "        staging_location, \n",
    "        one_feature_class\n",
    "    )\n",
    "    \n",
    "    subset_meta_dict = meta_dict[one_feature_class]\n",
    "\n",
    "    update_metadata_class(this_feature_class, subset_meta_dict)\n",
    "    \n",
    "    source_metadata = md.Metadata(this_feature_class)\n",
    "    \n",
    "    # Export metadata XML in FGDC   \n",
    "    meta_output = os.path.join(working_dir, \n",
    "                               f\"./{one_feature_class}_fgdc.xml\")\n",
    "            \n",
    "    TRANSLATOR = \"FGDC_CSDGM\"     \n",
    "\n",
    "    source_metadata.exportMetadata(\n",
    "        outputPath = meta_output, \n",
    "        metadata_export_option = TRANSLATOR\n",
    "    )\n",
    "    print(f\"Exported FGDC XML for {one_feature_class}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Do field data dictionary updates in Jupyter Hub\n",
    "### Use shapefile and write it to a file gdb layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Clean up last run (if applicable)\n",
    "for f in in_features:\n",
    "    feature_path = f\"{working_dir}\\{f}.xml\"\n",
    "    if os.path.exists(feature_path):\n",
    "        os.remove(feature_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div class='gpresult'><h2>Messages</h2><div id='messages' data-messages='[\"Start Time: Tuesday, May 27, 2025 11:30:23 AM\",\"Succeeded at Tuesday, May 27, 2025 11:30:23 AM (Elapsed Time: 0.09 seconds)\"]' data-show='true'><div id = 'default' /></div></div>"
      ],
      "text/plain": [
       "<Result 'C:\\\\Users\\\\s156485\\\\Documents\\\\ArcGIS\\\\staging.gdb'>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "arcpy.management.CreateFileGDB(working_dir, 'staging')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "staging.gdb\\ca_hq_transit_areas\n",
      "OBJECTID\n",
      "Shape\n",
      "agency_pri\n",
      "agency_sec\n",
      "hqta_type\n",
      "hqta_detai\n",
      "route_id\n",
      "base64_url\n",
      "base64_u_1\n",
      "org_id_pri\n",
      "org_id_sec\n",
      "mpo\n",
      "plan_name\n",
      "stop_id\n",
      "avg_trips_\n",
      "Shape_Length\n",
      "Shape_Area\n",
      "staging.gdb\\ca_hq_transit_stops\n",
      "OBJECTID\n",
      "Shape\n",
      "agency_pri\n",
      "hqta_type\n",
      "stop_id\n",
      "route_id\n",
      "hqta_detai\n",
      "agency_sec\n",
      "base64_url\n",
      "base64_u_1\n",
      "org_id_pri\n",
      "org_id_sec\n",
      "avg_trips_\n",
      "mpo\n",
      "plan_name\n",
      "staging.gdb\\ca_transit_routes\n",
      "OBJECTID\n",
      "Shape\n",
      "org_id\n",
      "agency\n",
      "route_id\n",
      "route_type\n",
      "route_name\n",
      "shape_id\n",
      "n_trips\n",
      "base64_url\n",
      "Shape_Length\n",
      "staging.gdb\\ca_transit_stops\n",
      "OBJECTID\n",
      "Shape\n",
      "org_id\n",
      "agency\n",
      "stop_id\n",
      "stop_name\n",
      "n_routes\n",
      "route_ids_\n",
      "routetypes\n",
      "n_arrivals\n",
      "n_hours_in\n",
      "meters_to_\n",
      "base64_url\n",
      "district_n\n",
      "staging.gdb\\speeds_by_stop_segments\n",
      "OBJECTID\n",
      "Shape\n",
      "route_id\n",
      "direction_\n",
      "stop_pair\n",
      "stop_pair_\n",
      "time_perio\n",
      "p50_mph\n",
      "n_trips\n",
      "p20_mph\n",
      "p80_mph\n",
      "district_n\n",
      "org_id\n",
      "agency\n",
      "base64_url\n",
      "Shape_Length\n",
      "staging.gdb\\speeds_by_route_time_of_day\n",
      "OBJECTID\n",
      "Shape\n",
      "route_id\n",
      "direction_\n",
      "time_perio\n",
      "speed_mph\n",
      "district_n\n",
      "org_id\n",
      "agency\n",
      "base64_url\n",
      "route_name\n",
      "Shape_Length\n"
     ]
    }
   ],
   "source": [
    "def shp_to_feature_class(file_name: str):\n",
    "    \"\"\"\n",
    "    From shapefile (directory of files), unpack those\n",
    "    and write it to our staging gdb as a feature class.\n",
    "    \"\"\"\n",
    "    # construct the filename, which is takes form of routes_assembled/routes_assembled.shp\n",
    "    shp_file_name = f\"{os.path.join(file_name, f'{file_name}.shp')}\"\n",
    "    \n",
    "    this_feature_class = os.path.join(staging_location, file_name)\n",
    "    \n",
    "    if arcpy.Exists(this_feature_class): \n",
    "        arcpy.management.Delete(this_feature_class)\n",
    "\n",
    "    # Execute FeatureClassToGeodatabase\n",
    "    arcpy.FeatureClassToGeodatabase_conversion(\n",
    "        shp_file_name, \n",
    "        staging_location\n",
    "    )\n",
    "    \n",
    "    # Print field names, just in case it needs renaming\n",
    "    # get a list of fields for each feature class\n",
    "    field_list = arcpy.ListFields(this_feature_class)  \n",
    "    \n",
    "    print(this_feature_class)\n",
    "    for field in field_list: \n",
    "        print(field.name)\n",
    "    \n",
    "    return\n",
    "\n",
    "\n",
    "for f in in_features:\n",
    "    shp_to_feature_class(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def rename_columns_with_dict(this_feature_class, rename_dict: dict):\n",
    "    \"\"\"\n",
    "    Get a list of fields for each feature class and use a dict to rename.\n",
    "    To change field names, must use AlterField_management, \n",
    "    because changing it in XML won't carry through when you sync\n",
    "    \"\"\"\n",
    "    field_list = arcpy.ListFields(this_feature_class)  \n",
    "\n",
    "    for field in field_list: \n",
    "        if field.name in rename_dict: \n",
    "            arcpy.AlterField_management(\n",
    "                this_feature_class, \n",
    "                field.name, \n",
    "                rename_dict[field.name], # new_field_name\n",
    "                rename_dict[field.name] # new_field_alias\n",
    "            ) \n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'agency_pri': 'agency_primary',\n",
       " 'agency_sec': 'agency_secondary',\n",
       " 'hqta_detai': 'hqta_details',\n",
       " 'base64_url': 'base64_url_primary',\n",
       " 'base64_u_1': 'base64_url_secondary',\n",
       " 'org_id_pri': 'org_id_primary',\n",
       " 'org_id_sec': 'org_id_secondary',\n",
       " 'avg_trips_': 'avg_trips_per_peak_hr'}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "meta_dict['ca_hq_transit_areas']['rename_cols']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "staging.gdb\\ca_hq_transit_areas\n",
      "OBJECTID\n",
      "Shape\n",
      "agency_primary\n",
      "agency_secondary\n",
      "hqta_type\n",
      "hqta_details\n",
      "route_id\n",
      "base64_url_primary\n",
      "base64_url_secondary\n",
      "org_id_primary\n",
      "org_id_secondary\n",
      "mpo\n",
      "plan_name\n",
      "stop_id\n",
      "avg_trips_per_peak_hr\n",
      "Shape_Length\n",
      "Shape_Area\n",
      "staging.gdb\\ca_hq_transit_stops\n",
      "OBJECTID\n",
      "Shape\n",
      "agency_primary\n",
      "hqta_type\n",
      "stop_id\n",
      "route_id\n",
      "hqta_details\n",
      "agency_secondary\n",
      "base64_url_primary\n",
      "base64_url_secondary\n",
      "org_id_primary\n",
      "org_id_secondary\n",
      "avg_trips_per_peak_hr\n",
      "mpo\n",
      "plan_name\n",
      "staging.gdb\\ca_transit_routes\n",
      "OBJECTID\n",
      "Shape\n",
      "org_id\n",
      "agency\n",
      "route_id\n",
      "route_type\n",
      "route_name\n",
      "shape_id\n",
      "n_trips\n",
      "base64_url\n",
      "Shape_Length\n",
      "staging.gdb\\ca_transit_stops\n",
      "OBJECTID\n",
      "Shape\n",
      "org_id\n",
      "agency\n",
      "stop_id\n",
      "stop_name\n",
      "n_routes\n",
      "route_ids_served\n",
      "routetypes\n",
      "n_arrivals\n",
      "n_hours_in_service\n",
      "meters_to_ca_state_highway\n",
      "base64_url\n",
      "district_name\n",
      "staging.gdb\\speeds_by_stop_segments\n",
      "OBJECTID\n",
      "Shape\n",
      "route_id\n",
      "direction_id\n",
      "stop_pair\n",
      "stop_pair_name\n",
      "time_period\n",
      "p50_mph\n",
      "n_trips\n",
      "p20_mph\n",
      "p80_mph\n",
      "district_name\n",
      "org_id\n",
      "agency\n",
      "base64_url\n",
      "Shape_Length\n",
      "staging.gdb\\speeds_by_route_time_of_day\n",
      "OBJECTID\n",
      "Shape\n",
      "route_id\n",
      "direction_id\n",
      "time_period\n",
      "speed_mph\n",
      "district_name\n",
      "org_id\n",
      "agency\n",
      "base64_url\n",
      "route_name\n",
      "Shape_Length\n"
     ]
    }
   ],
   "source": [
    "def update_feature_class_with_json(one_feature_class, meta_json_dict: dict):\n",
    "    \"\"\"\n",
    "    Update a single feature class.\n",
    "    Rename columns, apply FGDC metadata fields \n",
    "    template, and update metadata class attributes\n",
    "    that can be accessed through the arcpy.metadata class.\n",
    "    \"\"\"\n",
    "    this_feature_class = feature_class_in_gdb_path(\n",
    "        staging_location, \n",
    "        one_feature_class\n",
    "    )\n",
    "        \n",
    "    subset_meta_dict = meta_json_dict[one_feature_class]\n",
    "        \n",
    "    if \"rename_cols\" in subset_meta_dict.keys():  \n",
    "        rename_dict = subset_meta_dict[\"rename_cols\"]\n",
    "\n",
    "        rename_columns_with_dict(this_feature_class, rename_dict)\n",
    "    \n",
    "    # Check that renaming is done\n",
    "    print(this_feature_class)\n",
    "    check_fields = arcpy.ListFields(this_feature_class)\n",
    "    for field in check_fields:\n",
    "        print(field.name)\n",
    "    \n",
    "    #  We don't seem to have this function anywhere, but it or something similar should be (re)created to properly update FGDC.\n",
    "    #  For now, it can be done semi-manually (Hub scripts update field-level data, manually edit summary/descriptions)\n",
    "    # Sync with FGDC metadata\n",
    "    # (this is on the one_feature_class, which sits outside of staging/)\n",
    "    #import_fgdc_metadata_and_sync(one_feature_class)\n",
    "    \n",
    "    # Now update the rest of the metadata elements\n",
    "    update_metadata_class(this_feature_class, subset_meta_dict)\n",
    "\n",
    "    return\n",
    "\n",
    "    \n",
    "for f in in_features:\n",
    "    update_feature_class_with_json(f, meta_dict)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exported FGDC XML for ca_hq_transit_areas\n",
      "Exported FGDC XML for ca_hq_transit_stops\n",
      "Exported FGDC XML for ca_transit_routes\n",
      "Exported FGDC XML for ca_transit_stops\n",
      "Exported FGDC XML for speeds_by_stop_segments\n",
      "Exported FGDC XML for speeds_by_route_time_of_day\n"
     ]
    }
   ],
   "source": [
    "# if there are updates to data_dictionary.yml, this needs to be run\n",
    "# so fields reflect new definitions.\n",
    "for f in in_features:\n",
    "    export_fgdc_metadata(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "successful export: ca_hq_transit_areas\n",
      "successful export: ca_hq_transit_stops\n",
      "successful export: ca_transit_routes\n",
      "successful export: ca_transit_stops\n",
      "successful export: speeds_by_stop_segments\n",
      "successful export: speeds_by_route_time_of_day\n"
     ]
    }
   ],
   "source": [
    "for f in in_features:\n",
    "    this_feature_class = feature_class_in_gdb_path(staging_location, f)\n",
    "\n",
    "    # Original metadata\n",
    "    # Migrating to Pro: https://pro.arcgis.com/en/pro-app/latest/arcpy/metadata/migrating-from-arcmap-to-arcgis-pro.htm\n",
    "\n",
    "    source_metadata = md.Metadata(this_feature_class)\n",
    "    # Export metadata XML    \n",
    "    meta_output = os.path.join(working_dir, f\"{f}.xml\")\n",
    "            \n",
    "    # In ArcGIS Pro, instead of FGDC for Desktop, use ISO 19139 GML 3.2\n",
    "    # https://sv03tmcpo.ct.dot.ca.gov/portal/apps/sites/#/geep/pages/open-data-request\n",
    "    TRANSLATOR = \"ISO19139_GML32\"     \n",
    "    \n",
    "    source_metadata.exportMetadata(\n",
    "        outputPath = meta_output, \n",
    "        metadata_export_option = TRANSLATOR\n",
    "    )\n",
    "    \n",
    "    print(f\"successful export: {f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Update XML in JupyterHub\n",
    "\n",
    "Run `python metadata_update_pro.py`\n",
    "\n",
    "### Import FGDC metadata for each dataset manually by opening ArcGIS Pro\n",
    "The button to Metadata > Import > type of metadata set to FGDC does something different than the `metadata.importMetadata` feature, which doesn't do it. Manually doing the import for the fgdb metadata works for each dataset only.\n",
    "\n",
    "Do this FGDC metadata first to get the field descriptions populated. If we do this second, certain items in the metadata will get overwritten and set to blank.\n",
    "\n",
    "Somewhere once FGDC applied first, it erases the tags we included. Sad.\n",
    "\n",
    "### With new XML, finish up workflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Write layers to open_data gdb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div class='gpresult'><h2>Messages</h2><div id='messages' data-messages='[\"Start Time: Tuesday, May 27, 2025 1:41:11 PM\",\"Succeeded at Tuesday, May 27, 2025 1:41:11 PM (Elapsed Time: 0.09 seconds)\"]' data-show='true'><div id = 'default' /></div></div>"
      ],
      "text/plain": [
       "<Result 'C:\\\\Users\\\\s156485\\\\Documents\\\\ArcGIS\\\\open_data.gdb'>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "arcpy.management.CreateFileGDB(working_dir, 'open_data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "in open_data.gdb: ca_hq_transit_areas\n",
      "in open_data.gdb: ca_hq_transit_stops\n",
      "in open_data.gdb: ca_transit_routes\n",
      "in open_data.gdb: ca_transit_stops\n",
      "in open_data.gdb: speeds_by_stop_segments\n",
      "in open_data.gdb: speeds_by_route_time_of_day\n"
     ]
    }
   ],
   "source": [
    "# Write layers to open_data (with the overwritten and updated XML already)\n",
    "def write_feature_class_to_open_data(\n",
    "    one_feature_class,\n",
    "    staging_gdb = staging_location, \n",
    "    output_gdb = out_location, \n",
    "):\n",
    "    \"\"\"\n",
    "    Move the feature class from the staging gdb to the output gdb.\n",
    "    Delete the feature class in the output gdb because\n",
    "    we don't want _1 appended to the end\n",
    "    \"\"\"\n",
    "    staging_feature_class = feature_class_in_gdb_path(\n",
    "        staging_gdb, \n",
    "        one_feature_class\n",
    "    )\n",
    "    out_feature_class = feature_class_in_gdb_path(\n",
    "        output_gdb, \n",
    "        one_feature_class\n",
    "    )\n",
    "    \n",
    "    if arcpy.Exists(out_feature_class): \n",
    "        arcpy.management.Delete(out_feature_class)\n",
    "\n",
    "    # Copy over the feature class from staging.gdb to open_data.gdb\n",
    "    arcpy.conversion.FeatureClassToFeatureClass(\n",
    "        staging_feature_class, \n",
    "        output_gdb, \n",
    "        one_feature_class\n",
    "    )\n",
    "    \n",
    "#     arcpy.conversion.FeatureClassToFeatureClass(\n",
    "#         staging_feature_class, \n",
    "#         output_gdb, \n",
    "#         one_feature_class\n",
    "#     )\n",
    "    \n",
    "    return\n",
    "    \n",
    "\n",
    "for f in in_features:\n",
    "    write_feature_class_to_open_data(f)\n",
    "    print(f\"in open_data.gdb: {f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exit and restart ArcPro to clear locks on layers in overwriting\n",
    "\n",
    "If we don't exit, the layer will be locked because it shows we're already using it (staging to open_data), and it will prevent writing from open_data to the enterprise gdb.\n",
    "\n",
    "License Select must be set to `Advanced` for this to work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ENTERPRISE_DATABASE = \"HQrail(edit)@sv03tmcsqlprd1.sde\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "for f in in_features:\n",
    "    out_feature_class = feature_class_in_gdb_path(out_location, f)\n",
    "    \n",
    "    arcpy.FeatureClassToFeatureClass_conversion(\n",
    "        in_features = out_feature_class,\n",
    "        out_path = ENTERPRISE_DATABASE,\n",
    "        out_name = f\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
