{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c7feec3-aa18-42ab-94b9-cab4be608152",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "import _speed_utils as speed_utils\n",
    "import _threshold_utils as threshold_utils\n",
    "import altair as alt\n",
    "import dask.dataframe as dd\n",
    "import geopandas as gpd\n",
    "import pandas as pd\n",
    "from segment_speed_utils import gtfs_schedule_wrangling, helpers, segment_calcs\n",
    "from segment_speed_utils.project_vars import (\n",
    "    COMPILED_CACHED_VIEWS,\n",
    "    PROJECT_CRS,\n",
    "    SEGMENT_GCS,\n",
    "    analysis_date,\n",
    "    CONFIG_PATH\n",
    ")\n",
    "from scripts import A1_sjoin_vp_segments\n",
    "from shared_utils import calitp_color_palette as cp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0108ae4a-4518-4487-85f7-a5faa3e9cbf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.options.display.max_columns = 100\n",
    "pd.options.display.float_format = \"{:.2f}\".format\n",
    "pd.set_option(\"display.max_rows\", None)\n",
    "pd.set_option(\"display.max_colwidth\", None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80014c5e-695d-4280-89cd-4e7e2bb3d302",
   "metadata": {},
   "outputs": [],
   "source": [
    "# alt.data_transformers.disable_max_rows()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35c1ba95-5929-4a7f-bfae-f6d126622104",
   "metadata": {},
   "source": [
    "### Merging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f0c5f4f-f419-42a8-8527-7060ed412092",
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_all_speeds(analysis_date:str) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Merge avg_speeds_stop_segments and\n",
    "    speed_stops parquets.\n",
    "    \n",
    "    Args:\n",
    "        date: analysis date\n",
    "    \"\"\"\n",
    "    # Open up avg speeds\n",
    "    avg_speeds = pd.read_parquet(f\"{SEGMENT_GCS}avg_speeds_stop_segments_{analysis_date}.parquet\")\n",
    "    avg_speeds = avg_speeds.drop(columns=[\"geometry\", \"district\", \"district_name\"])\n",
    "    # Filter  for all day flags\n",
    "    avg_speeds = avg_speeds[avg_speeds.time_of_day == 'all_day'].reset_index(drop = True)\n",
    "    \n",
    "    # Open up speeds\n",
    "    speeds = pd.read_parquet(f\"{SEGMENT_GCS}speeds_stop_segments_{analysis_date}\")\n",
    "    \n",
    "    merge_cols = ['gtfs_dataset_key','shape_array_key', 'stop_sequence']\n",
    "    m1 = pd.merge(avg_speeds, speeds, on = merge_cols, how = 'inner')\n",
    "    \n",
    "    m1 = m1.drop_duplicates().reset_index(drop = True)\n",
    "    \n",
    "    return m1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84ac97bf-ee4f-4d85-b523-8a36823f9d9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "m1 = merge_all_speeds(analysis_date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68950ae7-4061-47d6-ac48-5eac0b1f29c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# m1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b04dfb8b-7476-49df-873a-cea75dc61763",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Picked 4 random routes\n",
    "sample_0_keys = [\n",
    "    \"0fb4f3627996269dc7075276d3b69e36\",\n",
    "    \"07c9a47264a43d8d0d16ef7109e8fd68\",\n",
    "    \"106d979b9a9e6338827a8e1c145e69fd\",\n",
    "    \"000624bd8453dbe4f2eb2765b04bcb98\",\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "898e3546-5298-4c4f-87d0-ee1d1a10f07d",
   "metadata": {},
   "source": [
    "### Categorize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e81e59fd-cc2f-408e-9148-1a1055425fc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def categorize_by_percentile_pandas(\n",
    "    df: pd.DataFrame, column_percentile: str, column_str: str\n",
    ") -> pd.DataFrame:\n",
    "\n",
    "    # Find percentiles\n",
    "    p5 = df[column_percentile].quantile(0.05).astype(float)\n",
    "    p95 = df[column_percentile].quantile(0.95).astype(float)\n",
    "    \n",
    "    def rate(row):\n",
    "        if ((row[column_percentile] >= 0) and (row[column_percentile] <= p5)):\n",
    "            return f\"{column_str} is low\"\n",
    "        elif (row[column_percentile] >= p95):\n",
    "               return f\"{column_str} is high\"\n",
    "        else:\n",
    "            return f\"{column_str} is avg\"\n",
    "    \n",
    "    # Apply flags\n",
    "    df[f\"{column_str}cat\"] = df.apply(lambda x: rate(x), axis=1)\n",
    "    \n",
    "    # Clean\n",
    "    df[f\"{column_str}cat\"] = df[f\"{column_str}cat\"].str.replace(\"_\", \"\")\n",
    "\n",
    "    print(f\"Done with {column_str}\")\n",
    "    \n",
    "    return df  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dfb836d-f919-4f2b-a0d1-9e4a4713ba8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df1 = categorize_by_percentile_pandas(subset, \"meters_elapsed\", \"meters_\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f84205d-93db-49f3-be99-6b5014f7faeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0d2184f-8a44-4489-a1b4-2be8317142f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df2 = categorize_by_percentile_pandas(df1, \"sec_elapsed\", \"sec_\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "940fb010-0dff-465e-bf8d-87dd3f4ba101",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d38d541-5c9c-4d31-8986-9c3928eb2f59",
   "metadata": {},
   "outputs": [],
   "source": [
    "def categorize_meters_speeds_pandas(df)-> pd.DataFrame:\n",
    "    start = datetime.datetime.now()\n",
    "    print(start)\n",
    "    \n",
    "    #df = merge_all_speeds(analysis_date)\n",
    "    \n",
    "    # Categorize\n",
    "    df1 = categorize_by_percentile_pandas(df, \"meters_elapsed\", \"meters_\")\n",
    "    df2 = categorize_by_percentile_pandas(df1, \"sec_elapsed\", \"sec_\")\n",
    "  \n",
    "    # Find size of categories\n",
    "    print(df2.groupby(['sec_cat','meters_cat']).size())\n",
    "\n",
    "    # Filter out for only meters that are low or seconds that are high\n",
    "    df2 = df2[(df2.meters_cat == 'meters is low') | (df2.sec_cat == 'sec is high')].reset_index(drop = True)\n",
    "    print(f\"{len(df2)} rows left after filtering for rows with either high seconds OR low meters\") \n",
    "    \n",
    "    def flag_round(row):\n",
    "        if (row[\"meters_elapsed\"] == 0) & (row[\"sec_elapsed\"] == 0):\n",
    "            return \"division by 0\"\n",
    "        elif row[\"meters_cat\"] == \"meters is low\":\n",
    "            return \"meters too low\"\n",
    "        elif row[\"sec_cat\"] == \"sec is high\":\n",
    "            return \"seconds too high\"\n",
    "        else:\n",
    "            return \"ok\"\n",
    "        \n",
    "    df2[\"flag\"] = df2.apply(lambda x: flag_round(x), axis=1)\n",
    "    print(df2.flag.value_counts())\n",
    "    \n",
    "    # Filter out for only division by 0 \n",
    "    df3 = df2[(df2.flag == 'division by 0')].reset_index(drop = True)\n",
    "    \n",
    "    end = datetime.datetime.now()\n",
    "    print(f\"Took {end-start}\")\n",
    "    return df3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7201d5e3-f765-4e5d-9bbd-aa6a336bcc0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "subset = m1[m1.shape_array_key.isin(sample_0_keys)].reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c5107cb-c574-449b-95b6-fb205f38502e",
   "metadata": {},
   "outputs": [],
   "source": [
    "m2 = categorize_meters_speeds_pandas(subset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d1bf90c-d9ed-4861-a1be-23f356165a4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "m2.flag.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce0fbb35-f81e-4343-92d2-4382d2173dbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(m1)-len(m2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "508f1411-4328-4b80-a029-0ae516107ed0",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(m2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fe08d2a-b874-4439-aa5b-a52de58cad09",
   "metadata": {},
   "outputs": [],
   "source": [
    "m2.trip_id.nunique(), m1.trip_id.nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e6d31ab-46a7-4e20-bb2f-9cac1a2d672d",
   "metadata": {},
   "outputs": [],
   "source": [
    "m2.shape_array_key.nunique(), m1.shape_array_key.nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "647fad46-7f9b-4ce2-a26a-1ea69d02daee",
   "metadata": {},
   "outputs": [],
   "source": [
    "m2._gtfs_dataset_name.nunique(), m1._gtfs_dataset_name.nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83036ccc-7339-42c2-b1f7-183734253c21",
   "metadata": {},
   "outputs": [],
   "source": [
    "m2.groupby([\"loop_or_inlining\"]).agg({\"shape_array_key\": \"nunique\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4486cd7c-31d7-4420-ac67-f9783676ede8",
   "metadata": {},
   "source": [
    "#### See how many trips for a shape ID have problematic rows\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "468be3c9-7a24-4f01-84fd-31c137bc45e8",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Number of trips that have at least one row that was divided by 0 \n",
    "# for this shape array key\n",
    "df1 = m2.groupby(['shape_array_key']).agg({'trip_id':'nunique'}).rename(columns = {'trip_id':'trips_with_zero'}).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4350f540-8f6b-4fb0-8b16-836245c0e44c",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Original number of trips\n",
    "df2 = m1.groupby(['shape_array_key']).agg({'trip_id':'nunique'}).rename(columns = {'trip_id':'all_trips'}).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac68bdf7-26a0-4679-9a35-26f8a670018a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df3 = pd.merge(df1, df2, how = \"inner\", on = 'shape_array_key')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81d443cc-122f-46f1-87ec-dbdc74e0ca6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df3['percent_of_trips_with_problematic_rows'] = df3.trips_with_zero/df3.all_trips * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "314d9baf-de0e-460a-8c29-4504ba94cfa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df3['percent_of_trips_with_problematic_rows'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5de3efe6-2233-4251-93a8-1f8dd6fb2dae",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# df3.sample(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a399d982-e400-43fa-b13f-fecafaa27262",
   "metadata": {},
   "source": [
    "### Investigate \n",
    "#### Stage3: \"vp_pared_stops\"/A3_loop_inlining\n",
    "* Rewrite this part to filter read_parquet with the shape array and whatnot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2a705af-b588-463b-b6ce-f999b2050208",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_vp_stage3(flagged_df:pd.DataFrame, date:str) -> pd.DataFrame:\n",
    "    \n",
    "    # Subset the dataframe and use it to filter out for only the values of interest\n",
    "    shape_array_keys = flagged_df.shape_array_key.unique().tolist()\n",
    "    stop_seq = flagged_df.stop_sequence.unique().tolist() \n",
    "    trip_id = flagged_df.trip_id.unique().tolist() \n",
    "    gtfs_dataset_key = flagged_df.gtfs_dataset_key.unique().tolist() \n",
    "    \n",
    "    #flagged_df = flagged_df[['gtfs_dataset_key', 'trip_id','stop_sequence','shape_array_key']]\n",
    "    vp = pd.read_parquet(f\"{SEGMENT_GCS}vp_pared_stops_{date}\",\n",
    "        filters = [[('shape_array_key', \"in\", shape_array_keys),\n",
    "                   ('stop_sequence', 'in', stop_seq), \n",
    "                   ('trip_id', 'in', trip_id), \n",
    "                   ('gtfs_dataset_key', 'in', gtfs_dataset_key)]],)\n",
    "    \n",
    "    # Merge to filter\n",
    "    vp2 = pd.merge(flagged_df, vp, how = \"inner\", on = ['gtfs_dataset_key', 'trip_id','stop_sequence','shape_array_key'])\n",
    "    \n",
    "    return vp2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e36c5fc-ab3f-4129-97f9-ad9472b7d32a",
   "metadata": {},
   "outputs": [],
   "source": [
    "vp2 = load_vp_stage3(subset, analysis_date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d6fe654-40ca-4758-bc2c-316e33d1a9d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# vp = pd.read_parquet(f\"{SEGMENT_GCS}vp_pared_stops_{analysis_date}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "148e75f1-08dd-44c8-8179-319164d8e020",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Check out stop sequences for the trip below that have division by 0\n",
    "# subset[subset.trip_id == \"1088383\"].stop_sequence.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4350206-c237-44a3-abce-f8f38cde8117",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Stop sequences that were flagged as division by 0\n",
    "# vp2[vp2.trip_id == \"1088383\"].sort_values(['trip_id', 'stop_sequence','location_timestamp_local'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa1e56d1-ec07-436c-8763-7bcf3dcbf7d4",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# All the stop sequences for this trip, even those that are ok\n",
    "# vp_pared[vp_pared.trip_id == \"1088383\"].sort_values(['trip_id', 'stop_sequence','location_timestamp_local'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22e42aae-9281-4040-ab8c-6a10b93f6cf4",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# All the stop sequences for this trip, even those that are ok\n",
    "# vp_pared[vp_pared.trip_id == \"1088383\"].sort_values(['location_timestamp_local','stop_sequence',])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f21f08f-d4eb-4bbd-94d3-f4b031e97cf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def stage3_repeated_timestamps(stage3_df:pd.DataFrame)-> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Look at how many times a time stamp is repeated a route-trip-location.\n",
    "    Each of these 3 combos should have a different time for each \n",
    "    stop sequence or else the vehicle is not changing locations.\n",
    "    \"\"\"\n",
    "    agg = (stage3_df\n",
    "     .groupby(['shape_array_key','trip_id', 'location_timestamp_local'])\n",
    "     .agg({'stop_sequence':'nunique'})\n",
    "     .reset_index()\n",
    "     .rename(columns = {'stop_sequence':'number_of_repeated_timestamps'})\n",
    "    )\n",
    "    \n",
    "    # Only keep timestamps that are repeated more than once\n",
    "    agg = (agg[agg.number_of_repeated_timestamps > 1]).reset_index(drop = True)\n",
    "\n",
    "    return agg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ce07566-c1f0-4fa7-9550-2fa07b98dba8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def stage3_repeated_locations(stage3_df:pd.DataFrame):\n",
    "    \"\"\"\n",
    "    Look at how many times a time stamp is repeated for a stop-trip-route combo.\n",
    "    Each of these 3 combos should have a different location for each \n",
    "    stop sequence or else the vehicle is not changing locations.\n",
    "    \"\"\"\n",
    "    # Concat x and y into a string\n",
    "    stage3_df['pair'] = stage3_df.x.astype(str) + '/' + vp2.y.astype(str)\n",
    "    \n",
    "    # Count number of different stops that reference the same location\n",
    "    agg = (stage3_df\n",
    "     .groupby(['shape_array_key','trip_id','pair'])\n",
    "     .agg({'stop_sequence':'nunique'})\n",
    "     .reset_index()\n",
    "     .sort_values('stop_sequence', ascending = False)\n",
    "     .rename(columns = {'stop_sequence':'number_of_repeated_locs'})               \n",
    "    )\n",
    "\n",
    "    # Only keep locations that are repeated more than once\n",
    "    agg = agg[agg.number_of_repeated_locs != 1].reset_index(drop = True)\n",
    "    \n",
    "    return agg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66e83169-2b4a-4912-bc0e-1a0b3e8deea6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def flag_stage3(flagged_df:pd.DataFrame, date:str) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Flag the errors in stage3\n",
    "    \"\"\"\n",
    "    start = datetime.datetime.now()\n",
    "    print(start)\n",
    "    \n",
    "    # Relevant rows from Vehicle Positions\n",
    "    vp = load_vp_stage3(flagged_df, date)\n",
    "    \n",
    "    # Find repeated timestamps.\n",
    "    multi_timestamps = stage3_repeated_timestamps(vp)\n",
    "    \n",
    "    # Find repeated locations\n",
    "    multi_locs = stage3_repeated_locations(vp)\n",
    "    \n",
    "    # Merge\n",
    "    timestamps_merge_cols = ['shape_array_key','trip_id','location_timestamp_local']\n",
    "    loc_merge_cols =  ['shape_array_key','trip_id','pair']\n",
    "    \n",
    "    # Want everything found in vehicle positions, so do left merges\n",
    "    m1 = (vp\n",
    "          .merge(multi_timestamps, how=\"left\", on= timestamps_merge_cols)\n",
    "          .merge(multi_locs, how=\"left\", on=loc_merge_cols)\n",
    "         )\n",
    "    \n",
    "    drop_cols = ['vp_idx','x','y','hour','activity_date',]\n",
    "    m1 = m1.drop(columns = drop_cols)\n",
    "    \n",
    "    # Flag\n",
    "    def flag(row):\n",
    "        if (row[\"number_of_repeated_timestamps\"] > 1) & (row[\"number_of_repeated_locs\"] > 1):\n",
    "            return \"repeated timestamps & locations\"\n",
    "        elif (row[\"number_of_repeated_timestamps\"] > 1):\n",
    "            return \"repeated timestamps\"\n",
    "        elif (row[\"number_of_repeated_locs\"] > 1):\n",
    "            return \"repeated locations\"\n",
    "        else:\n",
    "            return \"check in stage 2\"\n",
    "        \n",
    "    m1[\"stage3_flag\"] = m1.apply(lambda x: flag(x), axis=1)\n",
    "    \n",
    "    print(m1.stage3_flag.value_counts())\n",
    "    \n",
    "    check_in_stage2 = m1[m1.stage3_flag == \"check in stage 2\"]\n",
    "    print(f\"Have to check {len(check_in_stage2)/len(m1) * 100} % of rows in stage 2\")\n",
    "    \n",
    "    end = datetime.datetime.now()\n",
    "    print(f\"Took {end-start}\")\n",
    "    return m1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cab32ef3-cc66-40ce-aa19-59631734f539",
   "metadata": {},
   "outputs": [],
   "source": [
    "m3 = flag_stage3(m2, analysis_date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68a9dbba-ee6b-42b1-9203-1146d6cd56e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "m3.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cca329c-14bc-4ad5-9465-1a63ca53df49",
   "metadata": {},
   "outputs": [],
   "source": [
    "m3 = m3[m3.stage3_flag == \"check in stage 2\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93e87778-edef-4d62-98aa-a4241f177892",
   "metadata": {},
   "outputs": [],
   "source": [
    "m3.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21799f42-873e-41bd-b764-42cc297686a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "sort_cols = ['trip_id', 'shape_array_key', 'stop_sequence']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b1876cf-9e8b-4c30-8723-2226133b8e01",
   "metadata": {},
   "source": [
    "#### Stage2: \"vp_stop_segment\"/A1_sjoin_vp_segments\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a469849-f903-44e4-9d2a-4f3775270a52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select one route to look at\n",
    "test_route = \"106d979b9a9e6338827a8e1c145e69fd\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c23a767c-80b5-439d-97e2-b7fe5e6bfd06",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_sequence = 39"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e946c68-3476-459d-a869-77ac37b5fb07",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_gtfs_key = \"db56b50ab86b5f7a4ae2fc2dd9889bbe\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4fa40bf-387c-4301-ba13-2bd16b15cd24",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_trip = '1088405'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef18eb20-a43e-4d32-80eb-7f902116a944",
   "metadata": {},
   "source": [
    "#### Look at export  file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6397dc45-c271-4057-a0d8-1962846d4f94",
   "metadata": {},
   "outputs": [],
   "source": [
    "def import_stage_2(date:str, route:str, stop_sequence:str):\n",
    "    df = pd.read_parquet(\n",
    "            f\"{SEGMENT_GCS}vp_sjoin/vp_stop_segment_{date}\",\n",
    "            filters = [[('shape_array_key', \"==\", route),\n",
    "                       ('stop_sequence', \"==\", stop_sequence)]],\n",
    "        )\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe8f800a-f180-4495-a387-0367528823ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# stg2 = import_stage_2(analysis_date, test_route, test_sequence)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b5dec8d-c4f5-49dd-9a11-1b10ff30fb55",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Look at vp trips -> import unique trips"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ade9e07f-0b55-4561-96d1-6fd6adec0f1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def import_unique_trips(gtfs_key:str, trip: str, route:str):\n",
    "    vp_trips = A1_sjoin_vp_segments.add_grouping_col_to_vp(\n",
    "        f\"vp_usable_{analysis_date}\",\n",
    "        analysis_date,\n",
    "       [\"shape_array_key\"]\n",
    "    )\n",
    "    \n",
    "    df = vp_trips[(vp_trips.gtfs_dataset_key == gtfs_key)\n",
    "                    & (vp_trips.shape_array_key == route)\n",
    "                    & (vp_trips.trip_id == trip)].reset_index(drop = True)\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8003044-b7e4-477e-9395-fa881a2fa2b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_trips = import_unique_trips(test_gtfs_key, test_trip, test_route)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52ce333b-9f75-4c9b-a1af-130c93786f94",
   "metadata": {},
   "source": [
    "#### Look at vehicle positions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac6e78c3-694d-4297-8db1-f0f4d6faadbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def import_vehicle_positions(unique_trips:pd.DataFrame, gtfs_key:str, trip_id:str)-> gpd.GeoDataFrame:\n",
    "    vp = helpers.import_vehicle_positions(\n",
    "            SEGMENT_GCS,\n",
    "            f\"vp_usable_{analysis_date}/\",\n",
    "            filters = [[(\"gtfs_dataset_key\", \"==\", gtfs_key),\n",
    "                      ('trip_id', '==', trip_id)]],\n",
    "            columns = [\"gtfs_dataset_key\", \"trip_id\", \n",
    "                       \"vp_idx\", \"x\", \"y\"],\n",
    "            partitioned = True\n",
    "        )\n",
    "    \n",
    "    vp = vp.compute()\n",
    "    vp = vp.merge(unique_trips, on = [\"gtfs_dataset_key\", \"trip_id\"],\n",
    "            how = \"inner\"\n",
    "        )\n",
    "    \n",
    "    vp_gdf = gpd.GeoDataFrame(\n",
    "        vp, \n",
    "        geometry = gpd.points_from_xy(vp.x, vp.y, crs = \"EPSG:4326\")\n",
    "    ).to_crs(PROJECT_CRS).drop(columns = [\"x\", \"y\"])\n",
    "    \n",
    "    return vp_gdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b47ea0cb-6031-4d98-b963-efbef949d169",
   "metadata": {},
   "outputs": [],
   "source": [
    "vehicle_positions = import_vehicle_positions(unique_trips, test_gtfs_key, test_trip)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0f96480-8328-43ed-9add-0a74b533fc8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(vehicle_positions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "896e00be-27f2-43b7-9cb4-69d61a061af0",
   "metadata": {},
   "source": [
    "#### Look at segments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17bb5083-ace2-4e2d-8400-3bf948625909",
   "metadata": {},
   "outputs": [],
   "source": [
    "def import_segments(flagged_df: pd.DataFrame, route:str, gtfs_key:str) -> gpd.GeoDataFrame:\n",
    "    gdf = gpd.read_parquet(f\"{SEGMENT_GCS}stop_segments_{analysis_date}.parquet\",\n",
    "                           filters = [[(\"shape_array_key\", \"==\", route),\n",
    "                                      (\"gtfs_dataset_key\", \"==\", gtfs_key),\n",
    "                                     ]]).to_crs(PROJECT_CRS)\n",
    "    \n",
    "    gdf[\"geometry_buffered\"] = gdf.geometry.buffer(35)\n",
    "    gdf = gdf.set_geometry('geometry_buffered')\n",
    "    \n",
    "    # Distinguish between \"correct\" and \"incorrect\" seq\n",
    "    # A sequence can be incorrect even if just one row is \"divided by 0\"\n",
    "    incorrect_segments = flagged_df[(flagged_df.shape_array_key == route) & (flagged_df.gtfs_dataset_key == gtfs_key)]\n",
    "    incorrect_segments_list = incorrect_segments.stop_sequence.unique().tolist()\n",
    "    incorrect_segments_filtered = gdf[gdf.stop_sequence.isin(incorrect_segments_list)].reset_index(drop = True)\n",
    "    incorrect_segments_filtered['flag'] = 'incorrect'\n",
    "    \n",
    "    # Filter for correct segments\n",
    "    correct_segments = flagged_df[~flagged_df.stop_sequence.isin(incorrect_segments_list)]\n",
    "    correct_segments_list = correct_segments.stop_sequence.unique().tolist()\n",
    "    correct_segments_filtered = gdf[gdf.stop_sequence.isin(correct_segments_list)].reset_index(drop = True)\n",
    "    correct_segments_filtered['flag'] = 'correct'\n",
    "    \n",
    "    final = pd.concat([correct_segments_filtered, incorrect_segments_filtered])\n",
    "    \n",
    "    return final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f3a302a-f604-49fe-ae9b-ee8db85466de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# flagged_segments = import_segments(m3, test_route, test_gtfs_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8a14cfd-38b8-4326-9eac-a711f1a189e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#segments = A1_sjoin_vp_segments.import_segments_and_buffer(\n",
    " #   f\"stop_segments_{analysis_date}\",\n",
    "#    35,\n",
    "   # [\"shape_array_key\", \"stop_sequence\"]+ [\"seg_idx\", \"geometry\"]\n",
    "#)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c08c38e-7419-4ff6-b74f-5f15615e52c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# segments = segments.compute()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a688361-e7a9-40f0-877d-3693be99a960",
   "metadata": {},
   "source": [
    "#### Stops kept: last and first"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ea744bf-8019-4b7c-988f-f95196b56435",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_first_last_points(route:str, trip:str, gtfs_key:str):\n",
    "    df = pd.read_parquet(f\"{SEGMENT_GCS}vp_pared_stops_{analysis_date}\",\n",
    "        filters = [[('shape_array_key', \"==\", route),\n",
    "                  \n",
    "                   ('trip_id', \"==\", trip), \n",
    "                   ('gtfs_dataset_key', '==', gtfs_key)]],)\n",
    "    \n",
    "    gdf =  gpd.GeoDataFrame(\n",
    "        df, \n",
    "        geometry = gpd.points_from_xy(df.x, df.y, crs = \"EPSG:4326\")\n",
    "    ).to_crs(PROJECT_CRS).drop(columns = [\"x\", \"y\"])\n",
    "    \n",
    "    gdf = gdf[['geometry','stop_sequence']]\n",
    "    \n",
    "    return gdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c7aa90f-3e80-472e-b61d-94afd6c0ec01",
   "metadata": {},
   "outputs": [],
   "source": [
    "first_last = find_first_last_points(test_route, test_trip, test_gtfs_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5634169a-f26b-4174-b46a-3aa872bc1bdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(first_last)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1aeb604b-249b-41e7-be71-fe8d3205e54a",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a785180-38bb-4d33-a96e-3385c84ed2f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_maps(all_points: gpd.GeoDataFrame, first_last_points: gpd.GeoDataFrame, segments: gpd.GeoDataFrame):\n",
    "    base1 = segments.explore('flag', cmap= 'tab10', height = 400, width = 600, name = 'segments')\n",
    "    \n",
    "    all_points_map = all_points.explore(m = base1, color = 'red',style_kwds = {'weight':5}, legend_kwds = {'caption': 'all_points'}, name= 'points')\n",
    "    \n",
    "    display(all_points_map) \n",
    "    first_last_map = first_last_points.explore('stop_sequence', cmap = 'tab10',style_kwds = {'weight':5},height = 400, width = 600,)\n",
    "    display(first_last_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bab6bfc4-3d08-46fe-be33-6179ac5df34d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# display_maps(vehicle_positions,first_last,flagged_segments)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7ed9dc6-80ce-46f9-ae59-a5ed2bbcad50",
   "metadata": {},
   "source": [
    "#### Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "509bc2c0-14f3-4021-baf1-6d89a2409a79",
   "metadata": {},
   "outputs": [],
   "source": [
    "def stage2_trouble_shooting(flagged_df:pd.DataFrame,\n",
    "                            date:str, route:str, \n",
    "                            stop_sequence:str, \n",
    "                            trip:str, gtfs_key:str):\n",
    "    stg2 = import_stage_2(date, route, stop_sequence)\n",
    "    unique_trips = import_unique_trips(gtfs_key, trip, route)\n",
    "    \n",
    "    vehicle_positions = import_vehicle_positions(unique_trips, gtfs_key, trip)\n",
    "    flagged_segments = import_segments(flagged_df, route, gtfs_key)\n",
    "    first_last = find_first_last_points(route, trip, gtfs_key)\n",
    "    \n",
    "    display_maps(vehicle_positions,first_last,flagged_segments)\n",
    "    return vehicle_positions, first_last, flagged_segments\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a4ae095-d010-46b5-80b2-0bbe948f249f",
   "metadata": {},
   "outputs": [],
   "source": [
    "test1_allpts, test1_firstlast_pts, test1_flagged = stage2_trouble_shooting(flagged_df= m3,\n",
    "                        date = analysis_date,\n",
    "                        route = test_route,\n",
    "                        stop_sequence = test_sequence,\n",
    "                        trip = test_trip,\n",
    "                        gtfs_key = test_gtfs_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b45a9b37-21ed-4ceb-9561-3c04568e8e68",
   "metadata": {},
   "outputs": [],
   "source": [
    "test1_allpts.shape, test1_firstlast_pts.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad92a3aa-84db-4b25-9514-bd7be30861a2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "268c5f17-df4a-4d84-b3b0-d76a2727bcf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_route2 = \"0fb4f3627996269dc7075276d3b69e36\"\n",
    "test_stop = 13\n",
    "test_gtfs_key2 = \"a4f6fd5552107e05fe9743ac7cce2c55\"\n",
    "test_trip2 = \"16939095\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "deaa6fdf-37b8-49ee-97f2-46f74d41a449",
   "metadata": {},
   "outputs": [],
   "source": [
    "test2_allpts, test2_firstlast_pts, test2_flagged = stage2_trouble_shooting(flagged_df= m3,\n",
    "                        date = analysis_date,\n",
    "                        route = test_route2,\n",
    "                        stop_sequence = test_stop,\n",
    "                        trip = test_trip2,\n",
    "                        gtfs_key = test_gtfs_key2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "330951ba-846e-408f-a581-ce946df0cc76",
   "metadata": {},
   "outputs": [],
   "source": [
    "test2_allpts.shape, test2_firstlast_pts.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db8465be-8938-4ebe-977c-6a784141b4da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test2_base = test2_flagged.explore('flag', cmap= 'tab10', height = 400, width = 600, name = 'segments')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d784404-f514-4648-b9c8-7809fb2fa474",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test2_all_pts_map = test2_allpts.explore(m = test2_base, color = 'red',style_kwds = {'weight':5}, legend_kwds = {'caption': 'all_points'}, name= 'points')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e58353e4-d9c9-402d-99f7-3dd259a4b00e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test2_all_pts_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d7e0287-d90f-45c0-b3f1-45a6a0e7a2aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test2_firstlast_pts.explore( color = 'red',style_kwds = {'weight':5}, height = 400, width = 600, )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0ff9ce9-9642-4f91-93de-f35bc45d2e1e",
   "metadata": {
    "tags": []
   },
   "source": [
    "##### Sjoin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd2a6beb-a2e9-498c-beed-ce7365ffa7a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sjoin_vp_segments(segments: gpd.GeoDataFrame, vp_gdf: gpd.GeoDataFrame):\n",
    "    vp_in_seg = gpd.sjoin(\n",
    "        vp_gdf,\n",
    "        segments,\n",
    "        how = \"inner\",\n",
    "        predicate = \"within\"\n",
    "    )\n",
    "    # vp_in_seg = vp_in_seg.set_geometry('geometry_left')\n",
    "    \n",
    "    return vp_in_seg\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06df58c1-c7b7-4769-bd8d-696e337eefb3",
   "metadata": {},
   "source": [
    "### Stage1: \"vp_usable\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9eab37f-0569-4f07-9113-87200b0c7dfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# What's the diff between stop segments normal/special/and without any notation?\n",
    "usable = pd.read_parquet(f\"{SEGMENT_GCS}vp_usable_{analysis_date}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "deb486c8-a800-485e-8a46-d994af1c0074",
   "metadata": {},
   "outputs": [],
   "source": [
    "usable.sample()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3d8352e-7c60-4368-b78f-02e20136947a",
   "metadata": {},
   "outputs": [],
   "source": [
    "subset_for_merge2 = subset_for_merge.drop(columns = ['stop_sequence','stop_id','meters_elapsed','sec_elapsed'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "813ae4db-0fef-4f10-9408-7284fc531ed2",
   "metadata": {},
   "outputs": [],
   "source": [
    "m_cols2 = ['gtfs_dataset_key',\n",
    " 'trip_id']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d08fa8db-f3a3-43f2-a763-a39cacc9cf9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "subset_for_merge2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68a59632-cfbd-43fd-aca4-a502e400a854",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# m2[m2.trip_id == '1350']"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
