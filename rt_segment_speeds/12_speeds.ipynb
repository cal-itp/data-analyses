{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2c7feec3-aa18-42ab-94b9-cab4be608152",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.9/site-packages/geopandas/_compat.py:123: UserWarning: The Shapely GEOS version (3.11.1-CAPI-1.17.1) is incompatible with the GEOS version PyGEOS was compiled with (3.10.1-CAPI-1.16.0). Conversions between both will be slow.\n",
      "  warnings.warn(\n",
      "/home/jovyan/data-analyses/rt_segment_speeds/_threshold_utils.py:1: UserWarning: Shapely 2.0 is installed, but because PyGEOS is also installed, GeoPandas will still use PyGEOS by default for now. To force to use and test Shapely 2.0, you have to set the environment variable USE_PYGEOS=0. You can do this before starting the Python process, or in your code before importing geopandas:\n",
      "\n",
      "import os\n",
      "os.environ['USE_PYGEOS'] = '0'\n",
      "import geopandas\n",
      "\n",
      "In a future release, GeoPandas will switch to using Shapely by default. If you are using PyGEOS directly (calling PyGEOS functions on geometries from GeoPandas), this will then stop working and you are encouraged to migrate from PyGEOS to Shapely 2.0 (https://shapely.readthedocs.io/en/latest/migration_pygeos.html).\n",
      "  import geopandas as gpd\n"
     ]
    }
   ],
   "source": [
    "import datetime\n",
    "import _speed_utils as speed_utils\n",
    "import _threshold_utils as threshold_utils\n",
    "import altair as alt\n",
    "import dask.dataframe as dd\n",
    "import geopandas as gpd\n",
    "import pandas as pd\n",
    "from segment_speed_utils import gtfs_schedule_wrangling, helpers, segment_calcs\n",
    "from segment_speed_utils.project_vars import (\n",
    "    COMPILED_CACHED_VIEWS,\n",
    "    PROJECT_CRS,\n",
    "    SEGMENT_GCS,\n",
    "    analysis_date,\n",
    "    CONFIG_PATH\n",
    ")\n",
    "from shared_utils import calitp_color_palette as cp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0108ae4a-4518-4487-85f7-a5faa3e9cbf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.options.display.max_columns = 100\n",
    "pd.options.display.float_format = \"{:.2f}\".format\n",
    "pd.set_option(\"display.max_rows\", None)\n",
    "pd.set_option(\"display.max_colwidth\", None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "80014c5e-695d-4280-89cd-4e7e2bb3d302",
   "metadata": {},
   "outputs": [],
   "source": [
    "# alt.data_transformers.disable_max_rows()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35c1ba95-5929-4a7f-bfae-f6d126622104",
   "metadata": {},
   "source": [
    "### Merging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2f0c5f4f-f419-42a8-8527-7060ed412092",
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_all_speeds(analysis_date:str) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Merge avg_speeds_stop_segments and\n",
    "    speed_stops parquets.\n",
    "    \n",
    "    Args:\n",
    "        date: analysis date\n",
    "    \"\"\"\n",
    "    # Open up avg speeds\n",
    "    avg_speeds = pd.read_parquet(f\"{speed_utils.GCS_PATH}avg_speeds_stop_segments_{analysis_date}.parquet\")\n",
    "    avg_speeds = avg_speeds.drop(columns=[\"geometry\", \"geometry_arrowized\", \"district\", \"district_name\"])\n",
    "    # Filter  for all day flags\n",
    "    avg_speeds = avg_speeds[avg_speeds.time_of_day == 'all_day'].reset_index(drop = True)\n",
    "    \n",
    "    # Open up speeds\n",
    "    speeds = pd.read_parquet(f\"{speed_utils.GCS_PATH}speeds_stop_segments_{analysis_date}\")\n",
    "    \n",
    "    merge_cols = ['gtfs_dataset_key','shape_array_key', 'stop_sequence']\n",
    "    m1 = pd.merge(avg_speeds, speeds, on = merge_cols, how = 'inner')\n",
    "    \n",
    "    m1 = m1.drop_duplicates().reset_index(drop = True)\n",
    "    \n",
    "    return m1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "84ac97bf-ee4f-4d85-b523-8a36823f9d9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "m1 = merge_all_speeds(analysis_date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "68950ae7-4061-47d6-ac48-5eac0b1f29c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# m1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b04dfb8b-7476-49df-873a-cea75dc61763",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Picked 4 random routes\n",
    "sample_0_keys = [\n",
    "    \"0fb4f3627996269dc7075276d3b69e36\",\n",
    "    \"07c9a47264a43d8d0d16ef7109e8fd68\",\n",
    "    \"106d979b9a9e6338827a8e1c145e69fd\",\n",
    "    \"000624bd8453dbe4f2eb2765b04bcb98\",\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "898e3546-5298-4c4f-87d0-ee1d1a10f07d",
   "metadata": {},
   "source": [
    "### Categorize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e81e59fd-cc2f-408e-9148-1a1055425fc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def categorize_by_percentile_pandas(\n",
    "    df: pd.DataFrame, column_percentile: str, column_str: str\n",
    ") -> pd.DataFrame:\n",
    "\n",
    "    # Find percentiles\n",
    "    p5 = df[column_percentile].quantile(0.05).astype(float)\n",
    "    p95 = df[column_percentile].quantile(0.95).astype(float)\n",
    "    \n",
    "    def rate(row):\n",
    "        if ((row[column_percentile] >= 0) and (row[column_percentile] <= p5)):\n",
    "            return f\"{column_str} is low\"\n",
    "        elif (row[column_percentile] >= p95):\n",
    "               return f\"{column_str} is high\"\n",
    "        else:\n",
    "            return f\"{column_str} is avg\"\n",
    "    \n",
    "    # Apply flags\n",
    "    df[f\"{column_str}cat\"] = df.apply(lambda x: rate(x), axis=1)\n",
    "    \n",
    "    # Clean\n",
    "    df[f\"{column_str}cat\"] = df[f\"{column_str}cat\"].str.replace(\"_\", \"\")\n",
    "\n",
    "    print(f\"Done with {column_str}\")\n",
    "    \n",
    "    return df  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0dfb836d-f919-4f2b-a0d1-9e4a4713ba8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df1 = categorize_by_percentile_pandas(subset, \"meters_elapsed\", \"meters_\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9f84205d-93db-49f3-be99-6b5014f7faeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b0d2184f-8a44-4489-a1b4-2be8317142f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df2 = categorize_by_percentile_pandas(df1, \"sec_elapsed\", \"sec_\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "940fb010-0dff-465e-bf8d-87dd3f4ba101",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9d38d541-5c9c-4d31-8986-9c3928eb2f59",
   "metadata": {},
   "outputs": [],
   "source": [
    "def categorize_meters_speeds_pandas(df)-> pd.DataFrame:\n",
    "    start = datetime.datetime.now()\n",
    "    print(start)\n",
    "    \n",
    "    #df = merge_all_speeds(analysis_date)\n",
    "    \n",
    "    # Categorize\n",
    "    df1 = categorize_by_percentile_pandas(df, \"meters_elapsed\", \"meters_\")\n",
    "    df2 = categorize_by_percentile_pandas(df1, \"sec_elapsed\", \"sec_\")\n",
    "  \n",
    "    # Find size of categories\n",
    "    print(df2.groupby(['sec_cat','meters_cat']).size())\n",
    "\n",
    "    # Filter out for only meters that are low or seconds that are high\n",
    "    df2 = df2[(df2.meters_cat == 'meters is low') | (df2.sec_cat == 'sec is high')].reset_index(drop = True)\n",
    "    print(f\"{len(df2)} rows left after filtering for rows with either high seconds OR low meters\") \n",
    "    \n",
    "    def flag_round(row):\n",
    "        if (row[\"meters_elapsed\"] == 0) & (row[\"sec_elapsed\"] == 0):\n",
    "            return \"division by 0\"\n",
    "        elif row[\"meters_cat\"] == \"meters is low\":\n",
    "            return \"meters too low\"\n",
    "        elif row[\"sec_cat\"] == \"sec is high\":\n",
    "            return \"seconds too high\"\n",
    "        else:\n",
    "            return \"ok\"\n",
    "        \n",
    "    df2[\"flag\"] = df2.apply(lambda x: flag_round(x), axis=1)\n",
    "    print(df2.flag.value_counts())\n",
    "    \n",
    "    # Filter out for only division by 0 \n",
    "    df3 = df2[(df2.flag == 'division by 0')].reset_index(drop = True)\n",
    "    \n",
    "    end = datetime.datetime.now()\n",
    "    print(f\"Took {end-start}\")\n",
    "    return df3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7201d5e3-f765-4e5d-9bbd-aa6a336bcc0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "subset = m1[m1.shape_array_key.isin(sample_0_keys)].reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2c5107cb-c574-449b-95b6-fb205f38502e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-06-26 09:14:38.408932\n",
      "Done with meters_\n",
      "Done with sec_\n",
      "sec_cat      meters_cat    \n",
      "sec is avg   meters is avg     2415102\n",
      "             meters is high      70745\n",
      "             meters is low      139528\n",
      "sec is high  meters is avg       57245\n",
      "             meters is high      83074\n",
      "             meters is low       13695\n",
      "sec is low   meters is low      296973\n",
      "dtype: int64\n",
      "590515 rows left after filtering for rows with either high seconds OR low meters\n",
      "division by 0       296973\n",
      "meters too low      153223\n",
      "seconds too high    140319\n",
      "Name: flag, dtype: int64\n",
      "Took 0:02:17.630093\n"
     ]
    }
   ],
   "source": [
    "m2 = categorize_meters_speeds_pandas(m1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2d1bf90c-d9ed-4861-a1be-23f356165a4c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "division by 0    296973\n",
       "Name: flag, dtype: int64"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m2.flag.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ce0fbb35-f81e-4343-92d2-4382d2173dbd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2779389"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(m1)-len(m2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "508f1411-4328-4b80-a029-0ae516107ed0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "296973"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(m2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9fe08d2a-b874-4439-aa5b-a52de58cad09",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(45357, 72067)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m2.trip_id.nunique(), m1.trip_id.nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8e6d31ab-46a7-4e20-bb2f-9cac1a2d672d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2682, 4837)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m2.shape_array_key.nunique(), m1.shape_array_key.nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "647fad46-7f9b-4ce2-a26a-1ea69d02daee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(63, 76)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m2._gtfs_dataset_name.nunique(), m1._gtfs_dataset_name.nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "83036ccc-7339-42c2-b1f7-183734253c21",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>shape_array_key</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>loop_or_inlining</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2682</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  shape_array_key\n",
       "loop_or_inlining                 \n",
       "0                            2682"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m2.groupby([\"loop_or_inlining\"]).agg({\"shape_array_key\": \"nunique\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4486cd7c-31d7-4420-ac67-f9783676ede8",
   "metadata": {},
   "source": [
    "#### See how many trips for a shape ID have problematic rows\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "468be3c9-7a24-4f01-84fd-31c137bc45e8",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Number of trips that have at least one row that was divided by 0 \n",
    "# for this shape array key\n",
    "df1 = m2.groupby(['shape_array_key']).agg({'trip_id':'nunique'}).rename(columns = {'trip_id':'trips_with_zero'}).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "4350f540-8f6b-4fb0-8b16-836245c0e44c",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Original number of trips\n",
    "df2 = m1.groupby(['shape_array_key']).agg({'trip_id':'nunique'}).rename(columns = {'trip_id':'all_trips'}).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "ac68bdf7-26a0-4679-9a35-26f8a670018a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df3 = pd.merge(df1, df2, how = \"inner\", on = 'shape_array_key')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "81d443cc-122f-46f1-87ec-dbdc74e0ca6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df3['percent_of_trips_with_problematic_rows'] = df3.trips_with_zero/df3.all_trips * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "314d9baf-de0e-460a-8c29-4504ba94cfa6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count   2682.00\n",
       "mean      82.86\n",
       "std       26.65\n",
       "min        1.52\n",
       "25%       75.00\n",
       "50%      100.00\n",
       "75%      100.00\n",
       "max      100.00\n",
       "Name: percent_of_trips_with_problematic_rows, dtype: float64"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df3['percent_of_trips_with_problematic_rows'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "5de3efe6-2233-4251-93a8-1f8dd6fb2dae",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>shape_array_key</th>\n",
       "      <th>trips_with_zero</th>\n",
       "      <th>all_trips</th>\n",
       "      <th>percent_of_trips_with_problematic_rows</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1397</th>\n",
       "      <td>82f0e3379d90a630b9e42e5ec79e0279</td>\n",
       "      <td>6</td>\n",
       "      <td>7</td>\n",
       "      <td>85.71</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>333</th>\n",
       "      <td>1e469c778efe30b55db3dd93ee1d9946</td>\n",
       "      <td>19</td>\n",
       "      <td>20</td>\n",
       "      <td>95.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2060</th>\n",
       "      <td>c750d9ce7a9e659d5d443f0925e175e7</td>\n",
       "      <td>6</td>\n",
       "      <td>7</td>\n",
       "      <td>85.71</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>908</th>\n",
       "      <td>59626d7e12b3fec5d917b3e052e87d70</td>\n",
       "      <td>17</td>\n",
       "      <td>17</td>\n",
       "      <td>100.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>0485a3b83c38283730ce3e9372baf031</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>66.67</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                       shape_array_key  trips_with_zero  all_trips  \\\n",
       "1397  82f0e3379d90a630b9e42e5ec79e0279                6          7   \n",
       "333   1e469c778efe30b55db3dd93ee1d9946               19         20   \n",
       "2060  c750d9ce7a9e659d5d443f0925e175e7                6          7   \n",
       "908   59626d7e12b3fec5d917b3e052e87d70               17         17   \n",
       "47    0485a3b83c38283730ce3e9372baf031                2          3   \n",
       "\n",
       "      percent_of_trips_with_problematic_rows  \n",
       "1397                                   85.71  \n",
       "333                                    95.00  \n",
       "2060                                   85.71  \n",
       "908                                   100.00  \n",
       "47                                     66.67  "
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df3.sample(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a399d982-e400-43fa-b13f-fecafaa27262",
   "metadata": {},
   "source": [
    "### Investigate \n",
    "#### Stage3: \"vp_pared_stops\"\n",
    "* Keeps only first and last point of a segment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "a2a705af-b588-463b-b6ce-f999b2050208",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_vp_stage3(flagged_df:pd.DataFrame, date:str) -> pd.DataFrame:\n",
    "    \n",
    "    # Subset the dataframe and use it to filter out for only the values of interest\n",
    "    flagged_df = flagged_df[['gtfs_dataset_key', 'trip_id','stop_sequence','shape_array_key']]\n",
    "    vp = pd.read_parquet(f\"{speed_utils.GCS_PATH}vp_pared_stops_{date}\")\n",
    "    \n",
    "    # Merge to filter\n",
    "    vp2 = pd.merge(flagged_df, vp, how = \"inner\", on = ['gtfs_dataset_key', 'trip_id','stop_sequence','shape_array_key'])\n",
    "    \n",
    "    return vp2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "1e36c5fc-ab3f-4129-97f9-ad9472b7d32a",
   "metadata": {},
   "outputs": [],
   "source": [
    "vp2 = load_vp_stage3(subset, analysis_date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "148e75f1-08dd-44c8-8179-319164d8e020",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Check out stop sequences for the trip below that have division by 0\n",
    "# subset[subset.trip_id == \"1088383\"].stop_sequence.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "b4350206-c237-44a3-abce-f8f38cde8117",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Stop sequences that were flagged as division by 0\n",
    "# vp2[vp2.trip_id == \"1088383\"].sort_values(['trip_id', 'stop_sequence','location_timestamp_local'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "aa1e56d1-ec07-436c-8763-7bcf3dcbf7d4",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# All the stop sequences for this trip, even those that are ok\n",
    "# vp_pared[vp_pared.trip_id == \"1088383\"].sort_values(['trip_id', 'stop_sequence','location_timestamp_local'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "22e42aae-9281-4040-ab8c-6a10b93f6cf4",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# All the stop sequences for this trip, even those that are ok\n",
    "# vp_pared[vp_pared.trip_id == \"1088383\"].sort_values(['location_timestamp_local','stop_sequence',])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "0f21f08f-d4eb-4bbd-94d3-f4b031e97cf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def stage3_repeated_timestamps(stage3_df:pd.DataFrame)-> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Look at how many times a time stamp is repeated a route-trip-location.\n",
    "    Each of these 3 combos should have a different time for each \n",
    "    stop sequence or else the vehicle is not changing locations.\n",
    "    \"\"\"\n",
    "    agg = (stage3_df\n",
    "     .groupby(['shape_array_key','trip_id', 'location_timestamp_local'])\n",
    "     .agg({'stop_sequence':'nunique'})\n",
    "     .reset_index()\n",
    "     .rename(columns = {'stop_sequence':'number_of_repeated_timestamps'})\n",
    "    )\n",
    "    \n",
    "    # Only keep timestamps that are repeated more than once\n",
    "    agg = (agg[agg.number_of_repeated_timestamps > 1]).reset_index(drop = True)\n",
    "\n",
    "    return agg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "5ce07566-c1f0-4fa7-9550-2fa07b98dba8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def stage3_repeated_locations(stage3_df:pd.DataFrame):\n",
    "    \"\"\"\n",
    "    Look at how many times a time stamp is repeated for a stop-trip-route combo.\n",
    "    Each of these 3 combos should have a different location for each \n",
    "    stop sequence or else the vehicle is not changing locations.\n",
    "    \"\"\"\n",
    "    # Concat x and y into a string\n",
    "    stage3_df['pair'] = stage3_df.x.astype(str) + '/' + vp2.y.astype(str)\n",
    "    \n",
    "    # Count number of different stops that reference the same location\n",
    "    agg = (stage3_df\n",
    "     .groupby(['shape_array_key','trip_id','pair'])\n",
    "     .agg({'stop_sequence':'nunique'})\n",
    "     .reset_index()\n",
    "     .sort_values('stop_sequence', ascending = False)\n",
    "     .rename(columns = {'stop_sequence':'number_of_repeated_locs'})               \n",
    "    )\n",
    "\n",
    "    # Only keep locations that are repeated more than once\n",
    "    agg = agg[agg.number_of_repeated_locs != 1].reset_index(drop = True)\n",
    "    \n",
    "    return agg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "66e83169-2b4a-4912-bc0e-1a0b3e8deea6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def flag_stage3(flagged_df:pd.DataFrame, date:str) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Flag the errors in stage3\n",
    "    \"\"\"\n",
    "    start = datetime.datetime.now()\n",
    "    print(start)\n",
    "    \n",
    "    # Relevant rows from Vehicle Positions\n",
    "    vp = load_vp_stage3(flagged_df, date)\n",
    "    \n",
    "    # Find repeated timestamps.\n",
    "    multi_timestamps = stage3_repeated_timestamps(vp)\n",
    "    \n",
    "    # Find repeated locations\n",
    "    multi_locs = stage3_repeated_locations(vp)\n",
    "    \n",
    "    # Merge\n",
    "    timestamps_merge_cols = ['shape_array_key','trip_id','location_timestamp_local']\n",
    "    loc_merge_cols =  ['shape_array_key','trip_id','pair']\n",
    "    \n",
    "    # Want everything found in vehicle positions, so do left merges\n",
    "    m1 = (vp\n",
    "          .merge(multi_timestamps, how=\"left\", on= timestamps_merge_cols)\n",
    "          .merge(multi_locs, how=\"left\", on=loc_merge_cols)\n",
    "         )\n",
    "    \n",
    "    drop_cols = ['vp_idx','x','y','hour','activity_date',]\n",
    "    m1 = m1.drop(columns = drop_cols)\n",
    "    \n",
    "    # Flag\n",
    "    def flag(row):\n",
    "        if (row[\"number_of_repeated_timestamps\"] > 1) & (row[\"number_of_repeated_locs\"] > 1):\n",
    "            return \"repeated timestamps & locations\"\n",
    "        elif (row[\"number_of_repeated_timestamps\"] > 1):\n",
    "            return \"repeated timestamps\"\n",
    "        elif (row[\"number_of_repeated_locs\"] > 1):\n",
    "            return \"repeated locations\"\n",
    "        else:\n",
    "            return \"check in stage 2\"\n",
    "        \n",
    "    m1[\"stage3_flag\"] = m1.apply(lambda x: flag(x), axis=1)\n",
    "    \n",
    "    print(m1.stage3_flag.value_counts())\n",
    "    \n",
    "    check_in_stage2 = m1[m1.stage3_flag == \"check in stage 2\"]\n",
    "    print(f\"Have to check {len(check_in_stage2)/len(m1) * 100} % of rows in stage 2\")\n",
    "    \n",
    "    end = datetime.datetime.now()\n",
    "    print(f\"Took {end-start}\")\n",
    "    return m1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "cab32ef3-cc66-40ce-aa19-59631734f539",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-06-26 09:17:07.320679\n",
      "check in stage 2                   538914\n",
      "repeated timestamps                 54883\n",
      "repeated timestamps & locations       107\n",
      "repeated locations                     42\n",
      "Name: stage3_flag, dtype: int64\n",
      "Have to check 90.73451121819154 % of rows in stage 2\n",
      "Took 0:00:27.798047\n"
     ]
    }
   ],
   "source": [
    "m3 = flag_stage3(m2, analysis_date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "68a9dbba-ee6b-42b1-9203-1146d6cd56e9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(593946, 11)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m3.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "21799f42-873e-41bd-b764-42cc297686a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "sort_cols = ['trip_id', 'shape_array_key', 'stop_sequence']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b1876cf-9e8b-4c30-8723-2226133b8e01",
   "metadata": {},
   "source": [
    "### Stage2: \"vp_stop_segment\"\n",
    "* Were the right points kept?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "933b83af-137c-4402-86ac-ebd3f2693ee1",
   "metadata": {},
   "outputs": [],
   "source": [
    "subset_cols = ['gtfs_dataset_key', 'trip_id','stop_sequence','shape_array_key', 'loop_or_inlining', 'stop_id', 'meters_elapsed','sec_elapsed']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55df584c-f36d-4c3d-9760-dd9ecb0471a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "m_cols  = ['gtfs_dataset_key', 'stop_sequence','shape_array_key', 'loop_or_inlining','stop_id']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68fba15b-7c2d-4c1b-a556-286dc4acc4e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find rows that need to be tagged in stage 2\n",
    "stage2_rows = m3[m3.stage3_flag == \"check in stage 2\"].reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f14e0ab1-59d2-42cc-884c-868da650cfa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "stage2_routes = stage2_rows.shape_array_key.unique().tolist() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddb1ae2a-d66a-45b7-9dd9-2b11f7d325b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use flagged df\n",
    "stage2_rows = m2[m2.shape_array_key.isin(stage2_routes)].reset_index(drop = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf64a406-1124-423c-8577-705a21c9b422",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Subset df to filter the vp \n",
    "subset_for_merge = stage2_rows[subset_cols].drop_duplicates().reset_index(drop = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6397dc45-c271-4057-a0d8-1962846d4f94",
   "metadata": {},
   "outputs": [],
   "source": [
    "# What's the diff between stop segments normal/special/and without any notation?\n",
    "stg2 = gpd.read_parquet(f\"{speed_utils.GCS_PATH}stop_segments_{analysis_date}.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45440033-aae7-4f94-9495-5e14529b7c5c",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Merge\n",
    "stg2_m = pd.merge(stg2,\n",
    "                  subset_for_merge, \n",
    "                  how = \"inner\",\n",
    "                  on = m_cols\n",
    "                 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe37945a-26d9-4891-831f-1bb92f85b39e",
   "metadata": {},
   "outputs": [],
   "source": [
    "stage2_rows.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "145e129e-b119-44d9-940b-18ac70c93e0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in ['geometry','geometry_arrowized']:\n",
    "    print(f\"{i}: {stg2_m[i].is_valid.sum()/len(stg2_m)}\")\n",
    "    print(f\"{i}: {len(stg2_m[stg2_m[i].is_empty])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fab30694-9948-4e82-b19b-0814a459c340",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delete out empty geo \n",
    "filtered = stg2[~stg2.geometry_arrowized.is_empty]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d9238cc-99da-49a1-abf1-392e3bd5bcc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "subset_for_merge.sample()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a72b5744-ff22-44b0-834d-5b3718162661",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delete out empty geometry arrowized\n",
    "geo_arrowized = stg2_m[~stg2_m.geometry_arrowized.is_empty]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf7a7ff5-7aca-4c1b-8751-34981d90da15",
   "metadata": {},
   "source": [
    "#### Look at the original routes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97403c1c-e39f-4179-a7bb-2fc8588d3249",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the original dataframe loaded in from merging stage\n",
    "# It's not even flagged. \n",
    "original = pd.merge(filtered, \n",
    "                    subset[subset_cols],\n",
    "                    how = \"inner\", \n",
    "                    on = m_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fad7b35-d1bf-4df2-99d7-b63b2381ba34",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(geo_arrowized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c86b76a0-bc6f-4e85-811c-0076892828da",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Find number of messed up sequences...are the same sequences being hit?\n",
    "\"\"\"\n",
    "(subset_for_merge\n",
    " .groupby(['shape_array_key','stop_sequence'])\n",
    " .agg({'trip_id':'nunique'})\n",
    " .rename(columns = {'trip_id':'number_of_trips_with_problematic_stop_seq'})\n",
    " .sort_values(['shape_array_key','number_of_trips_with_problematic_stop_seq']\n",
    "              , ascending = False)\n",
    ")\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0c6367d-cd6b-44e8-a325-4f2bb1615599",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# subset_for_merge.groupby(['shape_array_key','trip_id']).agg({'stop_sequence':'nunique'})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6cdedf7-c112-40ac-ac3e-81bf1144cefe",
   "metadata": {},
   "source": [
    "#### Look at all the stop sequences vs the ones flagged as 0 for each trip"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44ea63a5-afdb-4030-b3d3-ec8804f3116b",
   "metadata": {},
   "source": [
    "##### 106d979b9a9e6338827a8e1c145e69fd\n",
    "* 1088383\n",
    "* 1088403"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "298f3ddb-0fde-4b90-b9b4-5ea9e45d461d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SEgments that show up have something wrong with them\n",
    "geo_arrowized[geo_arrowized.trip_id == '1088383'].set_geometry(\"geometry_arrowized\").explore('stop_sequence',  style_kwds = {'weight':10})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e42cd7cf-3521-498c-be7d-c08146fe8070",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SEgments that show up have something wrong with them\n",
    "geo_arrowized[geo_arrowized.trip_id == '1088403'].set_geometry(\"geometry_arrowized\").explore('stop_sequence',  style_kwds = {'weight':10})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b28c57bb-95ee-4a27-85c9-12cb0d6f3c19",
   "metadata": {},
   "outputs": [],
   "source": [
    "original[original.trip_id == '1088383'].set_geometry(\"geometry_arrowized\").explore('stop_sequence', style_kwds = {'weight':10})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80f135f9-141e-4e43-9e9e-5e483fcd4f23",
   "metadata": {},
   "source": [
    "#### 0fb4f3627996269dc7075276d3b69e36 \n",
    "* 16939089"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9121bc1-9192-45fa-976a-4f818c4a39a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "geo_arrowized[geo_arrowized.trip_id == '16939089'].set_geometry(\"geometry_arrowized\").explore('stop_sequence', style_kwds = {'weight':10})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4213b966-f843-4d14-ab9a-f2344df1962f",
   "metadata": {},
   "outputs": [],
   "source": [
    "original[original.trip_id == '16939089'].set_geometry(\"geometry_arrowized\").explore('stop_sequence', style_kwds = {'weight':10})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f048e93-5ffc-45e8-8cc2-24d891a9dac3",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### 000624bd8453dbe4f2eb2765b04bcb98 \n",
    "* 1350"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75d7e270-48c5-4421-ae89-50d696604da0",
   "metadata": {},
   "outputs": [],
   "source": [
    "geo_arrowized[geo_arrowized.trip_id == '1359'].set_geometry(\"geometry_arrowized\").explore('stop_sequence', style_kwds = {'weight':10})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ac43a58-184c-41b3-9d53-9e21ef321bc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "geo_arrowized[geo_arrowized.trip_id == '1350'].set_geometry(\"geometry_arrowized\").explore('stop_sequence', style_kwds = {'weight':10})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68a59632-cfbd-43fd-aca4-a502e400a854",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "m2[m2.trip_id == '1350']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60a53891-66cb-40ea-a42a-55ea290c6584",
   "metadata": {},
   "outputs": [],
   "source": [
    "original[original.trip_id == '1350'].set_geometry(\"geometry_arrowized\").explore('stop_sequence', style_kwds = {'weight':10})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dad7c9b8-9025-4d91-bf75-0e23c3ac2a52",
   "metadata": {},
   "outputs": [],
   "source": [
    "summarize(few_routes_cat, high_low_zero)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cde431f9-10ad-484f-b954-dd3c13a6e683",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Draft\n",
    "* Show which stops are excluded from flags\n",
    "* Show how many stops are dropped\n",
    "* Show % of stops that were flagged compared to total stops."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a9ba66d-5421-4170-9201-881ad3704b39",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_back_gcs():\n",
    "    # Read back all the partitioned stuff - grab the file number\n",
    "    # part0.parquet, part1.parquet\n",
    "    start = datetime.datetime.now()\n",
    "    print(f\"Begin: {start}\")\n",
    "    gcs_file_path1 = f\"{speed_utils.GCS_PATH}partitioned_flags\"\n",
    "    file_names_dask = extract_number(gcs_file_path1, \"part\")\n",
    "\n",
    "    # https://www.geeksforgeeks.org/read-multiple-csv-files-into-separate-dataframes-in-python/\n",
    "    # create empty list\n",
    "    all_df = []\n",
    "\n",
    "    # append datasets into the list\n",
    "    for i in range(len(file_names_dask)):\n",
    "        gcs_file_path2 = f\"{gcs_file_path1}/part.\"\n",
    "        temp_df = dd.read_parquet(f\"{gcs_file_path2}{file_names_dask[i]}.parquet\")\n",
    "        all_df.append(temp_df)\n",
    "\n",
    "    final_df = dd.concat(all_df, axis=0).reset_index(drop=True)\n",
    "    print(\"Begin computing\")\n",
    "    final_df = final_df.compute()\n",
    "    print(\"Done computing\")\n",
    "    end = datetime.datetime.now()\n",
    "    print(f\"Finish: {end-start}\")\n",
    "    return final_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8354439b-0ffc-4b3f-8114-05516a8e48ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "def categorize_by_percentile_pandas(\n",
    "    df: pd.DataFrame, column_percentile: str, column_str: str\n",
    ") -> pd.DataFrame:\n",
    "\n",
    "    # Find percentiles\n",
    "    agg1 = (\n",
    "        df.groupby([\"shape_array_key\", \"stop_sequence\"])[column_percentile]\n",
    "        .describe(percentiles=[0.05, 0.95])\n",
    "        .reset_index()\n",
    "        .add_prefix(column_str)\n",
    "    )\n",
    "    \n",
    "    # Merge \n",
    "    m1 = dd.merge(\n",
    "        df,\n",
    "        agg1,\n",
    "        how=\"inner\",\n",
    "        left_on=[\"shape_array_key\", \"stop_sequence\"],\n",
    "        right_on=[\n",
    "            f\"{column_str}shape_array_key\",\n",
    "            f\"{column_str}stop_sequence\",\n",
    "        ],\n",
    "    )\n",
    "    \n",
    "    def percentile(row):\n",
    "\n",
    "        if row[column_percentile] == row[f\"{column_str}mean\"]:\n",
    "            return f\"{column_str} elapsed avg\"\n",
    "        elif row[f\"{column_str}5%\"] < row[column_percentile] <= row[f\"{column_str}95%\"]:\n",
    "            return f\"{column_str} elapsed avg\"\n",
    "        elif row[column_percentile] <= row[f\"{column_str}5%\"]:\n",
    "            return f\"{column_str} elapsed low\"\n",
    "        elif row[column_percentile] > row[f\"{column_str}95%\"]:\n",
    "            return f\"{column_str} elapsed high\"\n",
    "\n",
    "        else:\n",
    "            return f\"{column_str} elapsed avg\"\n",
    "    \n",
    "    \n",
    "    # Apply flags\n",
    "    m1[f\"{column_str}cat\"] = m1.apply(lambda x: percentile(x), axis=1)\n",
    "    \n",
    "    # Delete out any average columns\n",
    "    m1 = m1.loc[m1[f\"{column_str}cat\"] != f\"{column_str} elapsed avg\"].reset_index(drop = True)\n",
    "    \n",
    "    # Clean\n",
    "    m1[f\"{column_str}cat\"] = m1[f\"{column_str}cat\"].str.replace(\"_\", \"\")\n",
    "    \n",
    "    columns_to_keep = [\n",
    "        \"shape_array_key\",\n",
    "        \"gtfs_dataset_key\",\n",
    "        \"_gtfs_dataset_name\",\n",
    "        \"speed_mph\",\n",
    "        \"loop_or_inlining\",\n",
    "        \"stop_sequence\",\n",
    "        \"stop_id\",\n",
    "        \"trip_id\",\n",
    "        \"n_trips\",\n",
    "        \"p20_mph\",\n",
    "        \"p80_mph\",\n",
    "        \"p50_mph\",\n",
    "        \"time_of_day\",\n",
    "        \"meters_elapsed\",\n",
    "        \"sec_elapsed\",\n",
    "        f\"{column_str}5%\",\n",
    "        f\"{column_str}95%\",\n",
    "        f\"{column_str}cat\",\n",
    "    ]\n",
    "    m1 = m1[columns_to_keep]\n",
    "    print(f\"Done with {column_str}\")\n",
    "    \n",
    "    return m1  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18c06e7d-9b09-4573-bdf6-1dc753b3d552",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_number(folder: str, phrase_to_find: str) -> list:\n",
    "    \"\"\"\n",
    "    Extract the numeric portion of a file path.\n",
    "    \"\"\"\n",
    "    files = find_files(folder, phrase_to_find)\n",
    "    all_file_numbers = []\n",
    "    for file in files:\n",
    "        # https://stackoverflow.com/questions/11339210/how-to-get-integer-values-from-a-string-in-python\n",
    "        file_number = \"\".join(i for i in file if i.isdigit())\n",
    "        all_file_numbers.append(file_number)\n",
    "    return all_file_numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dba2d199-47b4-4116-9ddb-adc1259ea3e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find all the parquets again\n",
    "def find_files(folder: str, phrase_to_find: str) -> list:\n",
    "    \"\"\"\n",
    "    Grab a list of files that contain the\n",
    "    phrase inputted.\n",
    "    \"\"\"\n",
    "    # Create a list of all the files in my folder\n",
    "    all_files_in_folder = fs.ls(folder)\n",
    "    my_files = [i for i in all_files_in_folder if phrase_to_find in i]\n",
    "\n",
    "    # String to add to read the files\n",
    "    my_string = \"gs://\"\n",
    "    my_files = [my_string + i for i in my_files]\n",
    "\n",
    "    # Extract digit of parquet\n",
    "    return my_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "354cf0e3-7b2c-4403-bef9-74533691a0e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def categorize_by_percentile(\n",
    "    df: pd.DataFrame, column_percentile: str, column_str: str\n",
    ") -> dd.DataFrame:\n",
    "    \n",
    "    # Find percentiles\n",
    "    agg1 = (\n",
    "        df.groupby([\"shape_array_key\", \"stop_sequence\"])[column_percentile]\n",
    "        .describe(percentiles=[0.05, 0.95])\n",
    "        .reset_index()\n",
    "        .add_prefix(column_str)\n",
    "    )\n",
    "    \n",
    "    \n",
    "    # Convert to dask because it takes a very long time\n",
    "    agg1_dask = dd.from_pandas(agg1, npartitions=1)\n",
    "    df_dask = dd.from_pandas(df, npartitions=1)\n",
    "\n",
    "    # Merge using dask\n",
    "    merge1_dask = dd.merge(\n",
    "        df_dask,\n",
    "        agg1_dask,\n",
    "        how=\"inner\",\n",
    "        left_on=[\"shape_array_key\", \"stop_sequence\"],\n",
    "        right_on=[\n",
    "            f\"{column_str}shape_array_key\",\n",
    "            f\"{column_str}stop_sequence\",\n",
    "        ],\n",
    "    )\n",
    "\n",
    "    def percentile(row):\n",
    "\n",
    "        if row[column_percentile] == row[f\"{column_str}mean\"]:\n",
    "            return f\"{column_str} elapsed avg\"\n",
    "        elif row[column_percentile] <= row[f\"{column_str}5%\"]:\n",
    "            return f\"{column_str} elapsed low\"\n",
    "        elif row[column_percentile] == 0:\n",
    "            return f\"{column_str} elapsed is 0\"\n",
    "        elif row[f\"{column_str}5%\"] < row[column_percentile] <= row[f\"{column_str}95%\"]:\n",
    "            return f\"{column_str} elapsed avg\"\n",
    "        elif row[column_percentile] > row[f\"{column_str}95%\"]:\n",
    "            return f\"{column_str} elapsed high\"\n",
    "\n",
    "        else:\n",
    "            return \"other\"\n",
    "\n",
    "    merge1_dask[f\"{column_str}cat\"] = merge1_dask.apply(\n",
    "        lambda x: percentile(x), axis=1, meta=(f\"{column_str}cat\", \"string\")\n",
    "    )\n",
    "    \n",
    "    # Filter for only unsually high and low stuff\n",
    "    merge1_dask = merge1_dask[merge1_dask[f\"{column_str}cat\"].isin([f\"{column_str} elapsed high\", f\"{column_str} elapsed low\"]).reset_index(drop = True)\n",
    "                              \n",
    "    # Clean\n",
    "    merge1_dask[f\"{column_str}cat\"] = merge1_dask[f\"{column_str}cat\"].str.replace(\n",
    "        \"_\", \"\"\n",
    "    )\n",
    "\n",
    "    columns_to_keep = [\n",
    "        \"shape_array_key\",\n",
    "        \"gtfs_dataset_key\",\n",
    "        \"_gtfs_dataset_name\",\n",
    "        \"speed_mph\",\n",
    "        \"loop_or_inlining\",\n",
    "        \"stop_sequence\",\n",
    "        \"stop_id\",\n",
    "        \"trip_id\",\n",
    "        \"n_trips\",\n",
    "        \"p20_mph\",\n",
    "        \"p80_mph\",\n",
    "        \"p50_mph\",\n",
    "        \"time_of_day\",\n",
    "        \"meters_elapsed\",\n",
    "        \"sec_elapsed\",\n",
    "        f\"{column_str}5%\",\n",
    "        f\"{column_str}95%\",\n",
    "        f\"{column_str}cat\",\n",
    "    ]\n",
    "    merge1_dask = merge1_dask[columns_to_keep]\n",
    "    print(f\"Done with {column_str}\")\n",
    "    return merge1_dask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d80535fb-8648-4216-918f-76e0484ba3ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "def flag_round1(row):\n",
    "    if (row[\"meters_elapsed\"] == 0) & (row[\"sec_elapsed\"] == 0):\n",
    "        return \"division by 0\"\n",
    "    elif row[\"meters_cat\"] == \"meters elapsed low\":\n",
    "        return \"meters too low\"\n",
    "    elif row[\"seconds_cat\"] == \"seconds elapsed high\":\n",
    "        return \"seconds too high\"\n",
    "    else:\n",
    "        return \"ok\"\n",
    "    \n",
    "#def flag_round2(row):\n",
    "#    if (row[\"meters_elapsed\"] == 0) & (row[\"sec_elapsed\"] == 0):\n",
    "#        return \"division by 0\"\n",
    "#    else:\n",
    "#        return \"meters/seconds are filled but flagged\"\n",
    "\n",
    "def categorize_meters_speeds_dask(df):\n",
    "    start = datetime.datetime.now()\n",
    "    print(f\"Begin: {start}\")\n",
    "\n",
    "    # Find percentiles\n",
    "    df.speed_mph = df.speed_mph.fillna(0)\n",
    "\n",
    "    # These are now dask dataframes\n",
    "    ddf_meters = categorize_by_percentile(df, \"meters_elapsed\", \"meters_\")\n",
    "    ddf_seconds = categorize_by_percentile(df, \"sec_elapsed\", \"seconds_\")\n",
    "\n",
    "    merge_cols = [\n",
    "        \"shape_array_key\",\n",
    "        \"gtfs_dataset_key\",\n",
    "        \"_gtfs_dataset_name\",\n",
    "        \"speed_mph\",\n",
    "        \"loop_or_inlining\",\n",
    "        \"stop_sequence\",\n",
    "        \"stop_id\",\n",
    "        \"n_trips\",\n",
    "        \"p20_mph\",\n",
    "        \"p80_mph\",\n",
    "        \"p50_mph\",\n",
    "        \"meters_elapsed\",\n",
    "        \"sec_elapsed\",\n",
    "        \"trip_id\",\n",
    "        \"time_of_day\",\n",
    "    ]\n",
    "\n",
    "    # Merge using dask\n",
    "    m1 = dd.merge(ddf_meters, ddf_seconds, how=\"inner\", on=merge_cols)\n",
    "\n",
    "    # Apply flags\n",
    "    m1[\"flag\"] = m1.apply(lambda x: flag_round1(x), axis=1, meta=(\"flag\", \"string\"))\n",
    "    print(\"Apply first round of flags\")\n",
    "\n",
    "    # Filter out for projects that are ok, retag for zeroes\n",
    "    m2 = m1[m1.flag != \"ok\"].reset_index()\n",
    "\n",
    "    # Apply flag for zeroes\n",
    "    m2[\"flag_division_0\"] = m2.apply(\n",
    "        lambda x: flag_round2(x), axis=1, meta=(\"flag\", \"string\")\n",
    "    )\n",
    "    print(\"Apply second round of flags\")\n",
    "\n",
    "    # Replace values in the original flag\n",
    "    # https://stackoverflow.com/questions/54302694/updating-the-values-of-a-column-in-a-dask-dataframe-based-on-some-condition-on-s\n",
    "    condition = m2.flag_division_0 == \"division by 0\"\n",
    "    m2[\"flag\"] = m2[\"flag\"].mask(condition, m2.flag_division_0)\n",
    "    print(\"Done flagging\")\n",
    "\n",
    "    # Print value counts\n",
    "    # print(f\"breakout of rows after separating out for 0: \\n {m2.flag.value_counts().compute()}\")\n",
    "\n",
    "    # Filter for only projects that are divided by 0\n",
    "    # m2 = m2[m2.flag == \"division by 0\"].reset_index()\n",
    "    # Delete older column\n",
    "    m2 = m2.drop(columns=[\"flag_division_0\", \"level_0\", \"index\"])\n",
    "    print(\"Drop columns\")\n",
    "\n",
    "    # Save\n",
    "    # m2 =  m2.repartition(partition_size=\"5MB\")\n",
    "    # m2.to_parquet(f\"{speed_utils.GCS_PATH}partitioned_flags\", overwrite = True)\n",
    "    print(\"Saved\")\n",
    "\n",
    "    end = datetime.datetime.now()\n",
    "    print(f\"Finish: {end-start}\")\n",
    "\n",
    "    return m2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34840f61-1857-4f5c-b3a6-abcac18af0db",
   "metadata": {},
   "outputs": [],
   "source": [
    "trips = list(equal_sampling.trip_id.unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "759ac80e-9c98-4c94-add3-859ab998e038",
   "metadata": {},
   "outputs": [],
   "source": [
    "stops = list(equal_sampling.stop_id.unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94620388-876e-4e6d-80ab-ac68eb1061c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot some of the trips\n",
    "sample_data = few_routes_cat[few_routes_cat.trip_id.isin(trips)].reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "417804f1-9e24-4ac6-9c96-95f4da8f9693",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "339a67c7-3634-43d4-b865-b10119507c44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample_data2 = sample_data[['shape_array_key','gtfs_dataset_key','trip_id']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39231b51-f0ab-4bbe-80d0-8a2081849e50",
   "metadata": {},
   "outputs": [],
   "source": [
    "plotting = sample_data.melt(\n",
    "    id_vars=[\n",
    "        \"_gtfs_dataset_name\",\n",
    "        \"shape_array_key\",\n",
    "        \"trip_id\",\n",
    "        \"stop_sequence\",\n",
    "        \"gtfs_dataset_key\",\n",
    "        \"loop_or_inlining\",\n",
    "        \"n_trips\",\n",
    "        \"meters_elapsed\",\n",
    "        \"meters_cat\",\n",
    "        \"seconds_cat\",\n",
    "        \"sec_elapsed\",\n",
    "        \"flag\",\n",
    "        \"p20_speed_mph\",\n",
    "        \"p80_speed_mph\",\n",
    "        \"median_speed_mph\",\n",
    "    ],\n",
    "    value_vars=[\"speed_mph\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "959bbe35-8c21-4795-b418-745a0caa94e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean\n",
    "plotting = threshold_utils.pre_clean(plotting)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd343cdd-acf3-4433-a702-8c3eb53ba1f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "plotting[\"Dropdown Menu\"] = plotting[\"Gtfs Dataset Name\"] + \" \" + plotting[\"Trip Id\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "072f39d9-4bf9-4efc-a393-24f096cecf7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def alt_dropdown(df, col_for_dropdown: str, dropdown_menu_title: str):\n",
    "    # Create dropdown menu\n",
    "    # Exclude \"none\" operators which are only scheduled data\n",
    "    df = df.loc[df[col_for_dropdown] != \"None\"][[col_for_dropdown]]\n",
    "    dropdown_list = df[col_for_dropdown].unique().tolist()\n",
    "\n",
    "    # Show only first operator by default\n",
    "    initialize_first_op = sorted(dropdown_list)[0]\n",
    "    input_dropdown = alt.binding_select(\n",
    "        options=sorted(dropdown_list), name=dropdown_menu_title\n",
    "    )\n",
    "\n",
    "    selection = alt.selection_single(\n",
    "        name=dropdown_menu_title,\n",
    "        fields=[col_for_dropdown],\n",
    "        bind=input_dropdown,\n",
    "        init={col_for_dropdown: initialize_first_op},\n",
    "    )\n",
    "\n",
    "    return selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b919e59-03e8-426f-b9b3-4468d5b1b06b",
   "metadata": {},
   "outputs": [],
   "source": [
    "selection_test = alt_dropdown(plotting, \"Dropdown Menu\", \"Route\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7b9d14f-b708-4426-a1b8-4afb4e7c95ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "(\n",
    "    threshold_utils.chart_size(\n",
    "        alt.Chart(plotting)\n",
    "        .mark_tick(\n",
    "            size=15,\n",
    "            thickness=5,\n",
    "        )\n",
    "        .encode(\n",
    "            x=\"Stop Sequence:N\",\n",
    "            y=\"Value:Q\",\n",
    "            color=alt.Color(\n",
    "                \"Flag:N\", scale=alt.Scale(range=cp.CALITP_CATEGORY_BOLD_COLORS)\n",
    "            ),\n",
    "            tooltip=plotting.columns.tolist(),\n",
    "        )\n",
    "        .interactive(),\n",
    "        1100,\n",
    "        400,\n",
    "    )\n",
    "    .add_selection(selection_test)\n",
    "    .transform_filter(selection_test)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ec90906-3364-440c-a951-eb5f2b1e84c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "stop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "140112e0-8ee0-481d-848c-4db091460418",
   "metadata": {},
   "outputs": [],
   "source": [
    "type(vehicle_positions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d843325a-f33c-40de-a3cb-befed24d645e",
   "metadata": {},
   "outputs": [],
   "source": [
    "vehicle_positions2 = vehicle_positions[\n",
    "    vehicle_positions.trip_id.isin(trips)\n",
    "].reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8840809b-dd6f-4c0e-a68b-0a37f508df14",
   "metadata": {},
   "outputs": [],
   "source": [
    "vehicle_positions2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "896b5c73-0835-4ee7-a4f2-9d960778fe35",
   "metadata": {},
   "outputs": [],
   "source": [
    "gdf1 = pd.merge(\n",
    "    vehicle_positions2,\n",
    "    sample_data,\n",
    "    how=\"inner\",\n",
    "    on=[\"gtfs_dataset_key\", \"_gtfs_dataset_name\", \"trip_id\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3725d07d-7b9f-483b-aa56-8d428b9f3d11",
   "metadata": {},
   "outputs": [],
   "source": [
    "gdf1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c82d7834-567b-4de4-b465-aea8c1a62715",
   "metadata": {},
   "outputs": [],
   "source": [
    "gdf1 = gdf1[gdf1.stop_id.isin(stops)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a86cec8e-e7fd-48ce-9692-c766df2b68e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "gdf1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bf3c599-2aec-4ae7-b8c8-53a4eff8795a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "gdf1[\n",
    "    [\n",
    "        \"geometry\",\n",
    "        \"stop_id\",\n",
    "        \"stop_sequence\",\n",
    "        \"_gtfs_dataset_name\",\n",
    "        \"shape_array_key\",\n",
    "        \"speed_mph\",\n",
    "        \"flag\",\n",
    "    ]\n",
    "].explore(\"flag\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6d7831f-aed2-4e87-aae1-8ab6ddc08666",
   "metadata": {},
   "outputs": [],
   "source": [
    "stop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aae6b6c0-ce47-4ca1-b104-68b39ebcf2ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "high_low_zero2 = high_low_zero.melt(\n",
    "    id_vars=[\n",
    "        \"_gtfs_dataset_name\",\n",
    "        \"shape_array_key\",\n",
    "        \"trip_id\",\n",
    "        \"stop_sequence\",\n",
    "        \"gtfs_dataset_key\",\n",
    "        \"loop_or_inlining\",\n",
    "        \"n_trips\",\n",
    "        \"meters_cat\",\n",
    "        \"seconds_cat\",\n",
    "        \"unusual_flag\",\n",
    "        \"time_of_day\",\n",
    "    ],\n",
    "    value_vars=[\"median_speed_mph\", \"speed_mph\", \"p20_speed_mph\", \"p80_speed_mph\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "767abd20-d030-42d3-b85f-6d3023d69b8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "high_low_zero2 = high_low_zero2.drop_duplicates(\n",
    "    subset=[\n",
    "        \"loop_or_inlining\",\n",
    "        \"shape_array_key\",\n",
    "        \"stop_sequence\",\n",
    "        \"time_of_day\",\n",
    "        \"variable\",\n",
    "        \"value\",\n",
    "    ]\n",
    ").reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52e7ee5a-a40e-423e-ba4b-dea14de17982",
   "metadata": {},
   "outputs": [],
   "source": [
    "high_low_zero2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23e7f746-b1b5-402f-92fd-dbc74840e013",
   "metadata": {},
   "outputs": [],
   "source": [
    "merge1.shape_array_key.nunique(), high_low_zero.shape_array_key.nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a25f067-956e-46a7-aa7a-5abf57e662f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean\n",
    "high_low_zero2 = threshold_utils.pre_clean(high_low_zero2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5eea02c-05ec-4707-a06d-9de1864e8fbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add dropdown menu\n",
    "high_low_zero2[\"Dropdown Menu\"] = (\n",
    "    high_low_zero2[\"Gtfs Dataset Name\"] + \" \" + high_low_zero2[\"Shape Array Key\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "571b471f-4a66-474f-8900-c3eaffde441e",
   "metadata": {},
   "outputs": [],
   "source": [
    "high_low_zero2[\"Route Type\"] = \"Route Type: \" + high_low_zero2[\n",
    "    \"Loop Or Inlining\"\n",
    "].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7b429be-057c-4692-927e-92107b015ae6",
   "metadata": {},
   "outputs": [],
   "source": [
    "selection_test = alt_dropdown(high_low_zero2, \"Dropdown Menu\", \"Route\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa0fad8b-b49e-48be-8070-adaf6e63d541",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://github.com/altair-viz/altair/issues/1168\n",
    "title = (\n",
    "    alt.Chart(high_low_zero2)\n",
    "    .mark_text(dy=-40, size=15, fontWeight=\"normal\")\n",
    "    .encode(\n",
    "        text=\"Route Type:N\",\n",
    "    )\n",
    "    .add_selection(selection_test)\n",
    "    .transform_filter(selection_test)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1042a774-165c-4f7e-bfc9-c4d4980bd29b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"total_stops_altair = (\n",
    "    alt.Chart(stop_info)\n",
    "    .mark_text(dy=-40, size=15, fontWeight=\"normal\")\n",
    "    .encode(\n",
    "        text=\"Percentage Of Unusual Stops:N\",\n",
    "    )\n",
    "    .add_selection(selection_test)\n",
    "    .transform_filter(selection_test)\n",
    ")\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6067e93b-3519-45fc-b027-11cbcc82d80f",
   "metadata": {},
   "outputs": [],
   "source": [
    "main_chart = (\n",
    "    threshold_utils.chart_size(\n",
    "        alt.Chart(high_low_zero2)\n",
    "        .mark_tick(\n",
    "            size=15,\n",
    "            thickness=5,\n",
    "        )\n",
    "        .encode(\n",
    "            x=\"Stop Sequence:N\",\n",
    "            y=\"Value:Q\",\n",
    "            color=alt.Color(\n",
    "                \"Variable:N\", scale=alt.Scale(range=cp.CALITP_CATEGORY_BRIGHT_COLORS)\n",
    "            ),\n",
    "            tooltip=high_low_zero2.columns.tolist(),\n",
    "        )\n",
    "        .interactive(),\n",
    "        1100,\n",
    "        400,\n",
    "    )\n",
    "    .add_selection(selection_test)\n",
    "    .transform_filter(selection_test)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da0d6aad-26c3-439b-93d2-ba5a3abac77d",
   "metadata": {},
   "outputs": [],
   "source": [
    "high_low_zero2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c77e2503-e211-48cc-a220-d96f82ab72df",
   "metadata": {},
   "outputs": [],
   "source": [
    "(title & total_stops_altair | main_chart)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f709e13-aa1e-44da-9027-dfafaead5dad",
   "metadata": {},
   "outputs": [],
   "source": [
    "high_low_zero.shape_array_key.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a2bda8b-d48c-44f3-a928-08aca894c565",
   "metadata": {},
   "outputs": [],
   "source": [
    "chart2 = threshold_utils.chart_size(chart2, 75, 200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd20293d-7c43-42c4-b053-de945860b6f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "chart2 = chart2.add_selection(selection_test).transform_filter(selection_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44dc6896-e95d-42df-bbd5-f2bb2c2a2cc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "title = threshold_utils.chart_size(title, 20, 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d651c8c6-179c-4157-a671-11006cb419df",
   "metadata": {},
   "outputs": [],
   "source": [
    "alt.data_transformers.enable(\"default\", max_rows=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e753711-8506-4939-8d9d-22566a641988",
   "metadata": {},
   "outputs": [],
   "source": [
    "title & (chart1.interactive() & chart2.interactive())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
