{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c7feec3-aa18-42ab-94b9-cab4be608152",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "import _speed_utils as speed_utils\n",
    "import _threshold_utils as threshold_utils\n",
    "import altair as alt\n",
    "import dask.dataframe as dd\n",
    "import numpy as np\n",
    "import geopandas as gpd\n",
    "import pandas as pd\n",
    "from segment_speed_utils import gtfs_schedule_wrangling, helpers, segment_calcs\n",
    "from segment_speed_utils.project_vars import (\n",
    "    COMPILED_CACHED_VIEWS,\n",
    "    PROJECT_CRS,\n",
    "    SEGMENT_GCS,\n",
    "    #analysis_date,\n",
    ")\n",
    "from scripts import A1_sjoin_vp_segments\n",
    "from shared_utils import calitp_color_palette as cp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0108ae4a-4518-4487-85f7-a5faa3e9cbf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.options.display.max_columns = 100\n",
    "pd.options.display.float_format = \"{:.2f}\".format\n",
    "pd.set_option(\"display.max_rows\", None)\n",
    "pd.set_option(\"display.max_colwidth\", None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c130ab8b-80ac-4edf-bb59-715a793c15a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "CONFIG_PATH = './scripts/config.yml'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c65c2fd-4eb6-46d0-8007-e91096133954",
   "metadata": {},
   "outputs": [],
   "source": [
    "STOP_SEG_DICT = helpers.get_parameters(CONFIG_PATH, \"stop_segments\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80014c5e-695d-4280-89cd-4e7e2bb3d302",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding analysis_date here since there aren't any files for June yet\n",
    "analysis_date = '2023-05-17'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "778a2aa1-d73b-4ebf-89e2-8f9eb5ca5aa1",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Observations (7/12)\n",
    "* Step 1: Flagging\n",
    "    * There are 2,704,812 rows in the dataframe original. About 10% of those rows are flagged as having zeroes in meters elapsed and seconds elapsed. \n",
    "    * There are around 4566 routes. About 57% of these routes had at least one trip with one or more rows flagged as zero.\n",
    "* I took 2 passes at trying to understand why both these columns recorded zeroes.\n",
    "\n",
    "* Step 2: `vp_pared_stops`. \n",
    "    * I grouped `vp_pared_stops` by 'shape_array_key','trip_id', and 'location_timestamp_local' OR `x` and `y`. I counted the number of unique stop sequences after grouping. If this trio had more than one unique stop sequence, that meant the timestamp or location recorded between sequences was duplicated.\n",
    "    * Only around 9% of rows were flagged as having (obviously) repeated timestamps and locations. \n",
    "    * For all of these rows, both the timestamp and location were duplicated. \n",
    "    * All the routes that were flagged in step one needed a further look in step 3.\n",
    "    \n",
    "* Step 3: `vp_usable`\n",
    "    * For one route and trip, find: all the recorded vehicle positions, sjoin of vps to segments,\n",
    "    and the first and last points kept. \n",
    "    * Plot the three gdfs in a map to visually inspect what's happening.\n",
    "    * Compare the sample route and trips with the trip with the highest percentage of non division by 0 rows to see what's going on. \n",
    "    \n",
    "* Buckets of errors (all based on `stage0 vp`). \n",
    "    * There is only one recorded point in that segment in the raw data.\n",
    "        * Fix: use the timestamp that comes after it.\n",
    "        * <img src= \"./speeds_images/only_one_pt_collected.png\" width = 300>\n",
    "    * Points are shared between segments\n",
    "        * Use p20/p50/p80.\n",
    "        * <img src= \"./speeds_images/shared_vp.png\" width = 300>\n",
    "    * Points recorded are really far out and they don't touch the buffered segments.\n",
    "        * Fix: figure out % of vehicle positions that are too far out.\n",
    "        * <img src= \"./speeds_images/dots_not_on_seg.png\" width = 300>\n",
    "    * No data captured for that segment at all.\n",
    "        * Use p20/p50/p80. \n",
    "        * <img src= \"./speeds_images/no_dots_collected.png\" width = 300> "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35c1ba95-5929-4a7f-bfae-f6d126622104",
   "metadata": {},
   "source": [
    "### Merging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f0c5f4f-f419-42a8-8527-7060ed412092",
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_all_speeds(analysis_date:str) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Merge avg_speeds_stop_segments and\n",
    "    speed_stops parquets.\n",
    "    \n",
    "    Args:\n",
    "        date: analysis date\n",
    "    \"\"\"\n",
    "    STG5_FILE = STOP_SEG_DICT['stage5']\n",
    "    # Open up avg speeds\n",
    "    avg_speeds = pd.read_parquet(f\"{SEGMENT_GCS}{STG5_FILE}_{analysis_date}.parquet\")\n",
    "    avg_speeds = avg_speeds.drop(columns=[\"geometry\", \"district\", \"district_name\"])\n",
    "    \n",
    "    # Filter  for all day flags\n",
    "    avg_speeds = avg_speeds[avg_speeds.time_of_day == 'all_day'].reset_index(drop = True)\n",
    "    \n",
    "    STG4_FILE = STOP_SEG_DICT['stage4']\n",
    "    # Open up speeds\n",
    "    speeds = pd.read_parquet(f\"{SEGMENT_GCS}{STG4_FILE}_{analysis_date}\")\n",
    "    \n",
    "    # Merge\n",
    "    merge_cols = ['gtfs_dataset_key','shape_array_key', 'stop_sequence']\n",
    "    m1 = pd.merge(avg_speeds, speeds, on = merge_cols, how = 'inner')\n",
    "    \n",
    "    m1 = m1.drop_duplicates().reset_index(drop = True)\n",
    "    \n",
    "    return m1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbae548e-a895-467f-a6ff-09f0c5f7550f",
   "metadata": {},
   "outputs": [],
   "source": [
    "m1 = merge_all_speeds(analysis_date)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "898e3546-5298-4c4f-87d0-ee1d1a10f07d",
   "metadata": {},
   "source": [
    "### Categorize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e81e59fd-cc2f-408e-9148-1a1055425fc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def categorize_by_percentile_pandas(\n",
    "    df: pd.DataFrame, \n",
    "    column_percentile: str, \n",
    "    column_str: str) -> pd.DataFrame:\n",
    "\n",
    "    # Find percentiles\n",
    "    p5 = df[column_percentile].quantile(0.05).astype(float)\n",
    "    p95 = df[column_percentile].quantile(0.95).astype(float)\n",
    "    \n",
    "    # Keep the most extremes of both ends across the entire dataframe\n",
    "    def rate(row):\n",
    "        if ((row[column_percentile] >= 0) and (row[column_percentile] <= p5)):\n",
    "            return f\"{column_str} is low\"\n",
    "        elif (row[column_percentile] >= p95):\n",
    "               return f\"{column_str} is high\"\n",
    "        else:\n",
    "            return f\"{column_str} is avg\"\n",
    "    \n",
    "    # Apply flags\n",
    "    df[f\"{column_str}cat\"] = df.apply(lambda x: rate(x), axis=1)\n",
    "    \n",
    "    # Clean\n",
    "    df[f\"{column_str}cat\"] = df[f\"{column_str}cat\"].str.replace(\"_\", \"\")\n",
    "    \n",
    "    return df  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fa50477-c176-4439-aca9-b6108bb7b5fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def categorize_meters_speeds_pandas(analysis_date:str)-> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Categorize the meters and seconds elapsed\n",
    "    rows as low/high/division by 0 across the entire \n",
    "    dataframe.\n",
    "    \"\"\"\n",
    "    start = datetime.datetime.now()\n",
    "    print(start)\n",
    "    \n",
    "    # Load original dataframe\n",
    "    df = merge_all_speeds(analysis_date)\n",
    "    print(f\"There are {len(df)} rows in the original dataframe\") \n",
    "    \n",
    "    # Categorize\n",
    "    df1 = categorize_by_percentile_pandas(df, \"meters_elapsed\", \"meters_\")\n",
    "    df2 = categorize_by_percentile_pandas(df1, \"sec_elapsed\", \"sec_\")\n",
    "  \n",
    "    # Find size of categories\n",
    "    print(df2.groupby(['sec_cat','meters_cat']).size())\n",
    "    \n",
    "    def flag_round(row):\n",
    "        if (row[\"meters_elapsed\"] == 0) & (row[\"sec_elapsed\"] == 0):\n",
    "            return \"division by 0\"\n",
    "        elif row[\"meters_cat\"] == \"meters is low\":\n",
    "            return \"meters too low\"\n",
    "        elif row[\"sec_cat\"] == \"sec is high\":\n",
    "            return \"seconds too high\"\n",
    "        else:\n",
    "            return \"ok\"\n",
    "        \n",
    "    df2[\"flag\"] = df2.apply(lambda x: flag_round(x), axis=1)\n",
    "    print(df2.flag.value_counts()/len(df2)*100)\n",
    "    \n",
    "    end = datetime.datetime.now()\n",
    "    print(f\"Took {end-start}\")\n",
    "    return df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de3cde0f-2a15-4ed3-bcdf-4018669d0936",
   "metadata": {},
   "outputs": [],
   "source": [
    "def keep_only_zeroes(analysis_date:str)-> pd.DataFrame:\n",
    "    \n",
    "    df = categorize_meters_speeds_pandas(analysis_date)\n",
    "    \n",
    "    # Filter out for only division by 0 \n",
    "    df2 = df[(df.flag == 'division by 0')].reset_index(drop = True)\n",
    "    \n",
    "    # Print out some stuff of interest\n",
    "    print(f\"{df.trip_id.nunique()-df2.trip_id.nunique()} unique trips flagged.\")\n",
    "    print(f\"{df2.shape_array_key.nunique()} routes flagged out of {df.shape_array_key.nunique()}.\")\n",
    "    print(f\"{df2.shape_array_key.nunique()/df.shape_array_key.nunique() * 100} routes have 1+ row that has zeroes for meters/sec elapsed\")\n",
    "    print(f\"{df._gtfs_dataset_name.nunique()-df2._gtfs_dataset_name.nunique()} operators are not flagged.\")\n",
    "    \n",
    "    return df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c5107cb-c574-449b-95b6-fb205f38502e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    " m2 = keep_only_zeroes(analysis_date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83036ccc-7339-42c2-b1f7-183734253c21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# m2.groupby([\"loop_or_inlining\"]).agg({\"shape_array_key\": \"nunique\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4486cd7c-31d7-4420-ac67-f9783676ede8",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### See how many trips for a shape ID have at least one problematic row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41f16d40-8d02-4329-8fca-e97384235b58",
   "metadata": {},
   "outputs": [],
   "source": [
    "def problematic_trips_percentage(flagged:pd.DataFrame)->pd.DataFrame:\n",
    "    \"\"\"\n",
    "    See the % of trips for a route that has \n",
    "    at least one row that is divided by 0.\n",
    "    The lower the %, the better. \n",
    "    \"\"\"\n",
    "    m1 = merge_all_speeds(analysis_date)\n",
    "    # Number of trips that have at least one row that was divided by 0 \n",
    "    # for this shape array key\n",
    "    df1 = flagged.groupby(['shape_array_key']).agg({'trip_id':'nunique'}).rename(columns = {'trip_id':'trips_with_zero'}).reset_index()\n",
    "    \n",
    "    # Original number of trips\n",
    "    df2 = m1.groupby(['shape_array_key']).agg({'trip_id':'nunique'}).rename(columns = {'trip_id':'all_trips'}).reset_index()\n",
    "    \n",
    "    # Merge\n",
    "    m1 = pd.merge(df2, df1, how = \"left\", on = 'shape_array_key')\n",
    "    \n",
    "    # Clean\n",
    "    m1['percent_of_trips_with_problematic_rows'] = (m1.trips_with_zero/m1.all_trips * 100).fillna(0)\n",
    "    m1.trips_with_zero = m1.trips_with_zero.fillna(0)\n",
    "    \n",
    "    print(f\"{len(m1[m1.percent_of_trips_with_problematic_rows == 0])/m1.shape_array_key.nunique() * 100}% of routes have 1+ division by 0 row\")\n",
    "    return m1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b517876b-9250-4b64-a0e6-117e49a7b256",
   "metadata": {},
   "outputs": [],
   "source": [
    "trips_count = problematic_trips_percentage(m2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a90334ea-95d5-43c5-b65c-c265e397e694",
   "metadata": {},
   "outputs": [],
   "source": [
    "# trips_count.sample(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "747fb981-c8cf-4025-9448-e20ee580d0ed",
   "metadata": {},
   "source": [
    "#### Find version of the route with the most populated info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff29857e-4cef-49f4-9c66-3a236c68f705",
   "metadata": {},
   "outputs": [],
   "source": [
    "def route_most_populated(analysis_date:str)-> pd.DataFrame:\n",
    "    \n",
    "    flagged = categorize_meters_speeds_pandas(analysis_date)\n",
    "    # First aggregation to count number of stops by flag\n",
    "    agg1 = (flagged\n",
    "        .groupby(['gtfs_dataset_key','shape_array_key','trip_id','flag'])\n",
    "        .agg({'stop_sequence':'nunique'})\n",
    "        .rename(columns = {'stop_sequence':'number_of_rows'})\n",
    "        .reset_index()\n",
    "       )\n",
    "    \n",
    "    # Create separate cols for the number of rows that are ok and rows that are division by 0\n",
    "    # https://stackoverflow.com/questions/49161120/set-value-of-one-pandas-column-based-on-value-in-another-column\n",
    "    agg1['division_by_zero'] = None\n",
    "    agg1['ok'] = None\n",
    "    agg1['division_by_zero'] = np.where(agg1.flag == 'division by 0', agg1.number_of_rows, agg1.division_by_zero)\n",
    "    agg1['ok'] = np.where(agg1.flag != 'division by 0', agg1.number_of_rows, agg1.ok)\n",
    "    agg1['division_by_zero'] = agg1['division_by_zero'].fillna(0)\n",
    "    agg1['ok'] = agg1['ok'].fillna(0)\n",
    "    \n",
    "    # Aggregate again to simplify the df \n",
    "    agg1 = agg1.drop(columns = ['flag','number_of_rows'])\n",
    "    agg2 = (agg1\n",
    "            .groupby(['gtfs_dataset_key','shape_array_key','trip_id'])\n",
    "            .agg({'division_by_zero':'sum','ok':'sum'})\n",
    "            .reset_index()\n",
    "           )\n",
    "    \n",
    "    # Find total rows for that trip\n",
    "    agg2['total_rows'] = agg2.division_by_zero + agg2.ok\n",
    "    \n",
    "    # Find total % of rows that are ok\n",
    "    agg2['percent_of_ok_rows'] = (agg2.ok/agg2.total_rows * 100)\n",
    "    \n",
    "    # Only keep the route/trip with the highest % of ok rows\n",
    "    agg2 = (agg2\n",
    "            .sort_values(['gtfs_dataset_key','shape_array_key','percent_of_ok_rows'], ascending = False)\n",
    "            .drop_duplicates(['shape_array_key', 'gtfs_dataset_key'])\n",
    "            .reset_index(drop = True)\n",
    "           )\n",
    "    \n",
    "    return agg2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bf38de2-5695-405e-aa4d-65d02f3e549d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "route_most_populated_df = route_most_populated(analysis_date)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e46f4ff-6182-42ba-a4fd-faa03eba3f04",
   "metadata": {},
   "source": [
    "#### One function\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72135ced-c5ba-4788-b287-a68ffc34ee00",
   "metadata": {},
   "outputs": [],
   "source": [
    "def stage_flagging(analysis_date:str)->pd.DataFrame:\n",
    "    \n",
    "    # Load original but merged avg speeds and speed_stop_segments\n",
    "    original_df = merge_all_speeds(analysis_date)\n",
    "    \n",
    "    # Flag original df\n",
    "    flagged_df = categorize_meters_speeds_pandas(original_df)\n",
    "    \n",
    "    # Rows that are only divided by 0\n",
    "    zeroes_only_df = keep_only_zeroes(analysis_date)\n",
    "    \n",
    "    # Find % trips with 1+ row that has a division by 0 for a route\n",
    "    problematic_trips_df = problematic_trips_percentage(original_df, zeroes_only_df)\n",
    "    \n",
    "    # Find the trip for a route that has the highest % of ok rows\n",
    "    # most_populated_trip_df = route_most_populated(flagged_df)\n",
    "    \n",
    "    return original_df, zeroes_only_df, problematic_trips_df, most_populated_trip_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be9888dd-232a-401d-8039-9c2d1954a167",
   "metadata": {},
   "outputs": [],
   "source": [
    "# m1, m2, trips_count, route_most_populated_df = stage_flagging(analysis_date)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ccd0ac3-2aa6-42ab-a121-6ba3ab7673b1",
   "metadata": {},
   "source": [
    "### Investigate "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e108e886-4ad0-48d4-9906-1af416f3e980",
   "metadata": {},
   "source": [
    "#### Stage3: \"vp_pared_stops\"/A3_loop_inlining"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2a705af-b588-463b-b6ce-f999b2050208",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_vp_stage3(flagged_df:pd.DataFrame, date:str) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Load vp_pared_stops\n",
    "    \n",
    "    Args:\n",
    "        flagged_df: df from categorize_meters_speeds_pandas()\n",
    "        date: analysis date\n",
    "    \"\"\"\n",
    "    # Subset to filter vp_pared_stops.\n",
    "    shape_array_keys = flagged_df.shape_array_key.unique().tolist()\n",
    "    stop_seq = flagged_df.stop_sequence.unique().tolist() \n",
    "    trip_id = flagged_df.trip_id.unique().tolist() \n",
    "    gtfs_dataset_key = flagged_df.gtfs_dataset_key.unique().tolist() \n",
    "    \n",
    "    FILE = STOP_SEG_DICT['stage3']\n",
    "    \n",
    "    vp = pd.read_parquet(f\"{SEGMENT_GCS}{FILE}_{date}\",\n",
    "        filters = [[('shape_array_key', \"in\", shape_array_keys),\n",
    "                   ('stop_sequence', 'in', stop_seq), \n",
    "                   ('trip_id', 'in', trip_id), \n",
    "                   ('gtfs_dataset_key', 'in', gtfs_dataset_key)]],)\n",
    "    \n",
    "    # Merge to capture original df information and filter down further\n",
    "    vp2 = pd.merge(flagged_df, vp, how = \"inner\", on = ['gtfs_dataset_key', 'trip_id','stop_sequence','shape_array_key','_gtfs_dataset_name'])\n",
    "    \n",
    "    return vp2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f21f08f-d4eb-4bbd-94d3-f4b031e97cf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def stage3_repeated_timestamps(stage3_df:pd.DataFrame)-> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Look at how many times a time stamp is repeated a route-trip-location.\n",
    "    Each of these 3 combos should have a different time for each \n",
    "    stop sequence or else the vehicle is not moving.\n",
    "    \n",
    "    Args:\n",
    "        stage3_df: df from load_vp_stage3()\n",
    "    \"\"\"\n",
    "    agg = (stage3_df\n",
    "     .groupby(['shape_array_key','trip_id', 'location_timestamp_local'])\n",
    "     .agg({'stop_sequence':'nunique'})\n",
    "     .reset_index()\n",
    "     .rename(columns = {'stop_sequence':'number_of_repeated_timestamps'})\n",
    "    )\n",
    "    \n",
    "    # Only keep timestamps that are repeated more than once\n",
    "    agg = (agg[agg.number_of_repeated_timestamps > 1]).reset_index(drop = True)\n",
    "\n",
    "    return agg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ce07566-c1f0-4fa7-9550-2fa07b98dba8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def stage3_repeated_locations(stage3_df:pd.DataFrame)-> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Look at how many times a time stamp is repeated for a stop-trip-route combo.\n",
    "    Each of these 3 combos should have a different location for each \n",
    "    stop sequence or else the vehicle is not changing moving.\n",
    "    \n",
    "    Args:\n",
    "        stage3_df: df from load_vp_stage3()\n",
    "    \"\"\"\n",
    "    # Concat x and y into a string\n",
    "    stage3_df['pair'] = stage3_df.x.astype(str) + '/' + stage3_df.y.astype(str)\n",
    "    \n",
    "    # Count number of different stops that reference the same location\n",
    "    agg = (stage3_df\n",
    "     .groupby(['shape_array_key','trip_id','pair'])\n",
    "     .agg({'stop_sequence':'nunique'})\n",
    "     .reset_index()\n",
    "     .sort_values('stop_sequence', ascending = False)\n",
    "     .rename(columns = {'stop_sequence':'number_of_repeated_locs'})               \n",
    "    )\n",
    "\n",
    "    # Only keep locations that are repeated more than once\n",
    "    agg = agg[agg.number_of_repeated_locs != 1].reset_index(drop = True)\n",
    "    \n",
    "    return agg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66e83169-2b4a-4912-bc0e-1a0b3e8deea6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def flag_stage3(flagged_df:pd.DataFrame, date:str) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Flag the errors in stage3.\n",
    "    \n",
    "    Args:\n",
    "        flagged_df: df from categorize_meters_speeds_pandas()\n",
    "        date: analysis date\n",
    "    \"\"\"\n",
    "    start = datetime.datetime.now()\n",
    "    print(start)\n",
    "    \n",
    "    # Relevant rows from Vehicle Positions\n",
    "    vp = load_vp_stage3(flagged_df, date)\n",
    "    \n",
    "    # Find repeated timestamps.\n",
    "    multi_timestamps = stage3_repeated_timestamps(vp)\n",
    "    \n",
    "    # Find repeated locations\n",
    "    multi_locs = stage3_repeated_locations(vp)\n",
    "    \n",
    "    # Merge\n",
    "    timestamps_merge_cols = ['shape_array_key','trip_id','location_timestamp_local']\n",
    "    loc_merge_cols =  ['shape_array_key','trip_id','pair']\n",
    "    \n",
    "    # Want everything found in vehicle positions, so do left merges\n",
    "    m1 = (vp\n",
    "          .merge(multi_timestamps, how=\"left\", on= timestamps_merge_cols)\n",
    "          .merge(multi_locs, how=\"left\", on=loc_merge_cols)\n",
    "         )\n",
    "    \n",
    "    drop_cols = ['vp_idx','x','y','hour','activity_date']\n",
    "    \n",
    "    # Drop columns\n",
    "    m1 = m1.drop(columns = drop_cols)\n",
    "    \n",
    "    # Flag\n",
    "    def flag(row):\n",
    "        if (row[\"number_of_repeated_timestamps\"] > 1) & (row[\"number_of_repeated_locs\"] > 1):\n",
    "            return \"repeated timestamps & locations\"\n",
    "        elif (row[\"number_of_repeated_timestamps\"] > 1):\n",
    "            return \"repeated timestamps\"\n",
    "        elif (row[\"number_of_repeated_locs\"] > 1):\n",
    "            return \"repeated locations\"\n",
    "        else:\n",
    "            return \"check in stage 2\"\n",
    "        \n",
    "    m1[\"stage3_flag\"] = m1.apply(lambda x: flag(x), axis=1)\n",
    "    m1 = m1.drop_duplicates().reset_index(drop = True)\n",
    "    \n",
    "    print(m1.stage3_flag.value_counts())\n",
    "    \n",
    "    check_in_stage2 = m1[m1.stage3_flag == \"check in stage 2\"]\n",
    "    print(f\"Have to check {len(check_in_stage2)/len(m1) * 100} % of rows in stage 2\")\n",
    "    \n",
    "    end = datetime.datetime.now()\n",
    "    print(f\"Took {end-start}\")\n",
    "    return m1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cab32ef3-cc66-40ce-aa19-59631734f539",
   "metadata": {},
   "outputs": [],
   "source": [
    "m3 = flag_stage3(m2, analysis_date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e4e731a-6e1d-47db-aa57-abf7c34b1bf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "m3.shape_array_key.nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdafc453-0d9d-4e4a-9355-38e2ad76a1a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "sort_by = ['_gtfs_dataset_name','shape_array_key','trip_id','stop_sequence']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70996816-e94c-4906-838c-0d9e0fe3049e",
   "metadata": {},
   "outputs": [],
   "source": [
    "preview_cols = sort_by + ['stop_id','gtfs_dataset_key','location_timestamp_local','pair','stage3_flag']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "537e7c3e-3b2f-464c-8ce8-c1c78c35c6ec",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# m3[(m3.stage3_flag != 'check in stage 2')].sort_values(by =sort_by).sample()[preview_cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9290738-2ac6-4655-b08b-7823d3ec747c",
   "metadata": {},
   "outputs": [],
   "source": [
    "m3[(m3.stage3_flag != 'check in stage 2')].sort_values(by =sort_by)[preview_cols].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b1876cf-9e8b-4c30-8723-2226133b8e01",
   "metadata": {},
   "source": [
    "#### Stage2: A1_sjoin_vp_segments\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74cfc8d7-dca4-42b0-a1b2-e6a470635c7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_trips_routes(stage3_df:pd.DataFrame):\n",
    "    \"\"\"\n",
    "    Find the route/trip combination with the \n",
    "    highest number of n_trips. \n",
    "    \n",
    "    Args:\n",
    "        stage3_df: df from flag_stage3() \n",
    "    \"\"\"\n",
    "    cols_to_keep = ['shape_array_key','_gtfs_dataset_name','gtfs_dataset_key', 'trip_id', 'n_trips']\n",
    "    m1 = (stage3_df\n",
    "     .sort_values(['n_trips'], ascending = False)\n",
    "     .drop_duplicates(['shape_array_key'])\n",
    "     [cols_to_keep]\n",
    "         )\n",
    "    return m1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00205695-1193-4e44-966e-733fc16d24fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "shape_trips = count_trips_routes(m3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05b804be-9dba-425a-bbae-6ccd65c22885",
   "metadata": {},
   "outputs": [],
   "source": [
    "shape_trips.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef18eb20-a43e-4d32-80eb-7f902116a944",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Look at export  file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6397dc45-c271-4057-a0d8-1962846d4f94",
   "metadata": {},
   "outputs": [],
   "source": [
    "def import_stage_2(date:str, route:str, stop_sequence:str):\n",
    "    FILE = STOP_SEG_DICT['stage2']\n",
    "    df = pd.read_parquet(\n",
    "            f\"{SEGMENT_GCS}vp_sjoin/{FILE}_{date}\",\n",
    "            filters = [[('shape_array_key', \"==\", route),\n",
    "                       ('stop_sequence', \"==\", stop_sequence)]],\n",
    "        )\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b5dec8d-c4f5-49dd-9a11-1b10ff30fb55",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Import unique trips"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50f89ab7-5d5b-43b4-bbb0-e25944a11376",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_route1 = \"03bc2779a66739316156b459ffc3eefa\"\n",
    "test_gtfs_key1 = \"cdd2ad81863b6d4ad51676a1cb781ea8\"\n",
    "test_trip1 = \"11776020\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ade9e07f-0b55-4561-96d1-6fd6adec0f1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def import_unique_trips(gtfs_key:str, trip: str, route:str):\n",
    "    \"\"\"\n",
    "    Read vp_usable file for one \n",
    "    trip/route/operator and find the unique trips.\n",
    "    \"\"\"\n",
    "    FILE = STOP_SEG_DICT['stage1']\n",
    "    vp_trips = A1_sjoin_vp_segments.add_grouping_col_to_vp(\n",
    "        f\"{FILE}_{analysis_date}\",\n",
    "        analysis_date,\n",
    "       [\"shape_array_key\"]\n",
    "    )\n",
    "    \n",
    "    # Filter to just one trip/route/operator\n",
    "    df = vp_trips[(vp_trips.gtfs_dataset_key == gtfs_key)\n",
    "                    & (vp_trips.shape_array_key == route)\n",
    "                    & (vp_trips.trip_id == trip)].reset_index(drop = True)\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8003044-b7e4-477e-9395-fa881a2fa2b3",
   "metadata": {},
   "outputs": [],
   "source": [
    " unique_trips = import_unique_trips(test_gtfs_key1, test_trip1, test_route1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52ce333b-9f75-4c9b-a1af-130c93786f94",
   "metadata": {},
   "source": [
    "#### Look at vehicle positions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac6e78c3-694d-4297-8db1-f0f4d6faadbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def import_vehicle_positions(unique_trips:pd.DataFrame, gtfs_key:str, trip_id:str)-> gpd.GeoDataFrame:\n",
    "    \"\"\"\n",
    "    Create geometry form x and y for only one trip. This plots\n",
    "    ALL points for the trip.\n",
    "    \n",
    "    Args:\n",
    "        unique_trips: df from import_unique_trips()\n",
    "    \"\"\"\n",
    "    FILE = STOP_SEG_DICT['stage0']\n",
    "    \n",
    "    vp = helpers.import_vehicle_positions(\n",
    "            SEGMENT_GCS,\n",
    "            f\"{FILE}_{analysis_date}/\",\n",
    "            \"gdf\",\n",
    "            filters = [[(\"gtfs_dataset_key\", \"==\", gtfs_key),\n",
    "                      ('trip_id', '==', trip_id)]],\n",
    "            columns = [\"gtfs_dataset_key\", \"trip_id\",\"geometry\"],\n",
    "            partitioned = False\n",
    "        )\n",
    "    vp = vp.compute()\n",
    "    \"\"\"\n",
    "    OLD\n",
    "    FILE =  STOP_SEG_DICT['stage1']\n",
    "    vp = helpers.import_vehicle_positions(\n",
    "            SEGMENT_GCS,\n",
    "            f\"{FILE}_{analysis_date}/\",\n",
    "            \"df\",\n",
    "            filters = [[(\"gtfs_dataset_key\", \"==\", gtfs_key),\n",
    "                      ('trip_id', '==', trip_id)]],\n",
    "            columns = [\"gtfs_dataset_key\", \"trip_id\", \n",
    "                       \"vp_idx\", \"x\", \"y\"],\n",
    "            partitioned = False\n",
    "        )\n",
    "    \n",
    "    vp = vp.compute()\n",
    "    vp = vp.merge(unique_trips, on = [\"gtfs_dataset_key\", \"trip_id\"],\n",
    "            how = \"inner\"\n",
    "        )\n",
    "    \n",
    "    vp_gdf = gpd.GeoDataFrame(\n",
    "        vp, \n",
    "        geometry = gpd.points_from_xy(vp.x, vp.y, crs = \"EPSG:4326\")\n",
    "    ).to_crs(PROJECT_CRS).drop(columns = [\"x\", \"y\"])\n",
    "    \"\"\"\n",
    "    vp = vp.merge(unique_trips, on = [\"gtfs_dataset_key\", \"trip_id\"],\n",
    "            how = \"inner\"\n",
    "        )\n",
    "    return vp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76b31261-1909-44a3-b77a-421386cfc3f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test = import_vehicle_positions(unique_trips,test_gtfs_key1, test_trip1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e6ff875-bd04-4337-9a54-9fac8b60b0b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# type(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "896e00be-27f2-43b7-9cb4-69d61a061af0",
   "metadata": {},
   "source": [
    "#### Look at segments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17bb5083-ace2-4e2d-8400-3bf948625909",
   "metadata": {},
   "outputs": [],
   "source": [
    "def import_segments(flagged_df: pd.DataFrame, \n",
    "                    route:str, \n",
    "                    gtfs_key:str, \n",
    "                    trip_id:str,\n",
    "                    analysis_date:str) -> gpd.GeoDataFrame:\n",
    "    \"\"\"\n",
    "    Import cut segments and colorcode  them based on \n",
    "    whether or not it has 1+ rows that is divided by 0\n",
    "    \n",
    "    Args:\n",
    "        flagged_df: result from df from categorize_meters_speeds_pandas()\n",
    "    \"\"\"\n",
    "    # Load in ALL segments, flag them.\n",
    "    FILE = STOP_SEG_DICT['segments_file']\n",
    "    gdf = gpd.read_parquet(f\"{SEGMENT_GCS}{FILE}_{analysis_date}.parquet\",\n",
    "                           filters = [[(\"shape_array_key\", \"==\", route),\n",
    "                                      (\"gtfs_dataset_key\", \"==\", gtfs_key),\n",
    "                                     ]]).to_crs(PROJECT_CRS)\n",
    "    \n",
    "    gdf[\"geometry_buffered\"] = gdf.geometry.buffer(35)\n",
    "    gdf = gdf.set_geometry('geometry_buffered')\n",
    "    \n",
    "    # Distinguish between \"correct\" and \"incorrect\" seq\n",
    "    # A sequence can be incorrect even if just one row is \"divided by 0\"\n",
    "    incorrect_segments = flagged_df[\n",
    "        (flagged_df.shape_array_key == route)\n",
    "        & (flagged_df.gtfs_dataset_key == gtfs_key)\n",
    "        & (flagged_df.trip_id == trip_id)\n",
    "    ]\n",
    "    incorrect_segments_list = incorrect_segments.stop_sequence.unique().tolist()\n",
    "    incorrect_segments_filtered = gdf[gdf.stop_sequence.isin(incorrect_segments_list)].reset_index(drop = True)\n",
    "    incorrect_segments_filtered['flag'] = 'contains 0m/0sec'\n",
    "    \n",
    "    # Filter for correct segments using \n",
    "    correct_segments = flagged_df[~flagged_df.stop_sequence.isin(incorrect_segments_list)]\n",
    "    correct_segments_list = correct_segments.stop_sequence.unique().tolist()\n",
    "    correct_segments_filtered = gdf[gdf.stop_sequence.isin(correct_segments_list)].reset_index(drop = True)\n",
    "    correct_segments_filtered['flag'] = 'does not contain 0m/0sec'\n",
    "    \n",
    "    final = pd.concat([correct_segments_filtered, incorrect_segments_filtered])\n",
    "    \n",
    "    return final"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a688361-e7a9-40f0-877d-3693be99a960",
   "metadata": {},
   "source": [
    "#### Stops kept: last and first"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ea744bf-8019-4b7c-988f-f95196b56435",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_first_last_points(route:str, trip:str, gtfs_key:str)-> gpd.GeoDataFrame:\n",
    "    \"\"\"\n",
    "    Load gdf with only the first and last points pared. \n",
    "    \"\"\"\n",
    "    FILE = STOP_SEG_DICT['stage3']\n",
    "    \n",
    "    df = pd.read_parquet(f\"{SEGMENT_GCS}{FILE}_{analysis_date}\",\n",
    "        filters = [[('shape_array_key', \"==\", route),\n",
    "                  \n",
    "                   ('trip_id', \"==\", trip), \n",
    "                   ('gtfs_dataset_key', '==', gtfs_key)]],)\n",
    "    \n",
    "    gdf =  gpd.GeoDataFrame(\n",
    "        df, \n",
    "        geometry = gpd.points_from_xy(df.x, df.y, crs = \"EPSG:4326\")\n",
    "    ).to_crs(PROJECT_CRS).drop(columns = [\"x\", \"y\"])\n",
    "    \n",
    "    gdf = gdf[['geometry','stop_sequence']]\n",
    "    \n",
    "    return gdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c7aa90f-3e80-472e-b61d-94afd6c0ec01",
   "metadata": {},
   "outputs": [],
   "source": [
    "# first_last = find_first_last_points(test_route, test_trip, test_gtfs_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5634169a-f26b-4174-b46a-3aa872bc1bdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# len(first_last)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9aba7f4e-2b1a-4f1b-aaf2-1bc7bb3cf221",
   "metadata": {},
   "source": [
    "#### Sjoin "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d535d059-efd9-49dd-9759-8663679ad5e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sjoin_vp_segments(segments: gpd.GeoDataFrame, vp_gdf: gpd.GeoDataFrame)-> gpd.GeoDataFrame:\n",
    "    \"\"\"\n",
    "    Sjoin all the points with the relevant segments\n",
    "    \"\"\"\n",
    "    vp_in_seg = gpd.sjoin(\n",
    "        vp_gdf,\n",
    "        segments,\n",
    "        how = \"inner\",\n",
    "        predicate = \"within\"\n",
    "    )\n",
    "    \n",
    "    return vp_in_seg"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1aeb604b-249b-41e7-be71-fe8d3205e54a",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a785180-38bb-4d33-a96e-3385c84ed2f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_maps(all_points: gpd.GeoDataFrame, \n",
    "                 first_last_points: gpd.GeoDataFrame,\n",
    "                 segments: gpd.GeoDataFrame,\n",
    "                 sjoin_results: gpd.GeoDataFrame):\n",
    "    \"\"\"\n",
    "    Create 3 maps: one to display all points, one to display\n",
    "    the sjoined points, one to display first and last points that are\n",
    "    automatically kept.\n",
    "    \n",
    "    Args:\n",
    "        all_points: gdf from import_vehicle_positions()\n",
    "        first_last_points: gdf from find_first_last_points()\n",
    "        segments: gdf from import_segments()\n",
    "        sjoin_results: gdf from sjoin_vp_segments()\n",
    "    \"\"\"\n",
    "    segments = segments[~segments.geometry_buffered.is_empty]\n",
    "    base1 = segments.explore('flag', cmap= 'tab10', height = 400, width = 600, name = 'segments')\n",
    "    all_points_map = all_points.explore(m = base1, color = 'red',style_kwds = {'weight':6}, name= 'points')\n",
    "    \n",
    "    print('ALL POINTS')\n",
    "    display(all_points_map) \n",
    "     \n",
    "    # Have to use a different base geometry for sjoin \n",
    "    sjoin_points = sjoin_results.set_geometry('geometry_left')\n",
    "    sjoin_segments = sjoin_results.set_geometry('geometry_right')\n",
    "    sjoin_segments.geometry_right = sjoin_segments.geometry_right.buffer(35)\n",
    "    base3 = sjoin_segments.explore('flag', cmap= 'tab10', height = 400, width = 600, name = 'segments')\n",
    "    sjoin_map = sjoin_points.explore(m = base3, color = 'orange',style_kwds = {'weight':6},  name= 'points')\n",
    "    \n",
    "    print('SJOIN')\n",
    "    display(sjoin_map)\n",
    "    \n",
    "    base2 = segments.explore('flag', cmap= 'tab10', height = 400, width = 600, name = 'segments')\n",
    "    first_last_map = first_last_points.explore(m = base2, color = 'pink',style_kwds = {'weight':6},height = 400, width = 600,)\n",
    "    \n",
    "    print('FIRST AND LAST')\n",
    "    display(first_last_map)\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bab6bfc4-3d08-46fe-be33-6179ac5df34d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# display_maps(vehicle_positions,first_last,flagged_segments)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7ed9dc6-80ce-46f9-ae59-a5ed2bbcad50",
   "metadata": {},
   "source": [
    "#### Function\n",
    "\n",
    "Previous tried routes\n",
    "test_route = \"106d979b9a9e6338827a8e1c145e69fd\"\n",
    "test_sequence = 39\n",
    "test_gtfs_key = \"db56b50ab86b5f7a4ae2fc2dd9889bbe\"\n",
    "test_trip = '1088405'\n",
    "\n",
    "test_route2 = \"0fb4f3627996269dc7075276d3b69e36\"\n",
    "test_gtfs_key2 = \"a4f6fd5552107e05fe9743ac7cce2c55\"\n",
    "test_trip2 = \"16939095\"\n",
    "\n",
    "test_route3 = \"07c9a47264a43d8d0d16ef7109e8fd68\"\n",
    "test_gtfs_key3 = \"db56b50ab86b5f7a4ae2fc2dd9889bbe\"\n",
    "test_trip3 = \"1089348\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "509bc2c0-14f3-4021-baf1-6d89a2409a79",
   "metadata": {},
   "outputs": [],
   "source": [
    "def stage2_trouble_shooting(flagged_df:pd.DataFrame,\n",
    "                            date:str, \n",
    "                            route:str, \n",
    "                            trip:str, \n",
    "                            gtfs_key:str,\n",
    "                            analysis_date:str):\n",
    "    \n",
    "    \"\"\"\n",
    "    Putting together all the functions for stage 2\n",
    "    \"\"\"\n",
    "    unique_trips = import_unique_trips(gtfs_key, trip, route)\n",
    "    \n",
    "    # Find all recorded vps\n",
    "    vehicle_positions = import_vehicle_positions(unique_trips, gtfs_key, trip)\n",
    "    \n",
    "    # Flag segments, whether one row contains one or more 0/0 division or not\n",
    "    flagged_segments = import_segments(flagged_df, route, gtfs_key, trip, analysis_date)\n",
    "    \n",
    "    # Find first and last pt kept\n",
    "    first_last = find_first_last_points(route, trip, gtfs_key)\n",
    "    \n",
    "    # Sjoin \n",
    "    sjoin_results = sjoin_vp_segments(flagged_segments,vehicle_positions)\n",
    "    \n",
    "    # Display maps\n",
    "    display_maps(vehicle_positions,first_last,flagged_segments,sjoin_results)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "056dc4ec-dde7-4f5a-bcd6-8ebd5b9d6982",
   "metadata": {},
   "source": [
    "#### Example Trip 1\n",
    "* Underestanding the result from `flag_stage3()`. \n",
    "* Looking at AC Transit: stop sequences 6 and 7 have different stop_ids. However, their time stamps and locations are the same. \n",
    "* It looks like they share the same point. \n",
    "* However, this isn't due to paring too many points: there just aren't enough points to choose from in the raw data.\n",
    "* Also sequence 2 is extremely long."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db254647-2f6a-4ec9-a900-25578c745419",
   "metadata": {},
   "outputs": [],
   "source": [
    "m3[(m3.stage3_flag != 'check in stage 2')].sort_values(by =sort_by)[preview_cols].head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63a03788-570d-41c3-b862-d8782a559f75",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "def original_df_rows(original_df: pd.DataFrame, \n",
    "                     trip_id:str, \n",
    "                     shape_array_key:str) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Look at the rows from the original df\n",
    "    \"\"\"\n",
    "    preview_cols2 = ['stop_sequence','stop_id','meters_elapsed','sec_elapsed',]\n",
    "    \n",
    "    df = (original_df[(original_df.trip_id == trip_id)\n",
    "            & (original_df.shape_array_key == shape_array_key)]\n",
    "            [preview_cols2]\n",
    "            .sort_values(['stop_sequence'])\n",
    "           )\n",
    "            \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f50dc43f-697f-400c-b598-968a59ac7452",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_route1 = \"03bc2779a66739316156b459ffc3eefa\"\n",
    "test_gtfs_key1 = \"cdd2ad81863b6d4ad51676a1cb781ea8\"\n",
    "test_trip1 = \"11776020\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aef433dd-9019-439a-abb2-70e0a65bfb1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# % of trips with problematic rows for this route\n",
    "trips_count[trips_count.shape_array_key == test_route1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03312246-3ea0-42a7-a552-86196a61b0f9",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "original_df_rows(m1, test_trip1, test_route1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a4ae095-d010-46b5-80b2-0bbe948f249f",
   "metadata": {},
   "outputs": [],
   "source": [
    "stage2_trouble_shooting(flagged_df= m3,\n",
    "                        date = analysis_date,\n",
    "                        route = test_route1,\n",
    "                        trip = test_trip1,\n",
    "                        gtfs_key = test_gtfs_key1,\n",
    "                        analysis_date = analysis_date)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38c6d374-155d-4fcc-985a-7c892eaecb46",
   "metadata": {},
   "source": [
    "#### Example Trip 2\n",
    "* Underestanding the result from `flag_stage3()`. \n",
    "* Same thing as Ex Trip 1 except kooking at LA Metro: stop sequences 45 and 46 have different stop_ids. However, their time stamps and locations are the same. \n",
    "* Same issue: segments 45 and 46 are sharing points. There aren't enough points captured.\n",
    "* In general, it looks like this route doesn't have a lot of rows that are ok."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "268c5f17-df4a-4d84-b3b0-d76a2727bcf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_route2 = \"38c814829dff816aa87c606c3aab4f45\"\n",
    "test_gtfs_key2 = \"65d9589130415c685b89f4f7c2d8bd7e\"\n",
    "test_trip2 = \"10294000051654-DEC22\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ae2cbab-5fa4-45bf-9965-962f33dc1946",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Original number of rows for this trip\n",
    "len(m1[(m1.trip_id == test_trip2) & (m1.shape_array_key == test_route2)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a927dfa-7b78-41f8-b387-6d03f4e328c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rows with zeroes...a lot of them.\n",
    "len(m3[(m3.trip_id == test_trip2) & (m3.shape_array_key == test_route2)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b193ab2-1cfa-471b-aa1b-857949f4e2d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "m3[(m3.stage3_flag != 'check in stage 2') & (m3.shape_array_key == test_route2) \n",
    "   & (m3.stop_sequence.isin([45,46]))].sort_values(by =sort_by)[preview_cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b3bc6c2-d9ff-44fd-b812-28c4dec1023f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# % of trips with problematic rows for this route\n",
    "trips_count[trips_count.shape_array_key == test_route2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d121f820-9838-4d27-8eba-be74044554c6",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "original_df_rows(m1, test_trip2, test_route2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "deaa6fdf-37b8-49ee-97f2-46f74d41a449",
   "metadata": {},
   "outputs": [],
   "source": [
    "stage2_trouble_shooting(flagged_df= m3,\n",
    "                        date = analysis_date,\n",
    "                        route = test_route2,\n",
    "                        trip = test_trip2,\n",
    "                        gtfs_key = test_gtfs_key2,\n",
    "                        analysis_date = analysis_date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f5485ec-4038-46d1-bca1-0116733f0128",
   "metadata": {},
   "outputs": [],
   "source": [
    "# See which trip has the most rows that are ok for this route\n",
    "route_most_populated_df[route_most_populated_df.shape_array_key == test_route2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be67628e-7c55-43d8-838c-251bc4439a8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the trip from this route with the highest % of ok rows\n",
    "stage2_trouble_shooting(flagged_df= m3,\n",
    "                        date = analysis_date,\n",
    "                        route = test_route2,\n",
    "                        trip = \"10294000051724-DEC22\",\n",
    "                        gtfs_key = test_gtfs_key2,\n",
    "                        analysis_date = analysis_date)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27d10dab-c7b0-4ddd-b70b-b3c6b7b3e579",
   "metadata": {},
   "source": [
    "#### Example Trip 3\n",
    "* Choosing a route/trip with high n_trips that isn't Muni or LA Metro to shake things up.\n",
    "* San Diego Vehicle Positions\n",
    "* Segments 44 and 45 don't have any rows in the original dataframe for trip 16938440."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f951ac97-43af-452f-9cb7-d40f71c114c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_route3 = \"1fc55d9df0cd785dddc864bf1b72976f\"\n",
    "test_gtfs_key3 = \"a4f6fd5552107e05fe9743ac7cce2c55\"\n",
    "test_trip3 = \"16938440\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26e85057-05a9-4606-af4a-7be3e08ae2a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# m1[(m1.stop_sequence == 38) & (m1.shape_array_key == test_route3)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91798284-13ab-459b-80b8-8d3b63053615",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "original_df_rows(m1, test_trip3, test_route3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91d07c20-9d78-4eea-8b9c-293df8ade5a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "stage2_trouble_shooting(flagged_df= m3,\n",
    "                        date = analysis_date,\n",
    "                        route = test_route3,\n",
    "                        trip = test_trip3,\n",
    "                        gtfs_key = test_gtfs_key3,\n",
    "                        analysis_date = analysis_date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ef5a418-34fb-4470-a576-71fc5281476b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Trip from this route with the most ok rows\n",
    "route_most_populated_df[route_most_populated_df.shape_array_key == test_route3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf92c615-47ab-4bd5-8513-0a9d202bd542",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the trip with the most ok rows\n",
    "stage2_trouble_shooting(flagged_df= m3,\n",
    "                        date = analysis_date,\n",
    "                        route = test_route3,\n",
    "                        trip = \"16938341\",\n",
    "                        gtfs_key = test_gtfs_key3,\n",
    "                        analysis_date = analysis_date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9561cd4-1b24-4415-ac6b-71adba8d0dfa",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "original_df_rows(m1, \"16938341\", test_route3)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
