{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c7feec3-aa18-42ab-94b9-cab4be608152",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "import _speed_utils as speed_utils\n",
    "import _threshold_utils as threshold_utils\n",
    "import altair as alt\n",
    "import dask.dataframe as dd\n",
    "import geopandas as gpd\n",
    "import pandas as pd\n",
    "from segment_speed_utils import gtfs_schedule_wrangling, helpers, segment_calcs\n",
    "from segment_speed_utils.project_vars import (\n",
    "    COMPILED_CACHED_VIEWS,\n",
    "    PROJECT_CRS,\n",
    "    SEGMENT_GCS,\n",
    "    CONFIG_PATH,\n",
    "    \n",
    "    # analysis_date,\n",
    ")\n",
    "from scripts import A1_sjoin_vp_segments\n",
    "from shared_utils import calitp_color_palette as cp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0108ae4a-4518-4487-85f7-a5faa3e9cbf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.options.display.max_columns = 100\n",
    "pd.options.display.float_format = \"{:.2f}\".format\n",
    "pd.set_option(\"display.max_rows\", None)\n",
    "pd.set_option(\"display.max_colwidth\", None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80014c5e-695d-4280-89cd-4e7e2bb3d302",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding analysis_date here since there aren't any files for June yet\n",
    "analysis_date = '2023-05-17'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f340774a-2dbd-4d00-942a-fa16dd2fbfe3",
   "metadata": {},
   "outputs": [],
   "source": [
    "SEGMENT_GCS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35c1ba95-5929-4a7f-bfae-f6d126622104",
   "metadata": {},
   "source": [
    "### Merging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f0c5f4f-f419-42a8-8527-7060ed412092",
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_all_speeds(analysis_date:str) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Merge avg_speeds_stop_segments and\n",
    "    speed_stops parquets.\n",
    "    \n",
    "    Args:\n",
    "        date: analysis date\n",
    "    \"\"\"\n",
    "    # Open up avg speeds\n",
    "    avg_speeds = pd.read_parquet(f\"{SEGMENT_GCS}avg_speeds_stop_segments_{analysis_date}.parquet\")\n",
    "    avg_speeds = avg_speeds.drop(columns=[\"geometry\", \"district\", \"district_name\"])\n",
    "    \n",
    "    # Filter  for all day flags\n",
    "    avg_speeds = avg_speeds[avg_speeds.time_of_day == 'all_day'].reset_index(drop = True)\n",
    "    \n",
    "    # Open up speeds\n",
    "    speeds = pd.read_parquet(f\"{SEGMENT_GCS}speeds_stop_segments_{analysis_date}\")\n",
    "    \n",
    "    # Merge\n",
    "    merge_cols = ['gtfs_dataset_key','shape_array_key', 'stop_sequence']\n",
    "    m1 = pd.merge(avg_speeds, speeds, on = merge_cols, how = 'inner')\n",
    "    \n",
    "    m1 = m1.drop_duplicates().reset_index(drop = True)\n",
    "    \n",
    "    return m1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f62b4e00-618e-4abe-bed0-0cc7026980b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "m1 = merge_all_speeds(analysis_date)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "898e3546-5298-4c4f-87d0-ee1d1a10f07d",
   "metadata": {},
   "source": [
    "### Categorize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e81e59fd-cc2f-408e-9148-1a1055425fc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def categorize_by_percentile_pandas(\n",
    "    df: pd.DataFrame, column_percentile: str, column_str: str\n",
    ") -> pd.DataFrame:\n",
    "\n",
    "    # Find percentiles\n",
    "    p5 = df[column_percentile].quantile(0.05).astype(float)\n",
    "    p95 = df[column_percentile].quantile(0.95).astype(float)\n",
    "    \n",
    "    # Keep the most extremes of both ends\n",
    "    def rate(row):\n",
    "        if ((row[column_percentile] >= 0) and (row[column_percentile] <= p5)):\n",
    "            return f\"{column_str} is low\"\n",
    "        elif (row[column_percentile] >= p95):\n",
    "               return f\"{column_str} is high\"\n",
    "        else:\n",
    "            return f\"{column_str} is avg\"\n",
    "    \n",
    "    # Apply flags\n",
    "    df[f\"{column_str}cat\"] = df.apply(lambda x: rate(x), axis=1)\n",
    "    \n",
    "    # Clean\n",
    "    df[f\"{column_str}cat\"] = df[f\"{column_str}cat\"].str.replace(\"_\", \"\")\n",
    "    \n",
    "    return df  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d38d541-5c9c-4d31-8986-9c3928eb2f59",
   "metadata": {},
   "outputs": [],
   "source": [
    "def categorize_meters_speeds_pandas(analysis_date:str)-> pd.DataFrame:\n",
    "    start = datetime.datetime.now()\n",
    "    print(start)\n",
    "    \n",
    "    df = merge_all_speeds(analysis_date)\n",
    "    \n",
    "    # Categorize\n",
    "    df1 = categorize_by_percentile_pandas(df, \"meters_elapsed\", \"meters_\")\n",
    "    df2 = categorize_by_percentile_pandas(df1, \"sec_elapsed\", \"sec_\")\n",
    "  \n",
    "    # Find size of categories\n",
    "    print(df2.groupby(['sec_cat','meters_cat']).size())\n",
    "\n",
    "    # Filter out for only meters that are low or seconds that are high\n",
    "    df2 = df2[(df2.meters_cat == 'meters is low') | (df2.sec_cat == 'sec is high')].reset_index(drop = True)\n",
    "    print(f\"{len(df2)} rows left after filtering for rows with either high seconds OR low meters\") \n",
    "    \n",
    "    def flag_round(row):\n",
    "        if (row[\"meters_elapsed\"] == 0) & (row[\"sec_elapsed\"] == 0):\n",
    "            return \"division by 0\"\n",
    "        elif row[\"meters_cat\"] == \"meters is low\":\n",
    "            return \"meters too low\"\n",
    "        elif row[\"sec_cat\"] == \"sec is high\":\n",
    "            return \"seconds too high\"\n",
    "        else:\n",
    "            return \"ok\"\n",
    "        \n",
    "    df2[\"flag\"] = df2.apply(lambda x: flag_round(x), axis=1)\n",
    "    print(df2.flag.value_counts()/len(df2)*100)\n",
    "    \n",
    "    # Filter out for only division by 0 \n",
    "    df3 = df2[(df2.flag == 'division by 0')].reset_index(drop = True)\n",
    "    \n",
    "    \n",
    "    # Print out some interesting tidbits\n",
    "    f\"{df.trip_id.nunique()-df3.trip_id.nunique()} unique trips flagged.\"\n",
    "    f\"{df.shape_array_key.nunique()-df3.shape_array_key.nunique()} routes flagged.\"\n",
    "    f\"{df._gtfs_dataset_name.nunique()-df3._gtfs_dataset_name.nunique()} operators remain.\"\n",
    "    \n",
    "    end = datetime.datetime.now()\n",
    "    print(f\"Took {end-start}\")\n",
    "    return df3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c5107cb-c574-449b-95b6-fb205f38502e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "m2 = categorize_meters_speeds_pandas(analysis_date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83036ccc-7339-42c2-b1f7-183734253c21",
   "metadata": {},
   "outputs": [],
   "source": [
    "m2.groupby([\"loop_or_inlining\"]).agg({\"shape_array_key\": \"nunique\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4486cd7c-31d7-4420-ac67-f9783676ede8",
   "metadata": {},
   "source": [
    "#### See how many trips for a shape ID have at least one problematic row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5de008d-6859-42b0-8709-ef8ddf2dbb00",
   "metadata": {},
   "outputs": [],
   "source": [
    "# m1 = merge_all_speeds(analysis_date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "468be3c9-7a24-4f01-84fd-31c137bc45e8",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Number of trips that have at least one row that was divided by 0 \n",
    "# for this shape array key\n",
    "# df1 = m2.groupby(['shape_array_key']).agg({'trip_id':'nunique'}).rename(columns = {'trip_id':'trips_with_zero'}).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4350f540-8f6b-4fb0-8b16-836245c0e44c",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Original number of trips\n",
    "# df2 = m1.groupby(['shape_array_key']).agg({'trip_id':'nunique'}).rename(columns = {'trip_id':'all_trips'}).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac68bdf7-26a0-4679-9a35-26f8a670018a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df3 = pd.merge(df1, df2, how = \"inner\", on = 'shape_array_key')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81d443cc-122f-46f1-87ec-dbdc74e0ca6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df3['percent_of_trips_with_problematic_rows'] = df3.trips_with_zero/df3.all_trips * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "314d9baf-de0e-460a-8c29-4504ba94cfa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df3['percent_of_trips_with_problematic_rows'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5de3efe6-2233-4251-93a8-1f8dd6fb2dae",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# df3.sample(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ccd0ac3-2aa6-42ab-a121-6ba3ab7673b1",
   "metadata": {},
   "source": [
    "### Investigate \n",
    "* TO DO: read file names through config path"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e108e886-4ad0-48d4-9906-1af416f3e980",
   "metadata": {},
   "source": [
    "#### Stage3: \"vp_pared_stops\"/A3_loop_inlining"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2a705af-b588-463b-b6ce-f999b2050208",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_vp_stage3(flagged_df:pd.DataFrame, date:str) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Load vp_pared_stops\n",
    "    \"\"\"\n",
    "    # Subset to filter vp_pared_stops.\n",
    "    shape_array_keys = flagged_df.shape_array_key.unique().tolist()\n",
    "    stop_seq = flagged_df.stop_sequence.unique().tolist() \n",
    "    trip_id = flagged_df.trip_id.unique().tolist() \n",
    "    gtfs_dataset_key = flagged_df.gtfs_dataset_key.unique().tolist() \n",
    "    \n",
    "    vp = pd.read_parquet(f\"{SEGMENT_GCS}vp_pared_stops_{date}\",\n",
    "        filters = [[('shape_array_key', \"in\", shape_array_keys),\n",
    "                   ('stop_sequence', 'in', stop_seq), \n",
    "                   ('trip_id', 'in', trip_id), \n",
    "                   ('gtfs_dataset_key', 'in', gtfs_dataset_key)]],)\n",
    "    \n",
    "    # Merge to capture original df information\n",
    "    vp2 = pd.merge(flagged_df, vp, how = \"inner\", on = ['gtfs_dataset_key', 'trip_id','stop_sequence','shape_array_key'])\n",
    "    \n",
    "    return vp2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f21f08f-d4eb-4bbd-94d3-f4b031e97cf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def stage3_repeated_timestamps(stage3_df:pd.DataFrame)-> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Look at how many times a time stamp is repeated a route-trip-location.\n",
    "    Each of these 3 combos should have a different time for each \n",
    "    stop sequence or else the vehicle is not changing locations.\n",
    "    \"\"\"\n",
    "    agg = (stage3_df\n",
    "     .groupby(['shape_array_key','trip_id', 'location_timestamp_local'])\n",
    "     .agg({'stop_sequence':'nunique'})\n",
    "     .reset_index()\n",
    "     .rename(columns = {'stop_sequence':'number_of_repeated_timestamps'})\n",
    "    )\n",
    "    \n",
    "    # Only keep timestamps that are repeated more than once\n",
    "    agg = (agg[agg.number_of_repeated_timestamps > 1]).reset_index(drop = True)\n",
    "\n",
    "    return agg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ce07566-c1f0-4fa7-9550-2fa07b98dba8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def stage3_repeated_locations(stage3_df:pd.DataFrame):\n",
    "    \"\"\"\n",
    "    Look at how many times a time stamp is repeated for a stop-trip-route combo.\n",
    "    Each of these 3 combos should have a different location for each \n",
    "    stop sequence or else the vehicle is not changing locations.\n",
    "    \"\"\"\n",
    "    # Concat x and y into a string\n",
    "    stage3_df['pair'] = stage3_df.x.astype(str) + '/' + stage3_df.y.astype(str)\n",
    "    \n",
    "    # Count number of different stops that reference the same location\n",
    "    agg = (stage3_df\n",
    "     .groupby(['shape_array_key','trip_id','pair'])\n",
    "     .agg({'stop_sequence':'nunique'})\n",
    "     .reset_index()\n",
    "     .sort_values('stop_sequence', ascending = False)\n",
    "     .rename(columns = {'stop_sequence':'number_of_repeated_locs'})               \n",
    "    )\n",
    "\n",
    "    # Only keep locations that are repeated more than once\n",
    "    agg = agg[agg.number_of_repeated_locs != 1].reset_index(drop = True)\n",
    "    \n",
    "    return agg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66e83169-2b4a-4912-bc0e-1a0b3e8deea6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def flag_stage3(flagged_df:pd.DataFrame, date:str) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Flag the errors in stage3\n",
    "    \"\"\"\n",
    "    start = datetime.datetime.now()\n",
    "    print(start)\n",
    "    \n",
    "    # Relevant rows from Vehicle Positions\n",
    "    vp = load_vp_stage3(flagged_df, date)\n",
    "    \n",
    "    # Find repeated timestamps.\n",
    "    multi_timestamps = stage3_repeated_timestamps(vp)\n",
    "    \n",
    "    # Find repeated locations\n",
    "    multi_locs = stage3_repeated_locations(vp)\n",
    "    \n",
    "    # Merge\n",
    "    timestamps_merge_cols = ['shape_array_key','trip_id','location_timestamp_local']\n",
    "    loc_merge_cols =  ['shape_array_key','trip_id','pair']\n",
    "    \n",
    "    # Want everything found in vehicle positions, so do left merges\n",
    "    m1 = (vp\n",
    "          .merge(multi_timestamps, how=\"left\", on= timestamps_merge_cols)\n",
    "          .merge(multi_locs, how=\"left\", on=loc_merge_cols)\n",
    "         )\n",
    "    \n",
    "    drop_cols = ['vp_idx','x','y','hour','activity_date']\n",
    "    \n",
    "    # Drop columns\n",
    "    m1 = m1.drop(columns = drop_cols)\n",
    "    \n",
    "    # Flag\n",
    "    def flag(row):\n",
    "        if (row[\"number_of_repeated_timestamps\"] > 1) & (row[\"number_of_repeated_locs\"] > 1):\n",
    "            return \"repeated timestamps & locations\"\n",
    "        elif (row[\"number_of_repeated_timestamps\"] > 1):\n",
    "            return \"repeated timestamps\"\n",
    "        elif (row[\"number_of_repeated_locs\"] > 1):\n",
    "            return \"repeated locations\"\n",
    "        else:\n",
    "            return \"check in stage 2\"\n",
    "        \n",
    "    m1[\"stage3_flag\"] = m1.apply(lambda x: flag(x), axis=1)\n",
    "    \n",
    "    print(m1.stage3_flag.value_counts())\n",
    "    \n",
    "    check_in_stage2 = m1[m1.stage3_flag == \"check in stage 2\"]\n",
    "    print(f\"Have to check {len(check_in_stage2)/len(m1) * 100} % of rows in stage 2\")\n",
    "    \n",
    "    end = datetime.datetime.now()\n",
    "    print(f\"Took {end-start}\")\n",
    "    return m1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cab32ef3-cc66-40ce-aa19-59631734f539",
   "metadata": {},
   "outputs": [],
   "source": [
    "m3 = flag_stage3(m2, analysis_date)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b1876cf-9e8b-4c30-8723-2226133b8e01",
   "metadata": {},
   "source": [
    "#### Stage2: A1_sjoin_vp_segments\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17ac977c-d220-414e-be9f-540eec051e06",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find routes with the most trips for testing\n",
    "(m3\n",
    " .sort_values(['n_trips'], ascending = False)\n",
    " .drop_duplicates(['shape_array_key'])\n",
    " [['shape_array_key','gtfs_dataset_key', 'trip_id', 'n_trips']]\n",
    " .head(6)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a469849-f903-44e4-9d2a-4f3775270a52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select one route to look at\n",
    "test_route = \"9a383dd115298a63c75d2a81d0f27528\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e946c68-3476-459d-a869-77ac37b5fb07",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_gtfs_key = \"c0e3039da063db95ebabd3fe4ee611a4\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4fa40bf-387c-4301-ba13-2bd16b15cd24",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_trip = '11312499_M21'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef18eb20-a43e-4d32-80eb-7f902116a944",
   "metadata": {},
   "source": [
    "#### Look at export  file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6397dc45-c271-4057-a0d8-1962846d4f94",
   "metadata": {},
   "outputs": [],
   "source": [
    "def import_stage_2(date:str, route:str, stop_sequence:str):\n",
    "    df = pd.read_parquet(\n",
    "            f\"{SEGMENT_GCS}vp_sjoin/vp_stop_segment_{date}\",\n",
    "            filters = [[('shape_array_key', \"==\", route),\n",
    "                       ('stop_sequence', \"==\", stop_sequence)]],\n",
    "        )\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe8f800a-f180-4495-a387-0367528823ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# stg2 = import_stage_2(analysis_date, test_route, test_sequence)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b5dec8d-c4f5-49dd-9a11-1b10ff30fb55",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Import unique trips"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ade9e07f-0b55-4561-96d1-6fd6adec0f1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def import_unique_trips(gtfs_key:str, trip: str, route:str):\n",
    "    \"\"\"\n",
    "    Read vp_usable file, filter out to just one \n",
    "    trip/route/operator and find the unique trips.\n",
    "    \"\"\"\n",
    "    vp_trips = A1_sjoin_vp_segments.add_grouping_col_to_vp(\n",
    "        f\"vp_usable_{analysis_date}\",\n",
    "        analysis_date,\n",
    "       [\"shape_array_key\"]\n",
    "    )\n",
    "    \n",
    "    # Filter to just one trip/route/operator\n",
    "    df = vp_trips[(vp_trips.gtfs_dataset_key == gtfs_key)\n",
    "                    & (vp_trips.shape_array_key == route)\n",
    "                    & (vp_trips.trip_id == trip)].reset_index(drop = True)\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8003044-b7e4-477e-9395-fa881a2fa2b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# unique_trips = import_unique_trips(test_gtfs_key, test_trip, test_route)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52ce333b-9f75-4c9b-a1af-130c93786f94",
   "metadata": {},
   "source": [
    "#### Look at vehicle positions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac6e78c3-694d-4297-8db1-f0f4d6faadbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def import_vehicle_positions(unique_trips:pd.DataFrame, gtfs_key:str, trip_id:str)-> gpd.GeoDataFrame:\n",
    "    \"\"\"\n",
    "    Create geometry form x and y for only the relevant trip.\n",
    "    \"\"\"\n",
    "    vp = helpers.import_vehicle_positions(\n",
    "            SEGMENT_GCS,\n",
    "            f\"vp_usable_{analysis_date}/\",\n",
    "            filters = [[(\"gtfs_dataset_key\", \"==\", gtfs_key),\n",
    "                      ('trip_id', '==', trip_id)]],\n",
    "            columns = [\"gtfs_dataset_key\", \"trip_id\", \n",
    "                       \"vp_idx\", \"x\", \"y\"],\n",
    "            partitioned = True\n",
    "        )\n",
    "    \n",
    "    vp = vp.compute()\n",
    "    vp = vp.merge(unique_trips, on = [\"gtfs_dataset_key\", \"trip_id\"],\n",
    "            how = \"inner\"\n",
    "        )\n",
    "    \n",
    "    vp_gdf = gpd.GeoDataFrame(\n",
    "        vp, \n",
    "        geometry = gpd.points_from_xy(vp.x, vp.y, crs = \"EPSG:4326\")\n",
    "    ).to_crs(PROJECT_CRS).drop(columns = [\"x\", \"y\"])\n",
    "    \n",
    "    return vp_gdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b47ea0cb-6031-4d98-b963-efbef949d169",
   "metadata": {},
   "outputs": [],
   "source": [
    "#vehicle_positions = import_vehicle_positions(unique_trips, test_gtfs_key, test_trip)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0f96480-8328-43ed-9add-0a74b533fc8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#len(vehicle_positions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "896e00be-27f2-43b7-9cb4-69d61a061af0",
   "metadata": {},
   "source": [
    "#### Look at segments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17bb5083-ace2-4e2d-8400-3bf948625909",
   "metadata": {},
   "outputs": [],
   "source": [
    "def import_segments(flagged_df: pd.DataFrame, route:str, gtfs_key:str) -> gpd.GeoDataFrame:\n",
    "    \"\"\"\n",
    "    Import cut segments and colorcode  them based on \n",
    "    whether or not it has 1+ rows that is divided by 0\n",
    "    \"\"\"\n",
    "    # Load in ALL segments, flag them.\n",
    "    gdf = gpd.read_parquet(f\"{SEGMENT_GCS}stop_segments_{analysis_date}.parquet\",\n",
    "                           filters = [[(\"shape_array_key\", \"==\", route),\n",
    "                                      (\"gtfs_dataset_key\", \"==\", gtfs_key),\n",
    "                                     ]]).to_crs(PROJECT_CRS)\n",
    "    \n",
    "    gdf[\"geometry_buffered\"] = gdf.geometry.buffer(35)\n",
    "    gdf = gdf.set_geometry('geometry_buffered')\n",
    "    \n",
    "    # Distinguish between \"correct\" and \"incorrect\" seq\n",
    "    # A sequence can be incorrect even if just one row is \"divided by 0\"\n",
    "    incorrect_segments = flagged_df[(flagged_df.shape_array_key == route) & (flagged_df.gtfs_dataset_key == gtfs_key)]\n",
    "    incorrect_segments_list = incorrect_segments.stop_sequence.unique().tolist()\n",
    "    incorrect_segments_filtered = gdf[gdf.stop_sequence.isin(incorrect_segments_list)].reset_index(drop = True)\n",
    "    incorrect_segments_filtered['flag'] = 'contains 0m/0sec'\n",
    "    \n",
    "    # Filter for correct segments using \n",
    "    correct_segments = flagged_df[~flagged_df.stop_sequence.isin(incorrect_segments_list)]\n",
    "    correct_segments_list = correct_segments.stop_sequence.unique().tolist()\n",
    "    correct_segments_filtered = gdf[gdf.stop_sequence.isin(correct_segments_list)].reset_index(drop = True)\n",
    "    correct_segments_filtered['flag'] = 'does not contain 0m/0sec'\n",
    "    \n",
    "    final = pd.concat([correct_segments_filtered, incorrect_segments_filtered])\n",
    "    \n",
    "    return final"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a688361-e7a9-40f0-877d-3693be99a960",
   "metadata": {},
   "source": [
    "#### Stops kept: last and first"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ea744bf-8019-4b7c-988f-f95196b56435",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_first_last_points(route:str, trip:str, gtfs_key:str)-> gpd.GeoDataFrame:\n",
    "    \"\"\"\n",
    "    Load gdf with first and last points\n",
    "    \"\"\"\n",
    "    df = pd.read_parquet(f\"{SEGMENT_GCS}vp_pared_stops_{analysis_date}\",\n",
    "        filters = [[('shape_array_key', \"==\", route),\n",
    "                  \n",
    "                   ('trip_id', \"==\", trip), \n",
    "                   ('gtfs_dataset_key', '==', gtfs_key)]],)\n",
    "    \n",
    "    gdf =  gpd.GeoDataFrame(\n",
    "        df, \n",
    "        geometry = gpd.points_from_xy(df.x, df.y, crs = \"EPSG:4326\")\n",
    "    ).to_crs(PROJECT_CRS).drop(columns = [\"x\", \"y\"])\n",
    "    \n",
    "    gdf = gdf[['geometry','stop_sequence']]\n",
    "    \n",
    "    return gdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c7aa90f-3e80-472e-b61d-94afd6c0ec01",
   "metadata": {},
   "outputs": [],
   "source": [
    "# first_last = find_first_last_points(test_route, test_trip, test_gtfs_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5634169a-f26b-4174-b46a-3aa872bc1bdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# len(first_last)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9aba7f4e-2b1a-4f1b-aaf2-1bc7bb3cf221",
   "metadata": {},
   "source": [
    "#### Sjoin "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d535d059-efd9-49dd-9759-8663679ad5e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sjoin_vp_segments(segments: gpd.GeoDataFrame, vp_gdf: gpd.GeoDataFrame)-> gpd.GeoDataFrame:\n",
    "    \"\"\"\n",
    "    Sjoin all the points with the relevant segments\n",
    "    \"\"\"\n",
    "    vp_in_seg = gpd.sjoin(\n",
    "        vp_gdf,\n",
    "        segments,\n",
    "        how = \"inner\",\n",
    "        predicate = \"within\"\n",
    "    )\n",
    "    \n",
    "    return vp_in_seg"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1aeb604b-249b-41e7-be71-fe8d3205e54a",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a785180-38bb-4d33-a96e-3385c84ed2f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_maps(all_points: gpd.GeoDataFrame, \n",
    "                 first_last_points: gpd.GeoDataFrame,\n",
    "                 segments: gpd.GeoDataFrame,\n",
    "                 sjoin_results: gpd.GeoDataFrame):\n",
    "    \"\"\"\n",
    "    Create 3 maps: one to display all points, one to display\n",
    "    the sjoined points, one to display first and last points that are\n",
    "    automatically kept.\n",
    "    \"\"\"\n",
    "    \n",
    "    base1 = segments.explore('flag', cmap= 'tab10', height = 400, width = 600, name = 'segments')\n",
    "    all_points_map = all_points.explore(m = base1, color = 'red',style_kwds = {'weight':6}, name= 'points')\n",
    "    \n",
    "    print('ALL POINTS')\n",
    "    display(all_points_map) \n",
    "     \n",
    "    # Have to use a different base geometry for sjoin \n",
    "    sjoin_points = sjoin_results.set_geometry('geometry_left')\n",
    "    sjoin_segments = sjoin_results.set_geometry('geometry_right')\n",
    "    sjoin_segments.geometry_right = sjoin_segments.geometry_right.buffer(35)\n",
    "    base3 = sjoin_segments.explore('flag', cmap= 'tab10', height = 400, width = 600, name = 'segments')\n",
    "    sjoin_map = sjoin_points.explore(m = base3, color = 'orange',style_kwds = {'weight':6},  name= 'points')\n",
    "    \n",
    "    print('SJOIN')\n",
    "    display(sjoin_map)\n",
    "    \n",
    "    base2 = segments.explore('flag', cmap= 'tab10', height = 400, width = 600, name = 'segments')\n",
    "    first_last_map = first_last_points.explore(m = base2, color = 'pink',style_kwds = {'weight':6},height = 400, width = 600,)\n",
    "    \n",
    "    print('FIRST AND LAST')\n",
    "    display(first_last_map)\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bab6bfc4-3d08-46fe-be33-6179ac5df34d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# display_maps(vehicle_positions,first_last,flagged_segments)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7ed9dc6-80ce-46f9-ae59-a5ed2bbcad50",
   "metadata": {},
   "source": [
    "#### Function\n",
    "\n",
    "Previous tried routes\n",
    "test_route = \"106d979b9a9e6338827a8e1c145e69fd\"\n",
    "test_sequence = 39\n",
    "test_gtfs_key = \"db56b50ab86b5f7a4ae2fc2dd9889bbe\"\n",
    "test_trip = '1088405'\n",
    "\n",
    "test_route2 = \"0fb4f3627996269dc7075276d3b69e36\"\n",
    "test_gtfs_key2 = \"a4f6fd5552107e05fe9743ac7cce2c55\"\n",
    "test_trip2 = \"16939095\"\n",
    "\n",
    "test_route3 = \"07c9a47264a43d8d0d16ef7109e8fd68\"\n",
    "test_gtfs_key3 = \"db56b50ab86b5f7a4ae2fc2dd9889bbe\"\n",
    "test_trip3 = \"1089348\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "509bc2c0-14f3-4021-baf1-6d89a2409a79",
   "metadata": {},
   "outputs": [],
   "source": [
    "def stage2_trouble_shooting(flagged_df:pd.DataFrame,\n",
    "                            date:str, \n",
    "                            route:str, \n",
    "                            trip:str, \n",
    "                            gtfs_key:str):\n",
    "    \n",
    "    unique_trips = import_unique_trips(gtfs_key, trip, route)\n",
    "    \n",
    "    # Find all recorded vps\n",
    "    vehicle_positions = import_vehicle_positions(unique_trips, gtfs_key, trip)\n",
    "    \n",
    "    # Flag segments, whether one row contains one or more 0/0 division or not\n",
    "    flagged_segments = import_segments(flagged_df, route, gtfs_key)\n",
    "    \n",
    "    # Find first and last pt kept\n",
    "    first_last = find_first_last_points(route, trip, gtfs_key)\n",
    "    \n",
    "    # Sjoin \n",
    "    sjoin_results = sjoin_vp_segments(flagged_segments,vehicle_positions)\n",
    "    \n",
    "    # Display maps\n",
    "    display_maps(vehicle_positions,first_last,flagged_segments,sjoin_results)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "056dc4ec-dde7-4f5a-bcd6-8ebd5b9d6982",
   "metadata": {},
   "source": [
    "#### Example Trip 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a4ae095-d010-46b5-80b2-0bbe948f249f",
   "metadata": {},
   "outputs": [],
   "source": [
    "stage2_trouble_shooting(flagged_df= m3,\n",
    "                        date = analysis_date,\n",
    "                        route = test_route,\n",
    "                        trip = test_trip,\n",
    "                        gtfs_key = test_gtfs_key)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38c6d374-155d-4fcc-985a-7c892eaecb46",
   "metadata": {},
   "source": [
    "#### Example Trip 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "268c5f17-df4a-4d84-b3b0-d76a2727bcf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_route2 = \"1fe993c63998b86090757c7cb36e7a3d\"\n",
    "test_gtfs_key2 = \"5222fe2cf728fd3f16b2ff51e133fe8c\"\n",
    "test_trip2 = \"183-gxphfsr7q\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0914a84-b24d-442f-aaca-acf401b9209c",
   "metadata": {},
   "outputs": [],
   "source": [
    "m1[(m1.stop_sequence == 18) & (m1.shape_array_key == test_route2)][['meters_elapsed','sec_elapsed']].sort_values(['meters_elapsed','sec_elapsed']).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "deaa6fdf-37b8-49ee-97f2-46f74d41a449",
   "metadata": {},
   "outputs": [],
   "source": [
    "stage2_trouble_shooting(flagged_df= m3,\n",
    "                        date = analysis_date,\n",
    "                        route = test_route2,\n",
    "                        trip = test_trip2,\n",
    "                        gtfs_key = test_gtfs_key2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27d10dab-c7b0-4ddd-b70b-b3c6b7b3e579",
   "metadata": {},
   "source": [
    "#### Example Trip 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f951ac97-43af-452f-9cb7-d40f71c114c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_route3 = \"daf9c3f9d0df9b8019db0d5cb71fc694\"\n",
    "test_gtfs_key3 = \"c0e3039da063db95ebabd3fe4ee611a4\"\n",
    "test_trip3 = \"11266910_M21\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26e85057-05a9-4606-af4a-7be3e08ae2a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# subset[(subset.stop_sequence == 34) & (subset.shape_array_key == test_route3)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91d07c20-9d78-4eea-8b9c-293df8ade5a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "stage2_trouble_shooting(flagged_df= m3,\n",
    "                        date = analysis_date,\n",
    "                        route = test_route3,\n",
    "                        trip = test_trip3,\n",
    "                        gtfs_key = test_gtfs_key3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06df58c1-c7b7-4769-bd8d-696e337eefb3",
   "metadata": {},
   "source": [
    "### Stage1: \"vp_usable\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9eab37f-0569-4f07-9113-87200b0c7dfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# What's the diff between stop segments normal/special/and without any notation?\n",
    "usable = pd.read_parquet(f\"{SEGMENT_GCS}vp_usable_{analysis_date}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "deb486c8-a800-485e-8a46-d994af1c0074",
   "metadata": {},
   "outputs": [],
   "source": [
    "usable.sample()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "813ae4db-0fef-4f10-9408-7284fc531ed2",
   "metadata": {},
   "outputs": [],
   "source": [
    "m_cols2 = ['gtfs_dataset_key',\n",
    " 'trip_id']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d08fa8db-f3a3-43f2-a763-a39cacc9cf9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "subset_for_merge2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68a59632-cfbd-43fd-aca4-a502e400a854",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# m2[m2.trip_id == '1350']"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
