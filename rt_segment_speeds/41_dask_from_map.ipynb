{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "90a5f6e6-01ea-4466-b120-e810173b718b",
   "metadata": {},
   "source": [
    "## Update `time_series_utils` and `dask_utils` with `dask.from_map` instead of delayed\n",
    "\n",
    "`time_series_utils` concatenating a bunch of months using `dask.delayed` is taking longer to do so. For segment geometries, this gdf is getting really big and using a lot of memory. In reality, we don't need to full gdf, we want to look across many dates and then dedupe.\n",
    "\n",
    "Dask delayed docs mentions the use of `from_map` as a way to read in parquets and do something with it.\n",
    "\n",
    "Dask docs: https://docs.dask.org/en/latest/generated/dask.dataframe.from_map.html\n",
    "\n",
    "Tutorial: https://blog.dask.org/2023/04/12/from-map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b80bacc-f2ce-4ff9-8ab6-161a1e55bbed",
   "metadata": {},
   "outputs": [],
   "source": [
    "import dask.dataframe as dd\n",
    "import geopandas as gpd\n",
    "import pandas as pd\n",
    "\n",
    "from typing import Literal\n",
    "\n",
    "from segment_speed_utils.project_vars import GTFS_DATA_DICT, SEGMENT_GCS\n",
    "from shared_utils import rt_dates\n",
    "\n",
    "analysis_date_list = rt_dates.y2024_dates\n",
    "\n",
    "segment_type = \"stop_segments\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04676acc-0306-43c0-b8ad-77b512637fc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def func(\n",
    "    path: str,\n",
    "    one_date: str, \n",
    "    data_type: Literal[\"df\", \"gdf\"] = \"df\",\n",
    "    **kwargs, \n",
    "):\n",
    "    if data_type == \"gdf\":\n",
    "        \n",
    "        df = gpd.read_parquet(\n",
    "            f\"{path}_{one_date}.parquet\", \n",
    "            **kwargs,\n",
    "        ).drop_duplicates()\n",
    "\n",
    "    else:\n",
    "        df = pd.read_parquet(\n",
    "            f\"{path}_{one_date}.parquet\", \n",
    "            **kwargs,\n",
    "        ).drop_duplicates()\n",
    "    \n",
    "    return df\n",
    "\n",
    "def get_ddf(paths, analysis_date_list, data_type, **kwargs):\n",
    "\n",
    "    return dd.from_map(\n",
    "        func, paths, \n",
    "        analysis_date_list, \n",
    "        data_type = data_type, \n",
    "        **kwargs\n",
    "    ).drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f717278b-eb1b-4ff5-abc5-d52306806f7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "segment_file = GTFS_DATA_DICT[segment_type][\"segments_file\"]\n",
    "segment_cols = [\"schedule_gtfs_dataset_key\", \"route_id\", \"geometry\"]\n",
    "\n",
    "segment_paths = [f\"{SEGMENT_GCS}{segment_file}\" for date in analysis_date_list]\n",
    "\n",
    "segment_paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cd3b5e9-03b8-442f-9e68-ce6ca7448d10",
   "metadata": {},
   "outputs": [],
   "source": [
    "segment_gddf = get_ddf(\n",
    "    segment_paths, \n",
    "    analysis_date_list, \n",
    "    data_type = \"gdf\",\n",
    "    columns = segment_cols\n",
    ")  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0286691c-10ce-40d5-ac72-d08077aa0661",
   "metadata": {},
   "outputs": [],
   "source": [
    "segment_gddf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcd1dd94-6335-4501-9b80-42a95eb75845",
   "metadata": {},
   "outputs": [],
   "source": [
    "segment_gddf.compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "070ff8c3-18dd-4d14-81b3-59a584a4c4df",
   "metadata": {},
   "outputs": [],
   "source": [
    "speed_file = GTFS_DATA_DICT[segment_type][\"route_dir_single_segment\"]\n",
    "speed_cols = [\"schedule_gtfs_dataset_key\", \"route_id\"]\n",
    "\n",
    "speed_paths = [f\"{SEGMENT_GCS}{speed_file}\" for date in analysis_date_list]\n",
    "speed_paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02247d42-0a9d-4f06-b619-541ce61c9056",
   "metadata": {},
   "outputs": [],
   "source": [
    "speed_ddf = get_ddf(\n",
    "    speed_paths, \n",
    "    analysis_date_list, \n",
    "    data_type = \"df\",\n",
    "    columns = speed_cols\n",
    ")  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0de3f973-e5b5-4018-ad3a-7b2e647143ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "speed_df = speed_ddf.compute()\n",
    "print(speed_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f28315e-4e8c-435a-b501-34db94435d1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def concatenate_datasets_across_dates(\n",
    "    gcs_bucket: str,\n",
    "    dataset_name: str,\n",
    "    date_list: list,\n",
    "    data_type: Literal[\"df\", \"gdf\"],\n",
    "    get_pandas: bool = True,\n",
    "    **kwargs\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Concatenate parquets across all months of available data.\n",
    "    \"\"\"  \n",
    "    paths = [f\"{gcs_bucket}{dataset_name}\" for date in date_list]\n",
    "\n",
    "    df = get_ddf(\n",
    "        paths, \n",
    "        date_list, \n",
    "        data_type = data_type,\n",
    "        **kwargs\n",
    "    )  \n",
    "    if get_pandas:\n",
    "        df = df.compute()\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c58061d0-6670-48fb-aafe-b271a35e1ea1",
   "metadata": {},
   "outputs": [],
   "source": [
    "speed_ddf2 = concatenate_datasets_across_dates(\n",
    "    SEGMENT_GCS,\n",
    "    speed_file,\n",
    "    analysis_date_list,\n",
    "    data_type = \"df\",\n",
    "    columns = speed_cols,\n",
    "    get_pandas = False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56a32dee-5179-4835-ab4d-5a2d20f78a77",
   "metadata": {},
   "outputs": [],
   "source": [
    "speed_df2 = speed_ddf2.compute()\n",
    "print(speed_df2.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "192689d5-bec2-4c0f-857f-741f7f7366f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "speed_df3 = concatenate_datasets_across_dates(\n",
    "    SEGMENT_GCS,\n",
    "    speed_file,\n",
    "    analysis_date_list,\n",
    "    data_type = \"df\",\n",
    "    get_pandas = True,\n",
    "    columns = speed_cols,\n",
    ")\n",
    "\n",
    "speed_df3.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cce5ea5c-ba9a-4110-956c-7acab35dee2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from segment_speed_utils import time_series_utils\n",
    "\n",
    "speed_df4 = time_series_utils.concatenate_datasets_across_dates(\n",
    "    SEGMENT_GCS,\n",
    "    speed_file,\n",
    "    analysis_date_list,\n",
    "    data_type = \"df\",\n",
    "    get_pandas = True,\n",
    "    columns = speed_cols,\n",
    ")\n",
    "\n",
    "speed_df4.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7caf46d9-f4d1-4a79-99f6-b06792b3d5a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "speed_df4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4e0fd9e-15b3-43aa-aaea-d5b279d9c7f8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
