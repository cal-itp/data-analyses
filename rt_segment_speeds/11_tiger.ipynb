{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "18ada7ff-048d-4c5c-9523-9f191e772137",
   "metadata": {},
   "source": [
    "## Tiger Census\n",
    "* https://www2.census.gov/geo/pdfs/maps-data/data/tiger/tgrshp2019/TGRSHP2019_TechDoc.pdf\n",
    "* S1200 - secondary road\n",
    "* S1100 - primary road\n",
    "* S1400 - local roads\n",
    "* Build off scripts/cut_road_segments.py\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "e1409ea6-c44a-45af-9a4b-7752f5f8cfd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "import dask.dataframe as dd\n",
    "import dask_geopandas as dg\n",
    "from dask import delayed, compute\n",
    "\n",
    "import geopandas as gpd\n",
    "import pandas as pd\n",
    "from calitp_data_analysis.sql import to_snakecase\n",
    "\n",
    "import geopandas\n",
    "from segment_speed_utils import helpers\n",
    "from segment_speed_utils.project_vars import analysis_date\n",
    "from shared_utils import dask_utils, geography_utils, utils\n",
    "\n",
    "GCS_FILE_PATH = \"gs://calitp-analytics-data/data-analyses/\"\n",
    "SHARED_GCS = f\"{GCS_FILE_PATH}shared_data/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c69abcdb-7509-47c8-b891-a99c899db38b",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.options.display.max_columns = 100\n",
    "pd.options.display.float_format = \"{:.2f}\".format\n",
    "pd.set_option(\"display.max_rows\", None)\n",
    "pd.set_option(\"display.max_colwidth\", None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af59e43d-f137-4a0a-9fb9-e6ebd0a6d030",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Tiger - Load Roads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "61786354-ec33-47bd-9891-cb292068c703",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_roads(road_type_wanted: list, buffer_or_not: bool = False):\n",
    "    \"\"\"\n",
    "    Load some or all of the roads based on what you filter.\n",
    "    Can also buffer the roads or not.\n",
    "\n",
    "    Args:\n",
    "        road_type_wanted (list): the type of roads you want.\n",
    "        https://www2.census.gov/geo/pdfs/maps-data/data/tiger/tgrshp2019/TGRSHP2019_TechDoc.pdf\n",
    "\n",
    "        buffer_or_not (bool): add a buffer of 200 or not\n",
    "\n",
    "    Returns:\n",
    "        GeoDataFrame. As of 4/18/23, returns 953914 nunique linearid\n",
    "    \"\"\"\n",
    "    df = gpd.read_parquet(\n",
    "        f\"{SHARED_GCS}all_roads_2020_state06.parquet\",\n",
    "        filters=[(\"MTFCC\", \"in\", road_type_wanted)],\n",
    "        columns=[\"LINEARID\", \"geometry\", \"FULLNAME\"],\n",
    "    ).to_crs(geography_utils.CA_NAD83Albers)\n",
    "\n",
    "    if buffer_or_not:\n",
    "        df = df.assign(geometry=df.geometry.buffer(200))\n",
    "\n",
    "    df = to_snakecase(df)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0451fb36-838d-479a-94c4-18fb48582937",
   "metadata": {},
   "outputs": [],
   "source": [
    "# local_primary_seconary_og_rds.linearid.nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "929aeccc-8427-4340-91a7-46876e5fb533",
   "metadata": {},
   "outputs": [],
   "source": [
    "# type(local_primary_seconary_og_rds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "15340a83-0098-4758-a1ba-458aa9c1c441",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Local roads that are sjoined to bus routes, not segmented\n",
    "# local_roads = gpd.read_parquet(f'{SHARED_GCS}local_roads_gtfs_shapes_sjoin.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1ddfb2b9-7ed6-40b2-bcb2-997734807654",
   "metadata": {},
   "outputs": [],
   "source": [
    "# local_roads.linearid.nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "64d35256-75bd-44b1-98b4-daba30f69f41",
   "metadata": {},
   "outputs": [],
   "source": [
    "# All primary/secondary roads regardless of instersection w bus routes\n",
    "# Segmented\n",
    "# primary_secondary = gpd.read_parquet(f'{SHARED_GCS}segmented_primary_secondary_roads.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7d87508a-8a83-4475-af73-b8da086b277d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# primary_secondary.linearid.nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "45665f78-049a-4459-a648-937eec7a4b3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# local_primary_seconary_og_rds.linearid.nunique() - (primary_secondary.linearid.nunique() + local_roads.linearid.nunique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "58b86364-9c5e-4f01-9d7c-e0cee3e27f78",
   "metadata": {},
   "outputs": [],
   "source": [
    "# primary_secondary.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fb2efafd-0046-4be6-af1a-326d20d083cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# local_roads.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5146a5b9-df08-40d2-855c-785fc5d02886",
   "metadata": {},
   "outputs": [],
   "source": [
    "# local_primary_seconary_og_rds.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45bdd874-29bf-4e67-8c07-04bd9cdda0d6",
   "metadata": {},
   "source": [
    "### GTFS Shapes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a67fb46e-c33b-4dee-82c4-dcd611443963",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gtfs_stops_operators(date) -> gpd.GeoDataFrame:\n",
    "    \"\"\"\n",
    "    Load and merge stops\n",
    "    with trips to get operator and\n",
    "    feed key information.\n",
    "\n",
    "    Args:\n",
    "        date: date wanted for the datasets to be drawn from\n",
    "\n",
    "    Returns:\n",
    "        GeoDataFrame\n",
    "    \"\"\"\n",
    "    stops = (\n",
    "        helpers.import_scheduled_stops(\n",
    "            date, (), [\"feed_key\", \"stop_id\", \"stop_key\", \"geometry\"]\n",
    "        )\n",
    "        .compute()\n",
    "        .drop_duplicates()\n",
    "    )\n",
    "\n",
    "    stops = stops.set_crs(geography_utils.CA_NAD83Albers)\n",
    "\n",
    "    # Buffer\n",
    "    stops = stops.assign(buffered_geometry=stops.geometry.buffer(50))\n",
    "\n",
    "    # Set geometry\n",
    "    stops = stops.set_geometry(\"buffered_geometry\")\n",
    "\n",
    "    # Merge for operator information\n",
    "    trips = (\n",
    "        helpers.import_scheduled_trips(analysis_date, (), [\"name\", \"feed_key\"])\n",
    "        .compute()\n",
    "        .drop_duplicates()\n",
    "    )\n",
    "    \n",
    "    \n",
    "    m1 = pd.merge(stops, trips, on=[\"feed_key\"], how=\"left\")\n",
    "\n",
    "    # Fill in na\n",
    "    m1.name = m1.name.fillna(\"None\")\n",
    "\n",
    "    return m1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0e17d8f7-07ca-48b7-9b9d-e28cbb3b0285",
   "metadata": {},
   "outputs": [],
   "source": [
    "# stops = gtfs_stops_operators(analysis_date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "af866cf6-65d8-4d4f-96d4-a2eb8f3bf88b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gtfs_routes_operators(date) -> gpd.GeoDataFrame:\n",
    "    \"\"\"\n",
    "    Load and merge routes\n",
    "    with trips to get operator and\n",
    "    feed key information.\n",
    "\n",
    "    Args:\n",
    "        date: date wanted for the datasets to be drawn from\n",
    "\n",
    "    Returns:\n",
    "        GeoDataFrame\n",
    "    \"\"\"\n",
    "    gtfs_shapes = helpers.import_scheduled_shapes(date).compute().drop_duplicates()\n",
    "    \n",
    "    gtfs_shapes = gtfs_shapes.set_crs(geography_utils.CA_NAD83Albers)\n",
    "    \n",
    "    trips = (\n",
    "        helpers.import_scheduled_trips(date, (), [\"name\", \"shape_array_key\"])\n",
    "        .compute()\n",
    "        .drop_duplicates()\n",
    "    )\n",
    "    \n",
    "    m1 = pd.merge(gtfs_shapes, trips, how=\"outer\", on=\"shape_array_key\")\n",
    "\n",
    "    return m1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ee6d2496-861a-4cbb-a3d9-98fab1735da5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def order_operators(date) -> list:\n",
    "    \"\"\"\n",
    "    Re order a list of operators so some of the largest\n",
    "    ones will be at the top of the list.\n",
    "\n",
    "    Args:\n",
    "        date: date wanted for the datasets to be drawn from\n",
    "    \"\"\"\n",
    "    operator_list = (\n",
    "        helpers.import_scheduled_trips(date, (), [\"name\"]).compute().sort_values(\"name\")\n",
    "    )\n",
    "    operator_list = operator_list.name.unique().tolist()\n",
    "\n",
    "    # Reorder list so the biggest operators are at the beginning\n",
    "    # based on NTD services data\n",
    "    big_operators = [\n",
    "        \"LA DOT Schedule\",\n",
    "        \"LA Metro Bus Schedule\",\n",
    "        \"LA Metro Rail Schedule\",\n",
    "        \"Bay Area 511 Muni Schedule\",\n",
    "        \"Bay Area 511 AC Transit Schedule\",\n",
    "        \"Bay Area 511 Santa Clara Transit Schedule\",\n",
    "        \"Bay Area 511 BART Schedule\",\n",
    "        \"San Diego Schedule\",\n",
    "        \"OCTA Schedule\",\n",
    "        \"Sacramento Schedule\",\n",
    "        \"Bay Area 511 Sonoma-Marin Area Rail Transit Schedule\",\n",
    "        \"Bay Area 511 SFO AirTrain Schedule\",\n",
    "        \"Bay Area 511 South San Francisco Shuttle Schedule\",\n",
    "        \"Bay Area 511 Marin Schedule\",\n",
    "        \"Bay Area 511 County Connection Schedule\",\n",
    "        \"Bay Area 511 MVGO Schedule\",\n",
    "        \"Bay Area 511 Commute.org Schedule\",\n",
    "        \"Bay Area 511 Union City Transit Schedule\",\n",
    "        \"Bay Area 511 BART Schedule\",\n",
    "        \"Bay Area 511 Caltrain Schedule\",\n",
    "        \"Bay Area 511 Fairfield and Suisun Transit Schedule\",\n",
    "        \"Bay Area 511 Dumbarton Express Schedule\",\n",
    "        \"Bay Area 511 SamTrans Schedule\",\n",
    "        \"Bay Area 511 Vine Transit Schedule\",\n",
    "        \"Bay Area 511 Tri-Valley Wheels Schedule\",\n",
    "        \"Bay Area 511 Sonoma County Transit Schedule\",\n",
    "        \"Bay Area 511 Santa Rosa CityBus Schedule\",\n",
    "        \"Bay Area 511 Golden Gate Transit Schedule\",\n",
    "        \"Bay Area 511 Golden Gate Ferry Schedule\",\n",
    "        \"Bay Area 511 San Francisco Bay Ferry Schedule\",\n",
    "        \"Bay Area 511 SolTrans Schedule\",\n",
    "        \"Bay Area 511 ACE Schedule\",\n",
    "        \"Bay Area 511 Emery Go-Round Schedule\",\n",
    "        \"Bay Area 511 Tri Delta Schedule\",\n",
    "        \"Bay Area 511 Petaluma Schedule\",\n",
    "        \"Bay Area 511 Capitol Corridor Schedule\",\n",
    "    ]\n",
    "\n",
    "    # Delete off the big operators\n",
    "    operator_list = list(set(operator_list) - set(big_operators))\n",
    "\n",
    "    # Add back in the operators\n",
    "    final_list = big_operators + operator_list\n",
    "\n",
    "    return final_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7d627065-9c9a-45ce-a674-d6f26dfd918c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# op_list = order_operators(analysis_date)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "876aad40-5b71-42b9-b899-11ecd15e2cfe",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Tiger Local Roads"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4002d6ad-0bac-45f5-a7a0-c9358fc39376",
   "metadata": {},
   "source": [
    "#### Cut all roads - stops 1st then routes \n",
    "* Use some small operators to test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "80e1cb44-e512-4de9-8a8c-d00621db840a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def loop_sjoin(date, local_roads_gdf, gdf_routes_stops):\n",
    "    \"\"\"\n",
    "    gdf: stops or routes gdf\n",
    "    \"\"\"\n",
    "    # Empty dataframe\n",
    "    sjoin_full_results = pd.DataFrame()\n",
    "    \n",
    "    # Find all unique operators, ordered by largest operators first\n",
    "    operators_list = order_operators(date)\n",
    "    \n",
    "    # Loop through and sjoin by operator\n",
    "    for operator in operators_list:\n",
    "        shapes_filtered = gdf_routes_stops.loc[\n",
    "            gdf_routes_stops.name == operator\n",
    "        ].reset_index(drop=True)\n",
    "\n",
    "        # Delete any local road linear ids that have already been found by an operator\n",
    "        try:\n",
    "            # List of linear IDS\n",
    "            linearid_to_delete = sjoin_full_results.linearid.unique().tolist()\n",
    "            \n",
    "            # Filter out the linear IDS in buffered local roads\n",
    "            local_roads_gdf = local_roads_gdf[\n",
    "                ~local_roads_gdf.linearid.isin(linearid_to_delete)\n",
    "            ].reset_index(drop=True)\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "        sjoin1 = (\n",
    "            gpd.sjoin(\n",
    "                local_roads_gdf,\n",
    "                shapes_filtered,\n",
    "                how=\"inner\",\n",
    "                predicate=\"intersects\",\n",
    "            )[[\"linearid\"]]\n",
    "            .drop_duplicates()\n",
    "            .reset_index(drop=True)\n",
    "        )\n",
    "\n",
    "        sjoin_full_results = pd.concat([sjoin_full_results, sjoin1], axis=0)\n",
    "\n",
    "    sjoin_full_results = sjoin_full_results.drop_duplicates()\n",
    "    \n",
    "    return sjoin_full_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "98c2aa4c-1260-4b7b-b4f4-8be3c91a0bce",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cut_stops1(date):\n",
    "    start = datetime.datetime.now()\n",
    "    \n",
    "    # Load stops\n",
    "    gtfs_stops = gtfs_stops_operators(date)\n",
    "    \n",
    "    # Load local roads - buffered\n",
    "    local_roads_buffered = load_roads([\"S1400\"], True)\n",
    "\n",
    "    # Load local roads - not buffered\n",
    "    local_roads_og = load_roads([\"S1400\"], False)\n",
    "\n",
    "    # Loop through and sjoin by operator\n",
    "    stops_sjoin = loop_sjoin(date, local_roads_buffered, gtfs_stops)\n",
    "\n",
    "    # Merge back to original local roads \n",
    "    m1 = pd.merge(local_roads_og, stops_sjoin, on=\"linearid\", how=\"inner\")\n",
    "\n",
    "    # Fill in null values for fullname\n",
    "    m1.fullname = m1.fullname.fillna(\"None\")\n",
    "    \n",
    "    # Find linear ids to delete\n",
    "    linearid_to_delete = m1.linearid.unique().tolist()\n",
    "    \n",
    "    # Save\n",
    "    m1.to_parquet(f\"{SHARED_GCS}local_roads_stops_sjoin.parquet\")\n",
    "    \n",
    "    end = datetime.datetime.now()\n",
    "    print(f\"Done with sjoin with stops and local roads. Time lapsed: {end-start}\")\n",
    "    \n",
    "    return m1, linearid_to_delete"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a7fed434-2654-4f18-96b2-193cba25f0fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cut_routes1(date, linearid_to_filter:list):\n",
    "    start = datetime.datetime.now()\n",
    "\n",
    "    # Load stops\n",
    "    gtfs_routes = gtfs_routes_operators(date)\n",
    "    \n",
    "    # Load local roads - buffered\n",
    "    local_roads_buffered = load_roads([\"S1400\"], True)\n",
    "    \n",
    "    # Delete out linear ids that have already been found\n",
    "    local_roads_buffered = local_roads_buffered[~local_roads_buffered.linearid.isin(linearid_to_filter)].reset_index(drop=True)\n",
    "    \n",
    "    # Load local roads - not buffered\n",
    "    local_roads_og = load_roads([\"S1400\"], False)\n",
    "    local_roads_og = local_roads_og[~local_roads_og.linearid.isin(linearid_to_filter)].reset_index(drop=True)\n",
    "    \n",
    "    # Loop through and sjoin by operator\n",
    "    routes_sjoin = loop_sjoin(date, local_roads_buffered,gtfs_routes)\n",
    "    \n",
    "    # Merge back to original local roads \n",
    "    m1 = pd.merge(local_roads_og, routes_sjoin, on=\"linearid\", how=\"inner\")\n",
    "\n",
    "    # Fill in null values for fullname\n",
    "    m1.fullname = m1.fullname.fillna(\"None\")\n",
    "    \n",
    "    # Save\n",
    "    m1.to_parquet(f\"{SHARED_GCS}local_roads_routes_sjoin.parquet\")\n",
    "    \n",
    "    end = datetime.datetime.now()\n",
    "    print(f\"Done with sjoin with routes and local roads. Time lapsed: {end-start}\")\n",
    "    return m1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d6c298fb-28a8-44b2-b423-e3c6ac75ff7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cut_local_roads(date):\n",
    "    start = datetime.datetime.now()\n",
    "    print(start)\n",
    "    \n",
    "    # Deal with stops first\n",
    "    stops_sjoin, linear_id_stops = cut_stops1(date)\n",
    "    \n",
    "    # Move onto routes\n",
    "    routes_sjoin = cut_routes1(date, linear_id_stops)\n",
    "    \n",
    "    # Stack\n",
    "    all_local_roads = pd.concat([stops_sjoin, routes_sjoin], axis=0)\n",
    "    \n",
    "    all_local_roads.to_parquet(f\"{SHARED_GCS}local_roads_all_routes_stops_sjoin.parquet\")\n",
    "    end = datetime.datetime.now()\n",
    "    \n",
    "    print(f\"Done with all local roads. Time lapsed: {end-start}\")\n",
    "    return all_local_roads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "992cd1c8-3aa0-40ba-a201-4b840cb02d95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-04-18 14:03:40.957812\n",
      "Done with sjoin with stops and local roads. Time lapsed: 0:06:29.965441\n",
      "Done with sjoin with routes and local roads. Time lapsed: 0:09:58.024634\n",
      "Done with all local roads. Time lapsed: 0:17:14.536012\n"
     ]
    }
   ],
   "source": [
    "# all_ops = cut_local_roads(analysis_date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "f2e4eacd-a663-40fc-90c4-fbb7718afa80",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(322720, 3)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_ops.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "id": "6e48ae43-89d1-496e-bf64-6d7c23423156",
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunk_df(gdf, chunk_row_size:int):\n",
    "    # Specify how many rows I want the gdf to broken into per df\n",
    "    n = chunk_row_size\n",
    "    \n",
    "    # Break it out\n",
    "    list_df = [gdf[i:i+n] for i in range(0,gdf.shape[0],n)]\n",
    "    \n",
    "    # Turn each dataframe to a dask one\n",
    "    my_ddfs = []\n",
    "    for df in list_df:\n",
    "        ddf = dd.from_pandas(df, npartitions= 1)\n",
    "        my_ddfs.append(ddf)\n",
    "    \n",
    "    return my_ddfs, len(my_ddfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "id": "90c61251-083c-4b0c-a3e4-93448b26afcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "ddfs, length = chunk_df(all_ops, 10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "id": "199015ff-5636-44a2-b3c1-dcb18b8b4f93",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "33"
      ]
     },
     "execution_count": 176,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "id": "821ed543-0b04-474a-924d-ca4ec5786265",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "33"
      ]
     },
     "execution_count": 175,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(ddfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "id": "4894d5d7-bc2f-4b64-aadf-272fa68fe47c",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_results = []\n",
    "for i in [*range(0,length)]:\n",
    "    my_df = ddfs[i]\n",
    "    cut_geometry = delayed(geography_utils.cut_segments)(my_df, [\"linearid\", \"fullname\"], 1_000)\n",
    "    \n",
    "    my_results.append(cut_geometry)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "id": "04bed90d-8c94-49dc-855f-c23fdb3fc30b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-04-18 16:36:43.202119\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[183], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m start \u001b[38;5;241m=\u001b[39m datetime\u001b[38;5;241m.\u001b[39mdatetime\u001b[38;5;241m.\u001b[39mnow()\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(start)\n\u001b[0;32m----> 3\u001b[0m results \u001b[38;5;241m=\u001b[39m [compute(i)[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m my_results] \n",
      "Cell \u001b[0;32mIn[183], line 3\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m      1\u001b[0m start \u001b[38;5;241m=\u001b[39m datetime\u001b[38;5;241m.\u001b[39mdatetime\u001b[38;5;241m.\u001b[39mnow()\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(start)\n\u001b[0;32m----> 3\u001b[0m results \u001b[38;5;241m=\u001b[39m [\u001b[43mcompute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mi\u001b[49m\u001b[43m)\u001b[49m[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m my_results] \n",
      "File \u001b[0;32m/opt/conda/lib/python3.9/site-packages/dask/base.py:600\u001b[0m, in \u001b[0;36mcompute\u001b[0;34m(traverse, optimize_graph, scheduler, get, *args, **kwargs)\u001b[0m\n\u001b[1;32m    597\u001b[0m     keys\u001b[38;5;241m.\u001b[39mappend(x\u001b[38;5;241m.\u001b[39m__dask_keys__())\n\u001b[1;32m    598\u001b[0m     postcomputes\u001b[38;5;241m.\u001b[39mappend(x\u001b[38;5;241m.\u001b[39m__dask_postcompute__())\n\u001b[0;32m--> 600\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[43mschedule\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdsk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkeys\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    601\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m repack([f(r, \u001b[38;5;241m*\u001b[39ma) \u001b[38;5;28;01mfor\u001b[39;00m r, (f, a) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(results, postcomputes)])\n",
      "File \u001b[0;32m/opt/conda/lib/python3.9/site-packages/dask/threaded.py:89\u001b[0m, in \u001b[0;36mget\u001b[0;34m(dsk, keys, cache, num_workers, pool, **kwargs)\u001b[0m\n\u001b[1;32m     86\u001b[0m     \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(pool, multiprocessing\u001b[38;5;241m.\u001b[39mpool\u001b[38;5;241m.\u001b[39mPool):\n\u001b[1;32m     87\u001b[0m         pool \u001b[38;5;241m=\u001b[39m MultiprocessingPoolExecutor(pool)\n\u001b[0;32m---> 89\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[43mget_async\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     90\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpool\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msubmit\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     91\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpool\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_max_workers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     92\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdsk\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     93\u001b[0m \u001b[43m    \u001b[49m\u001b[43mkeys\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     94\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     95\u001b[0m \u001b[43m    \u001b[49m\u001b[43mget_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_thread_get_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     96\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpack_exception\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpack_exception\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     97\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     98\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    100\u001b[0m \u001b[38;5;66;03m# Cleanup pools associated to dead threads\u001b[39;00m\n\u001b[1;32m    101\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m pools_lock:\n",
      "File \u001b[0;32m/opt/conda/lib/python3.9/site-packages/dask/local.py:500\u001b[0m, in \u001b[0;36mget_async\u001b[0;34m(submit, num_workers, dsk, result, cache, get_id, rerun_exceptions_locally, pack_exception, raise_exception, callbacks, dumps, loads, chunksize, **kwargs)\u001b[0m\n\u001b[1;32m    498\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m state[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwaiting\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;129;01mor\u001b[39;00m state[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mready\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;129;01mor\u001b[39;00m state[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrunning\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n\u001b[1;32m    499\u001b[0m     fire_tasks(chunksize)\n\u001b[0;32m--> 500\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m key, res_info, failed \u001b[38;5;129;01min\u001b[39;00m \u001b[43mqueue_get\u001b[49m\u001b[43m(\u001b[49m\u001b[43mqueue\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mresult():\n\u001b[1;32m    501\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m failed:\n\u001b[1;32m    502\u001b[0m             exc, tb \u001b[38;5;241m=\u001b[39m loads(res_info)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.9/site-packages/dask/local.py:137\u001b[0m, in \u001b[0;36mqueue_get\u001b[0;34m(q)\u001b[0m\n\u001b[1;32m    136\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mqueue_get\u001b[39m(q):\n\u001b[0;32m--> 137\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mq\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.9/queue.py:171\u001b[0m, in \u001b[0;36mQueue.get\u001b[0;34m(self, block, timeout)\u001b[0m\n\u001b[1;32m    169\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m timeout \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    170\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_qsize():\n\u001b[0;32m--> 171\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnot_empty\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    172\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m timeout \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    173\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtimeout\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m must be a non-negative number\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.9/threading.py:312\u001b[0m, in \u001b[0;36mCondition.wait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    310\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:    \u001b[38;5;66;03m# restore state no matter what (e.g., KeyboardInterrupt)\u001b[39;00m\n\u001b[1;32m    311\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 312\u001b[0m         \u001b[43mwaiter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43macquire\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    313\u001b[0m         gotit \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    314\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "start = datetime.datetime.now()\n",
    "print(start)\n",
    "results = [compute(i)[0] for i in my_results] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19783430-a382-4e12-b4a5-5f2adba3313a",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_gdf = pd.concat(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd4fd082-4823-4267-ad9b-62b3e8d2a93b",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_gdf.to_parquet(f\"{SHARED_GCS}segmented_local_roads.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38fc4152-b1f3-487f-a757-afeaba7153e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "end = datetime.datetime.now()\n",
    "print(end)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76fc064d-991c-4bd1-aa7d-8cf36dac1630",
   "metadata": {},
   "source": [
    "### Concat local roads and primary/secondary ones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89d85183-516f-4b88-a6c7-5da78281fc48",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cut_primary_secondary_roads():\n",
    "    start = datetime.datetime.now()\n",
    "    print(start)\n",
    "\n",
    "    # Find all primary and secondary roads\n",
    "    # regardless of intersection w/ GTFS shapes\n",
    "    primary_secondary_mtfcc = [\"S1100\", \"S1200\"]\n",
    "    primary_secondary_roads = load_roads(primary_secondary_mtfcc)\n",
    "\n",
    "    segments = geography_utils.cut_segments(\n",
    "        primary_secondary_roads, [\"linearid\", \"fullname\"], 1_000  # 1 km segments\n",
    "    )\n",
    "\n",
    "    segments.to_parquet(f\"{SHARED_GCS}segmented_primary_secondary_roads.parquet\")\n",
    "\n",
    "    end = datetime.datetime.now()\n",
    "    print(f\"time lapsed for cutting primary & secondary roads: {end-start}\")\n",
    "    return segments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d2a7a9c-1102-4a3c-a673-cd4cba2a204b",
   "metadata": {},
   "outputs": [],
   "source": [
    "primary_secondary = cut_primary_secondary_roads()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0ff123b-81fa-44b3-825d-ea9b8b68625d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cut_all_roads(date):\n",
    "    start = datetime.datetime.now()\n",
    "\n",
    "    # Segment primary and secondary roads\n",
    "    segmented_primary_secondary_rds = cut_primary_secondary_roads()\n",
    "\n",
    "    # Find local roads that intersect  with GTFS shapes, then\n",
    "    # segment them\n",
    "    local_roads_gdf = cut_local_roads(date)\n",
    "\n",
    "    # Concat\n",
    "    all_roads = pd.concat(\n",
    "        [segmented_primary_secondary_rds, segmented_local_rds], axis=0\n",
    "    )\n",
    "    all_roads.to_parquet(f\"{SHARED_GCS}segmented_all_roads.parquet\")\n",
    "\n",
    "    end = datetime.datetime.now()\n",
    "    print(f\"time lapsed: {end-start}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3482f156-a50b-4eb9-8a3e-130e9fe2b7c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_all_roads = cut_all_roads(analysis_date)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "791d8a17-f512-4a82-b465-b0f4ef2cdbbc",
   "metadata": {},
   "source": [
    "### Draft"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d45f4029-5dac-462e-98a6-4187c1e0c415",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def cut_stops(date):\n",
    "    start = datetime.datetime.now()\n",
    "    print(start)\n",
    "    \n",
    "    # Load stops\n",
    "    gtfs_stops = gtfs_stops_operators(date)\n",
    "    \n",
    "    # Load local roads - buffered\n",
    "    local_roads_buffered = load_roads([\"S1400\"], True)\n",
    "\n",
    "    # Load local roads - not buffered\n",
    "    local_roads_og = load_roads([\"S1400\"], False)\n",
    "\n",
    "    # Empty dataframe\n",
    "    sjoin_full_results = pd.DataFrame()\n",
    "\n",
    "    # Loop through and sjoin by operator\n",
    "    for operator in ['TART, North Lake Tahoe Schedule','Eastern Sierra Schedule']:\n",
    "        shapes_filtered = gtfs_stops.loc[\n",
    "            gtfs_stops.name == operator\n",
    "        ].reset_index(drop=True)\n",
    "\n",
    "        # Delete any local road linear ids that have already been found by an operator\n",
    "        try:\n",
    "            # List of linear IDS\n",
    "            linearid_to_delete = sjoin_full_results.linearid.unique().tolist()\n",
    "            \n",
    "            # Filter out the linear IDS in buffered local roads\n",
    "            local_roads_buffered = local_roads_buffered[\n",
    "                ~local_roads_buffered.linearid.isin(linearid_to_delete)\n",
    "            ].reset_index(drop=True)\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "        sjoin1 = (\n",
    "            gpd.sjoin(\n",
    "                local_roads_buffered,\n",
    "                shapes_filtered,\n",
    "                how=\"inner\",\n",
    "                predicate=\"intersects\",\n",
    "            )[[\"linearid\"]]\n",
    "            .drop_duplicates()\n",
    "            .reset_index(drop=True)\n",
    "        )\n",
    "\n",
    "        sjoin_full_results = pd.concat([sjoin_full_results, sjoin1], axis=0)\n",
    "\n",
    "    sjoin_full_results = sjoin_full_results.drop_duplicates()\n",
    "\n",
    "    # Merge back to original local roads \n",
    "    m1 = pd.merge(local_roads_og, sjoin_full_results, on=\"linearid\", how=\"inner\")\n",
    "\n",
    "    # Fill in null values for fullname\n",
    "    m1.fullname = m1.fullname.fillna(\"None\")\n",
    "    \n",
    "    # Save\n",
    "    # m1.to_parquet(f\"{SHARED_GCS}local_roads_gtfs_shapes_sjoin.parquet\")\n",
    "    \n",
    "    end = datetime.datetime.now()\n",
    "    print(f\"Done with sjoin with stops and local roads. Time lapsed: {end-start}\")\n",
    "    return m1, linearid_to_delete"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a4b03f6-3a06-42d3-bac0-96b46a900c33",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def cut_routes(date, linearid_to_filter:list):\n",
    "    start = datetime.datetime.now()\n",
    "    print(start)\n",
    "    \n",
    "    # Load stops\n",
    "    gtfs_routes = gtfs_routes_operators(date)\n",
    "    \n",
    "    # Load local roads - buffered\n",
    "    local_roads_buffered = load_roads([\"S1400\"], True)\n",
    "    # Delete out linear ids that have already been found\n",
    "    local_roads_buffered = local_roads_buffered[~local_roads_buffered.linearid.isin(linearid_to_filter)].reset_index(drop=True)\n",
    "    \n",
    "    # Load local roads - not buffered\n",
    "    local_roads_og = load_roads([\"S1400\"], False)\n",
    "    local_roads_og = local_roads_og[~local_roads_og.linearid.isin(linearid_to_filter)].reset_index(drop=True)\n",
    "\n",
    "    # Find all unique operators, ordered by largest operators first\n",
    "    # operators_list = order_operators(date)\n",
    "\n",
    "    # Empty dataframe\n",
    "    sjoin_full_results = pd.DataFrame()\n",
    "\n",
    "    # Loop through and sjoin by operator\n",
    "    for operator in ['TART, North Lake Tahoe Schedule','Eastern Sierra Schedule']:\n",
    "        shapes_filtered = gtfs_routes.loc[\n",
    "            gtfs_routes.name == operator\n",
    "        ].reset_index(drop=True)\n",
    "\n",
    "        # Delete any local road linear ids that have already been found by an operator\n",
    "        try:\n",
    "            # List of linear IDS\n",
    "            linearid_to_delete = sjoin_full_results.linearid.unique().tolist()\n",
    "            \n",
    "            # Filter out the linear IDS in buffered local roads\n",
    "            local_roads_buffered = local_roads_buffered[\n",
    "                ~local_roads_buffered.linearid.isin(linearid_to_delete)\n",
    "            ].reset_index(drop=True)\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "        sjoin1 = (\n",
    "            gpd.sjoin(\n",
    "                local_roads_buffered,\n",
    "                shapes_filtered,\n",
    "                how=\"inner\",\n",
    "                predicate=\"intersects\",\n",
    "            )[[\"linearid\"]]\n",
    "            .drop_duplicates()\n",
    "            .reset_index(drop=True)\n",
    "        )\n",
    "\n",
    "        sjoin_full_results = pd.concat([sjoin_full_results, sjoin1], axis=0)\n",
    "\n",
    "    sjoin_full_results = sjoin_full_results.drop_duplicates()\n",
    "\n",
    "    # Merge back to original local roads \n",
    "    m1 = pd.merge(local_roads_og, sjoin_full_results, on=\"linearid\", how=\"inner\")\n",
    "\n",
    "    # Fill in null values for fullname\n",
    "    m1.fullname = m1.fullname.fillna(\"None\")\n",
    "    \n",
    "    # Save\n",
    "    # m1.to_parquet(f\"{SHARED_GCS}local_roads_gtfs_shapes_sjoin.parquet\")\n",
    "    \n",
    "    end = datetime.datetime.now()\n",
    "    print(f\"Done with sjoin with stops and local roads. Time lapsed: {end-start}\")\n",
    "    return m1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7dad6db-fb7c-4b67-b783-fd7a2714365f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "def join_primary_secondary(gtfs_shape_gdf):\n",
    "    \n",
    "    # Load secondary-primary roads\n",
    "    primary_secondary_mtfcc = ['S1100','S1200']\n",
    "    primary_secondary_roads = load_roads(primary_secondary_mtfcc)\n",
    "    \n",
    "    sjoin1 = gpd.sjoin(\n",
    "        primary_secondary_roads,\n",
    "        gtfs_shape_gdf,\n",
    "        how = \"inner\",\n",
    "        predicate = \"intersects\"\n",
    "    ).drop_duplicates().reset_index(drop=True)\n",
    "    \n",
    "    # Save to GCS\n",
    "    sjoin1.to_parquet(f'{SHARED_GCS}primary_secondary_roads_gtfs_shapes.parquet')\n",
    "    print('Done with primary and secondary roads')\n",
    "    return sjoin1\n",
    "    \"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06844ac3-a82a-4aa1-89e2-e284cc1eefaa",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Test w/ LA Metro first - 25 minutes long"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc02467a-d8dd-42ba-a782-75cb4bdb379d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def la_metro(date):\n",
    "\n",
    "    # Load Shapes\n",
    "    gtfs_shape_gdf = gtfs_shapes_operators(date)\n",
    "\n",
    "    # Load local roads - buffered\n",
    "    local_roads_buffered = load_roads([\"S1400\"], True)\n",
    "\n",
    "    # Load local roads - not buffered\n",
    "    local_roads_og = load_roads([\"S1400\"], False)\n",
    "\n",
    "    # Empty dataframe\n",
    "    sjoin_full_results = pd.DataFrame()\n",
    "\n",
    "    # Loop through and join by operator\n",
    "    for operator in [\n",
    "        \"LA Metro Bus Schedule\",\n",
    "        \"LA Metro Rail Schedule\",\n",
    "    ]:\n",
    "\n",
    "        shapes_filtered = gtfs_shape_gdf.loc[\n",
    "            gtfs_shape_gdf.name == operator\n",
    "        ].reset_index(drop=True)\n",
    "\n",
    "        # Delete any local road linearids that have already been found by an operator\n",
    "        try:\n",
    "            linearid_to_delete = sjoin_full_results.linearid.unique().tolist()\n",
    "            local_roads_buffered = local_roads_buffered[\n",
    "                ~local_roads_buffered.linearid.isin(linearid_to_delete)\n",
    "            ].reset_index(drop=True)\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "        sjoin1 = (\n",
    "            gpd.sjoin(\n",
    "                local_roads_buffered,\n",
    "                shapes_filtered,\n",
    "                how=\"inner\",\n",
    "                predicate=\"intersects\",\n",
    "            )[[\"linearid\"]]\n",
    "            .drop_duplicates()\n",
    "            .reset_index(drop=True)\n",
    "        )\n",
    "\n",
    "        sjoin_full_results = pd.concat([sjoin_full_results, sjoin1], axis=0)\n",
    "\n",
    "    sjoin_full_results = sjoin_full_results.drop_duplicates()\n",
    "\n",
    "    # Merge back to original local roads -> a GDF\n",
    "    merge1 = pd.merge(local_roads_og, sjoin_full_results, on=\"linearid\", how=\"inner\")\n",
    "\n",
    "    # Fill in null values for fullname\n",
    "    merge1.fullname = merge1.fullname.fillna(\"None\")\n",
    "\n",
    "    # Save\n",
    "    print(\"Done with LA Metro\")\n",
    "\n",
    "    return merge1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63b5b9cd-11b3-4e52-b50d-809d1091626c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2023-04-10 11:20:11.194365\n",
    "# Done with LA Metro\n",
    "# time lapsed: 0:25:57.435602\n",
    "def cut_metro(date):\n",
    "    start = datetime.datetime.now()\n",
    "    print(start)\n",
    "\n",
    "    # Find local roads that intersect  with LA Metro\n",
    "    local_roads = la_metro(date)\n",
    "\n",
    "    segments = geography_utils.cut_segments(\n",
    "        local_roads, [\"linearid\", \"fullname\"], 1_000  # 1 km segments\n",
    "    )\n",
    "\n",
    "    end = datetime.datetime.now()\n",
    "    print(f\"time lapsed: {end-start}\")\n",
    "\n",
    "    return segments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4566492f-36b9-44a5-90bb-5117ebaff339",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gtfs_shapes_all_roads(date):\n",
    "\n",
    "    start = datetime.datetime.now()\n",
    "    print(start)\n",
    "\n",
    "    # Find all primary and secondary roads\n",
    "    # regardless of intersection\n",
    "    primary_secondary_mtfcc = [\"S1100\", \"S1200\"]\n",
    "    primary_secondary_roads = load_roads(primary_secondary_mtfcc)\n",
    "\n",
    "    # Find local roads that intersect  with GTFS shapes\n",
    "    local_roads_gdf = join_local_roads(date)\n",
    "\n",
    "    # Concat\n",
    "    all_roads = pd.concat([primary_secondary_roads, local_roads_gdf], axis=0)\n",
    "\n",
    "    # Save\n",
    "    all_roads.to_parquet(f\"{SHARED_GCS}intersected_local_all_primary_sec_roads.parquet\")\n",
    "\n",
    "    \"\"\"\n",
    "    # Find segments\n",
    "    segments = geography_utils.cut_segments(\n",
    "        all_roads,\n",
    "        [\"linearid\", \"fullname\"],\n",
    "        1_000 # 1 km segments\n",
    "    )\n",
    "    \n",
    "    segments.to_parquet(f\"{SHARED_GCS}census_road_segments.parquet\")\n",
    "    \"\"\"\n",
    "    end = datetime.datetime.now()\n",
    "    print(f\"time lapsed: {end-start}\")\n",
    "    return all_roads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4790b37b-1545-4637-86d3-e34e268f9315",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cut_local_roads(date):\n",
    "    start = datetime.datetime.now()\n",
    "    # Load Shapes\n",
    "    gtfs_shape_gdf = gtfs_shapes_operators(date)\n",
    "\n",
    "    # Load local roads - buffered\n",
    "    local_roads_buffered = load_roads([\"S1400\"], True)\n",
    "\n",
    "    # Load local roads - not buffered\n",
    "    local_roads_og = load_roads([\"S1400\"], False)\n",
    "\n",
    "    # Find all unique operators, ordered by\n",
    "    # largest operators first\n",
    "    operators_list = order_operators(date)\n",
    "\n",
    "    # Empty dataframe\n",
    "    sjoin_full_results = pd.DataFrame()\n",
    "\n",
    "    # Loop through and join by operator\n",
    "    for operator in operators_list:\n",
    "\n",
    "        shapes_filtered = gtfs_shape_gdf.loc[\n",
    "            gtfs_shape_gdf.name == operator\n",
    "        ].reset_index(drop=True)\n",
    "\n",
    "        # Delete any local road linearids that have already been found by an operator\n",
    "        try:\n",
    "            linearid_to_delete = sjoin_full_results.linearid.unique().tolist()\n",
    "            local_roads_buffered = local_roads_buffered[\n",
    "                ~local_roads_buffered.linearid.isin(linearid_to_delete)\n",
    "            ].reset_index(drop=True)\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "        sjoin1 = (\n",
    "            gpd.sjoin(\n",
    "                local_roads_buffered,\n",
    "                shapes_filtered,\n",
    "                how=\"inner\",\n",
    "                predicate=\"intersects\",\n",
    "            )[[\"linearid\"]]\n",
    "            .drop_duplicates()\n",
    "            .reset_index(drop=True)\n",
    "        )\n",
    "\n",
    "        sjoin_full_results = pd.concat([sjoin_full_results, sjoin1], axis=0)\n",
    "\n",
    "    sjoin_full_results = sjoin_full_results.drop_duplicates()\n",
    "    sjoin_full_results.to_parquet(f\"{SHARED_GCS}local_roads_gtfs_shapes.parquet\")\n",
    "\n",
    "    # Merge back to original local roads -> a GDF\n",
    "    merge1 = pd.merge(local_roads_og, sjoin_full_results, on=\"linearid\", how=\"inner\")\n",
    "\n",
    "    # Fill in null values for fullname\n",
    "    merge1.fullname = merge1.fullname.fillna(\"None\")\n",
    "\n",
    "    # Save\n",
    "    merge1.to_parquet(f\"{SHARED_GCS}local_roads_gtfs_shapes_sjoin.parquet\")\n",
    "    print(\"Done with sjoin local roads\")\n",
    "\n",
    "    end = datetime.datetime.now()\n",
    "    print(f\"time lapsed for cutting local roads: {end-start}\")\n",
    "    return segments"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
