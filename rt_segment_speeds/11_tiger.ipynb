{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "18ada7ff-048d-4c5c-9523-9f191e772137",
   "metadata": {},
   "source": [
    "## Tiger Census\n",
    "* https://www2.census.gov/geo/pdfs/maps-data/data/tiger/tgrshp2019/TGRSHP2019_TechDoc.pdf\n",
    "* S1200 - secondary road\n",
    "* S1100 - primary road\n",
    "* S1400 - local roads\n",
    "* Build off scripts/cut_road_segments.py\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e1409ea6-c44a-45af-9a4b-7752f5f8cfd5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.9/site-packages/geopandas/_compat.py:123: UserWarning: The Shapely GEOS version (3.11.1-CAPI-1.17.1) is incompatible with the GEOS version PyGEOS was compiled with (3.10.1-CAPI-1.16.0). Conversions between both will be slow.\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/dask_geopandas/backends.py:13: UserWarning: Shapely 2.0 is installed, but because PyGEOS is also installed, GeoPandas will still use PyGEOS by default for now. To force to use and test Shapely 2.0, you have to set the environment variable USE_PYGEOS=0. You can do this before starting the Python process, or in your code before importing geopandas:\n",
      "\n",
      "import os\n",
      "os.environ['USE_PYGEOS'] = '0'\n",
      "import geopandas\n",
      "\n",
      "In a future release, GeoPandas will switch to using Shapely by default. If you are using PyGEOS directly (calling PyGEOS functions on geometries from GeoPandas), this will then stop working and you are encouraged to migrate from PyGEOS to Shapely 2.0 (https://shapely.readthedocs.io/en/latest/migration_pygeos.html).\n",
      "  import geopandas\n"
     ]
    }
   ],
   "source": [
    "import datetime\n",
    "import dask.dataframe as dd\n",
    "import dask_geopandas as dg\n",
    "from dask import delayed, compute\n",
    "\n",
    "import geopandas as gpd\n",
    "import pandas as pd\n",
    "from calitp_data_analysis.sql import to_snakecase\n",
    "\n",
    "import geopandas\n",
    "from segment_speed_utils import helpers\n",
    "from segment_speed_utils.project_vars import analysis_date\n",
    "from shared_utils import dask_utils, geography_utils, utils\n",
    "\n",
    "GCS_FILE_PATH = \"gs://calitp-analytics-data/data-analyses/\"\n",
    "SHARED_GCS = f\"{GCS_FILE_PATH}shared_data/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c69abcdb-7509-47c8-b891-a99c899db38b",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.options.display.max_columns = 100\n",
    "pd.options.display.float_format = \"{:.2f}\".format\n",
    "pd.set_option(\"display.max_rows\", None)\n",
    "pd.set_option(\"display.max_colwidth\", None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af59e43d-f137-4a0a-9fb9-e6ebd0a6d030",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Tiger - Load Roads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "61786354-ec33-47bd-9891-cb292068c703",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_roads(road_type_wanted: list, buffer_or_not: bool = False)-> gpd.GeoDataFrame:\n",
    "    \"\"\"\n",
    "    Load roads based on what you filter.\n",
    "    Can also buffer the roads or not.\n",
    "\n",
    "    Args:\n",
    "        road_type_wanted (list): the type of roads you want.\n",
    "        \n",
    "        https://www2.census.gov/geo/pdfs/maps-data/data/tiger/tgrshp2019/TGRSHP2019_TechDoc.pdf\n",
    "        buffer_or_not (bool): add a buffer of 200.\n",
    "\n",
    "    Returns:\n",
    "        GDF. As of 4/18/23, returns 953914 nunique linearid\n",
    "    \"\"\"\n",
    "    df = gpd.read_parquet(\n",
    "        f\"{SHARED_GCS}all_roads_2020_state06.parquet\",\n",
    "        filters=[(\"MTFCC\", \"in\", road_type_wanted)],\n",
    "        columns=[\"LINEARID\", \"geometry\", \"FULLNAME\"],\n",
    "    ).to_crs(geography_utils.CA_NAD83Albers)\n",
    "\n",
    "    if buffer_or_not:\n",
    "        df = df.assign(geometry=df.geometry.buffer(200))\n",
    "\n",
    "    df = to_snakecase(df)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45bdd874-29bf-4e67-8c07-04bd9cdda0d6",
   "metadata": {},
   "source": [
    "### GTFS Shapes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a67fb46e-c33b-4dee-82c4-dcd611443963",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gtfs_stops_operators(date) -> gpd.GeoDataFrame:\n",
    "    \"\"\"\n",
    "    Load stops with operator and\n",
    "    feed key information.\n",
    "\n",
    "    Args:\n",
    "        date: date wanted for the datasets to be drawn from\n",
    "\n",
    "    Returns:\n",
    "        GDF\n",
    "    \"\"\"\n",
    "    stops = (\n",
    "        helpers.import_scheduled_stops(\n",
    "            date, (), [\"feed_key\", \"stop_id\", \"stop_key\", \"geometry\"]\n",
    "        )\n",
    "        .compute()\n",
    "        .drop_duplicates()\n",
    "    )\n",
    "\n",
    "    stops = stops.set_crs(geography_utils.CA_NAD83Albers)\n",
    "\n",
    "    # Buffer each stop by 50 feet\n",
    "    stops = stops.assign(buffered_geometry=stops.geometry.buffer(50))\n",
    "\n",
    "    # Set geometry\n",
    "    stops = stops.set_geometry(\"buffered_geometry\")\n",
    "\n",
    "    # Merge for operator information\n",
    "    trips = (\n",
    "        helpers.import_scheduled_trips(analysis_date, (), [\"name\", \"feed_key\"])\n",
    "        .compute()\n",
    "        .drop_duplicates()\n",
    "    )\n",
    "    \n",
    "    \n",
    "    m1 = pd.merge(stops, trips, on=[\"feed_key\"], how=\"left\")\n",
    "\n",
    "    # Fill in na\n",
    "    m1.name = m1.name.fillna(\"None\")\n",
    "\n",
    "    return m1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0e17d8f7-07ca-48b7-9b9d-e28cbb3b0285",
   "metadata": {},
   "outputs": [],
   "source": [
    "# stops = gtfs_stops_operators(analysis_date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "af866cf6-65d8-4d4f-96d4-a2eb8f3bf88b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gtfs_routes_operators(date) -> gpd.GeoDataFrame:\n",
    "    \"\"\"\n",
    "    Load routes with operator and feed key information.\n",
    "\n",
    "    Args:\n",
    "        date: date wanted for the datasets to be drawn from\n",
    "\n",
    "    Returns:\n",
    "        GeoDataFrame\n",
    "    \"\"\"\n",
    "    gtfs_shapes = helpers.import_scheduled_shapes(date).compute().drop_duplicates()\n",
    "    \n",
    "    gtfs_shapes = gtfs_shapes.set_crs(geography_utils.CA_NAD83Albers)\n",
    "    \n",
    "    trips = (\n",
    "        helpers.import_scheduled_trips(date, (), [\"name\", \"shape_array_key\"])\n",
    "        .compute()\n",
    "        .drop_duplicates()\n",
    "    )\n",
    "    \n",
    "    m1 = pd.merge(gtfs_shapes, trips, how=\"outer\", on=\"shape_array_key\")\n",
    "\n",
    "    return m1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ee6d2496-861a-4cbb-a3d9-98fab1735da5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def order_operators(date) -> list:\n",
    "    \"\"\"\n",
    "    Re order a list of operators the largest\n",
    "    ones will be at the top of the list.\n",
    "\n",
    "    Args:\n",
    "        date: date wanted for the datasets to be drawn from\n",
    "    \"\"\"\n",
    "    operator_list = (\n",
    "        helpers.import_scheduled_trips(date, (), [\"name\"]).compute().sort_values(\"name\")\n",
    "    )\n",
    "    operator_list = operator_list.name.unique().tolist()\n",
    "\n",
    "    # Reorder list so the biggest operators are at the beginning\n",
    "    # based on NTD services data\n",
    "    big_operators = [\n",
    "        \"LA DOT Schedule\",\n",
    "        \"LA Metro Bus Schedule\",\n",
    "        \"LA Metro Rail Schedule\",\n",
    "        \"Bay Area 511 Muni Schedule\",\n",
    "        \"Bay Area 511 AC Transit Schedule\",\n",
    "        \"Bay Area 511 Santa Clara Transit Schedule\",\n",
    "        \"Bay Area 511 BART Schedule\",\n",
    "        \"San Diego Schedule\",\n",
    "        \"OCTA Schedule\",\n",
    "        \"Sacramento Schedule\",\n",
    "        \"Bay Area 511 Sonoma-Marin Area Rail Transit Schedule\",\n",
    "        \"Bay Area 511 SFO AirTrain Schedule\",\n",
    "        \"Bay Area 511 South San Francisco Shuttle Schedule\",\n",
    "        \"Bay Area 511 Marin Schedule\",\n",
    "        \"Bay Area 511 County Connection Schedule\",\n",
    "        \"Bay Area 511 MVGO Schedule\",\n",
    "        \"Bay Area 511 Commute.org Schedule\",\n",
    "        \"Bay Area 511 Union City Transit Schedule\",\n",
    "        \"Bay Area 511 BART Schedule\",\n",
    "        \"Bay Area 511 Caltrain Schedule\",\n",
    "        \"Bay Area 511 Fairfield and Suisun Transit Schedule\",\n",
    "        \"Bay Area 511 Dumbarton Express Schedule\",\n",
    "        \"Bay Area 511 SamTrans Schedule\",\n",
    "        \"Bay Area 511 Vine Transit Schedule\",\n",
    "        \"Bay Area 511 Tri-Valley Wheels Schedule\",\n",
    "        \"Bay Area 511 Sonoma County Transit Schedule\",\n",
    "        \"Bay Area 511 Santa Rosa CityBus Schedule\",\n",
    "        \"Bay Area 511 Golden Gate Transit Schedule\",\n",
    "        \"Bay Area 511 Golden Gate Ferry Schedule\",\n",
    "        \"Bay Area 511 San Francisco Bay Ferry Schedule\",\n",
    "        \"Bay Area 511 SolTrans Schedule\",\n",
    "        \"Bay Area 511 ACE Schedule\",\n",
    "        \"Bay Area 511 Emery Go-Round Schedule\",\n",
    "        \"Bay Area 511 Tri Delta Schedule\",\n",
    "        \"Bay Area 511 Petaluma Schedule\",\n",
    "        \"Bay Area 511 Capitol Corridor Schedule\",\n",
    "    ]\n",
    "\n",
    "    # Delete off the big operators\n",
    "    operator_list = list(set(operator_list) - set(big_operators))\n",
    "\n",
    "    # Add back in the operators\n",
    "    final_list = big_operators + operator_list\n",
    "\n",
    "    return final_list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "876aad40-5b71-42b9-b899-11ecd15e2cfe",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Tiger Local Roads"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4002d6ad-0bac-45f5-a7a0-c9358fc39376",
   "metadata": {},
   "source": [
    "#### Cut all roads - stops 1st then routes \n",
    "* Use some small operators to test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "80e1cb44-e512-4de9-8a8c-d00621db840a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def loop_sjoin(date, local_roads_gdf, gdf_routes_stops) -> gpd.GeoDataFrame:\n",
    "    \n",
    "    \"\"\"\n",
    "    By operator, sjoin its routes/stops to \n",
    "    local roads gdf. Delete off any linear ids that are joined\n",
    "    to save time/memory.\n",
    "    \n",
    "    Args:\n",
    "        local_roads_gdf: local roads gdf (should be buffered roads).\n",
    "        gdf_routes_stops: stops or routes gdf\n",
    "        date: date wanted for the datasets to be drawn from\n",
    "\n",
    "    Returns:\n",
    "        GDF\n",
    "    \"\"\"\n",
    "    # Empty dataframe\n",
    "    sjoin_full_results = pd.DataFrame()\n",
    "    \n",
    "    # Find all unique operators, ordered by largest operators first\n",
    "    operators_list = order_operators(date)\n",
    "    \n",
    "    # Loop through and sjoin by operator\n",
    "    for operator in operators_list:\n",
    "        shapes_filtered = gdf_routes_stops.loc[\n",
    "            gdf_routes_stops.name == operator\n",
    "        ].reset_index(drop=True)\n",
    "\n",
    "        # Delete any local road linear ids that have already been found by an operator\n",
    "        try:\n",
    "            # List of linear IDS\n",
    "            linearid_to_delete = sjoin_full_results.linearid.unique().tolist()\n",
    "            \n",
    "            # Filter out the linear IDS in buffered local roads\n",
    "            local_roads_gdf = local_roads_gdf[\n",
    "                ~local_roads_gdf.linearid.isin(linearid_to_delete)\n",
    "            ].reset_index(drop=True)\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "        sjoin1 = (\n",
    "            gpd.sjoin(\n",
    "                local_roads_gdf,\n",
    "                shapes_filtered,\n",
    "                how=\"inner\",\n",
    "                predicate=\"intersects\",\n",
    "            )[[\"linearid\"]]\n",
    "            .drop_duplicates()\n",
    "            .reset_index(drop=True)\n",
    "        )\n",
    "\n",
    "        sjoin_full_results = pd.concat([sjoin_full_results, sjoin1], axis=0)\n",
    "\n",
    "    sjoin_full_results = sjoin_full_results.drop_duplicates()\n",
    "    \n",
    "    return sjoin_full_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "98c2aa4c-1260-4b7b-b4f4-8be3c91a0bce",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sjoin_stops(buffered_roads, original_roads, date):\n",
    "    \"\"\"\n",
    "    Sjoin stops to local roads.\n",
    "    \n",
    "    Returns:\n",
    "        A list of linear IDs that have already\n",
    "        been found and a GDF.\n",
    "    \"\"\"\n",
    "    start = datetime.datetime.now()\n",
    "    \n",
    "    # Load stops\n",
    "    gtfs_stops = gtfs_stops_operators(date)\n",
    "    \n",
    "    # Load local roads - buffered\n",
    "    # local_roads_buffered = load_roads([\"S1400\"], True)\n",
    "\n",
    "    # Load local roads - not buffered\n",
    "    # local_roads_og = load_roads([\"S1400\"], False)\n",
    "\n",
    "    # Loop through and sjoin by operator\n",
    "    stops_sjoin = loop_sjoin(date, buffered_roads, gtfs_stops)\n",
    "\n",
    "    # Merge back to original local roads \n",
    "    m1 = pd.merge(original_roads, stops_sjoin, on=\"linearid\", how=\"inner\")\n",
    "\n",
    "    # Fill in null values for fullname\n",
    "    m1.fullname = m1.fullname.fillna(\"None\")\n",
    "    \n",
    "    # Find linear ids to delete\n",
    "    linearid_to_delete = m1.linearid.unique().tolist()\n",
    "    \n",
    "    # Save\n",
    "    m1.to_parquet(f\"{SHARED_GCS}local_roads_stops_sjoin.parquet\")\n",
    "    \n",
    "    end = datetime.datetime.now()\n",
    "    print(f\"Done with sjoin with stops with roads. Time lapsed: {end-start}\")\n",
    "    \n",
    "    return m1, linearid_to_delete"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e97a8499-6db4-4f20-ae60-a4845988515e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sjoin_routes(buffered_roads, original_roads, date, linearid_to_filter:list):\n",
    "    start = datetime.datetime.now()\n",
    "    \n",
    "    # Load stops\n",
    "    gtfs_routes = gtfs_routes_operators(date)\n",
    "    \n",
    "    # Delete out linear ids that have already been found\n",
    "    local_roads_buffered = buffered_roads[~buffered_roads.linearid.isin(linearid_to_filter)].reset_index(drop=True)\n",
    "    local_roads_og = original_roads[~original_roads.linearid.isin(linearid_to_filter)].reset_index(drop=True)\n",
    "    \n",
    "    routes_sjoin = loop_sjoin(date, local_roads_buffered, gtfs_routes)\n",
    "    \n",
    "    # Merge back to original local roads \n",
    "    m1 = pd.merge(local_roads_og, routes_sjoin, on=\"linearid\", how=\"inner\")\n",
    "\n",
    "    # Fill in null values for fullname\n",
    "    m1.fullname = m1.fullname.fillna(\"None\")\n",
    "    \n",
    "    # Save\n",
    "    m1.to_parquet(f\"{SHARED_GCS}local_roads_routes_sjoin.parquet\")\n",
    "    \n",
    "    end = datetime.datetime.now()\n",
    "    print(f\"Done with sjoin with routes and local roads. Time lapsed: {end-start}\")\n",
    "    \n",
    "    return m1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d6c298fb-28a8-44b2-b423-e3c6ac75ff7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sjoin_local_roads(date):\n",
    "    \"\"\"\n",
    "    Sjoin local roads with stops first, then routes.\n",
    "    \"\"\"\n",
    "    start = datetime.datetime.now()\n",
    "    print(start)\n",
    "    \n",
    "    # Load local roads - buffered\n",
    "    local_roads_buffered = load_roads([\"S1400\"], True)\n",
    "    \n",
    "    # Load local roads - not buffered\n",
    "    local_roads_og = load_roads([\"S1400\"], False)\n",
    "    \n",
    "    # Deal with stops first\n",
    "    stops_sjoin, linear_id_stops = sjoin_stops(local_roads_buffered, local_roads_og, date)\n",
    "    \n",
    "    # Move onto routes\n",
    "    routes_sjoin = sjoin_routes(local_roads_buffered, local_roads_og, date, linear_id_stops)\n",
    "    \n",
    "    # Stack\n",
    "    all_local_roads = pd.concat([stops_sjoin, routes_sjoin], axis=0)\n",
    "    \n",
    "    all_local_roads.to_parquet(f\"{SHARED_GCS}local_roads_all_routes_stops_sjoin.parquet\")\n",
    "    end = datetime.datetime.now()\n",
    "    \n",
    "    print(f\"Done with doing an sjoin w/ all local roads. Time lapsed: {end-start}\")\n",
    "    return all_local_roads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "992cd1c8-3aa0-40ba-a201-4b840cb02d95",
   "metadata": {},
   "outputs": [],
   "source": [
    "# all_ops = cut_local_roads(analysis_date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2b7ecbb8-0f54-4bd5-bd33-e0c8ed10f4f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# all_ops.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a821e63b-0878-4eb3-893a-b5aab32a5519",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  all_ops.sample()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7f5f75cc-3514-41ff-9bc7-5f1fc98d6750",
   "metadata": {},
   "outputs": [],
   "source": [
    "# all_ops.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6e48ae43-89d1-496e-bf64-6d7c23423156",
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunk_dask_df(gdf, chunk_row_size:int):\n",
    "    \"\"\"\n",
    "    Break up dataframes by a certain \n",
    "    number of rows, turn them into a dask\n",
    "    dataframe\n",
    "    \n",
    "    Args:\n",
    "        gdf: the local roads that intersect w/ stops and routes\n",
    "        chunk_row_size(int): how many rows each dataframe should\n",
    "        be after splitting it out.\n",
    "    \n",
    "    Returns:\n",
    "        List of dask dataframes. Length of how many dask dataframes\n",
    "        are returned after cutting.\n",
    "    \"\"\"\n",
    "    # Specify how many rows I want the gdf to broken into per df\n",
    "    n = chunk_row_size\n",
    "    \n",
    "    # Break it out\n",
    "    list_df = [gdf[i:i+n] for i in range(0,gdf.shape[0],n)]\n",
    "    \n",
    "    # Turn each dataframe to a dask one\n",
    "    my_ddfs = []\n",
    "    for df in list_df:\n",
    "        ddf = dd.from_pandas(df, npartitions= 1)\n",
    "        my_ddfs.append(ddf)\n",
    "    \n",
    "    return my_ddfs, len(my_ddfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "90c61251-083c-4b0c-a3e4-93448b26afcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ddfs, length = chunk_dask_df(all_ops, 10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "821ed543-0b04-474a-924d-ca4ec5786265",
   "metadata": {},
   "outputs": [],
   "source": [
    "# len(ddfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "328f7871-6dfa-4eca-bc22-df6b5e9e03ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# type(ddfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7621c1cc-8499-46ea-85fa-0675d230a5fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# type(ddfs[0]), type(ddfs[15])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "894db252-681e-4cf3-9964-0dd14c6473a0",
   "metadata": {},
   "source": [
    "* Second part, dfs 16 to 32 took 39 minutes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "4f8048ab-d3a9-4a33-9aeb-46879f341a7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dask_segment(ddf_list:list, ddfs_range: list)-> gpd.GeoDataFrame:\n",
    "    \"\"\"\n",
    "    Use dask to cut roads into segments.\n",
    "    \n",
    "    Args:\n",
    "        ddf_list: dask dataframes stored in a list. \n",
    "        ddf_list[0] will yield a ddf.\n",
    "        \n",
    "        ddfs_range: how many items are in the ddf_list.\n",
    "    \n",
    "    Returns:\n",
    "        A GDF.\n",
    "    \"\"\"\n",
    "    # Empty dataframe\n",
    "    my_results = []\n",
    "    \n",
    "    # For each dask dataframe int the list\n",
    "    # cut them and append the results into the empty df.\n",
    "    for i in ddfs_range:\n",
    "        my_df = ddf_list[i]\n",
    "        cut_geometry = delayed(geography_utils.cut_segments)(my_df, [\"linearid\", \"fullname\"], 1_000)\n",
    "        my_results.append(cut_geometry)\n",
    "        print(f\"done with {i}\")\n",
    "        \n",
    "    # Compute results into a normal gdf\n",
    "    compute_results = [compute(i)[0] for i in my_results] \n",
    "    \n",
    "    # Concat results\n",
    "    results_gdf = pd.concat(compute_results)\n",
    "    \n",
    "    return results_gdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a8245e97-95b3-4db0-aa70-49fce1925f3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test = dask_segment(ddfs, [0,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "260c7c04-6e36-4727-93ab-6ec081155448",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "7cf61650-c7e7-4575-bfe9-6e424f1a2847",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cut_local_roads(date, chunk_row_size: int)-> gpd.GeoDataFrame:\n",
    "    start = datetime.datetime.now()\n",
    "    print(start)\n",
    "    \n",
    "    # Find all local roads that intersect with\n",
    "    # stops and routes.\n",
    "    local_roads_unsegmented = sjoin_local_roads(date)\n",
    "    \n",
    "    # Divide the gdf into equal sized chunks (roughly)\n",
    "    # and turn them into dask gdfs\n",
    "    ddfs, length = chunk_dask_df(local_roads_unsegmented, chunk_row_size)\n",
    "    \n",
    "    # Split the list of split dask dataframes\n",
    "    # into half. Seems to run better like this?? Kernel \n",
    "    # does not die?\n",
    "    length_list = [*range(0,length)]\n",
    "    ddf1 = length_list[:len(length_list)//2]\n",
    "    ddf2 = length_list[len(length_list)//2:]\n",
    "    \n",
    "    part1 = dask_segment(ddfs, ddf1)\n",
    "    print(\"Done with cutting part1\")\n",
    "    \n",
    "    part2 = dask_segment(ddfs, ddf2)\n",
    "    print(\"Done with cutting part2\")\n",
    "    \n",
    "    segmented_local_roads = pd.concat([part1, part1])\n",
    "    segmented_local_roads.to_parquet(f\"{SHARED_GCS}all_segmented_local_rds.parquet\")\n",
    "    \n",
    "    end = datetime.datetime.now()\n",
    "    print(f\"Took {end-start} minutes\")\n",
    "    return segmented_local_roads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "63f2492d-87b9-4b1c-85c1-516f7096b5cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-04-24 14:39:43.974082\n",
      "2023-04-24 14:39:43.974865\n",
      "Done with sjoin with stops with roads. Time lapsed: 0:02:57.666337\n",
      "Done with sjoin with routes and local roads. Time lapsed: 0:05:19.120140\n",
      "Done with doing an sjoin w/ all local roads. Time lapsed: 0:12:12.809197\n",
      "Took 1:27:24.128682 minutes\n"
     ]
    }
   ],
   "source": [
    "test = cut_local_roads(analysis_date, 10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "4894d5d7-bc2f-4b64-aadf-272fa68fe47c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#my_results = []\n",
    "#for i in [16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32]:\n",
    "#    my_df = ddfs[i]\n",
    "#    cut_geometry = delayed(geography_utils.cut_segments)(my_df, [\"linearid\", \"fullname\"], 1_000)\n",
    "#    \n",
    "#    my_results.append(cut_geometry)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04bed90d-8c94-49dc-855f-c23fdb3fc30b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#start = datetime.datetime.now()\n",
    "#print(start)\n",
    "#results = [compute(i)[0] for i in my_results] \n",
    "#end = datetime.datetime.now()\n",
    "#print(end)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19783430-a382-4e12-b4a5-5f2adba3313a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# results_gdf = pd.concat(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a74ad21-4665-4540-bd54-259f340f958a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# results_gdf.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f90b9955-d2bb-4e0a-b7b9-ed99f8740e3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# results_gdf.to_parquet(f\"{SHARED_GCS}segmented_local_rds_second_pt_sjoin.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd4fd082-4823-4267-ad9b-62b3e8d2a93b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# results1 = gpd.read_parquet('gs://calitp-analytics-data/data-analyses/shared_data/segmented_local_rds_first_pt_sjoin.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51c4d917-c0d5-46b3-8db4-1228f5d30af2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# results1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51fda3be-261c-4479-b016-cba4ffea96c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# results2 = gpd.read_parquet('gs://calitp-analytics-data/data-analyses/shared_data/segmented_local_rds_second_pt_sjoin.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "867cfa8b-a705-48e7-b623-dc3952ee327d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# all_local_roads = pd.concat([results1, results2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5cb5f06-5627-4628-a94b-0f9ae5b483fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# len(all_local_roads), all_local_roads.linearid.nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60b9c3d3-0198-45f1-b602-cd7ee597436b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# all_local_roads.groupby(['linearid']).agg({'segment_sequence':'nunique'}).describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f1e4988-8a9a-4a58-836e-28a7450a9392",
   "metadata": {},
   "outputs": [],
   "source": [
    "# type(results_gdf), results_gdf.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76fc064d-991c-4bd1-aa7d-8cf36dac1630",
   "metadata": {},
   "source": [
    "### Concat local roads and primary/secondary ones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89d85183-516f-4b88-a6c7-5da78281fc48",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cut_primary_secondary_roads():\n",
    "    start = datetime.datetime.now()\n",
    "    print(start)\n",
    "\n",
    "    # Find all primary and secondary roads\n",
    "    # regardless of intersection w/ GTFS shapes\n",
    "    primary_secondary_mtfcc = [\"S1100\", \"S1200\"]\n",
    "    primary_secondary_roads = load_roads(primary_secondary_mtfcc)\n",
    "\n",
    "    segments = geography_utils.cut_segments(\n",
    "        primary_secondary_roads, [\"linearid\", \"fullname\"], 1_000  # 1 km segments\n",
    "    )\n",
    "\n",
    "    segments.to_parquet(f\"{SHARED_GCS}segmented_primary_secondary_roads.parquet\")\n",
    "\n",
    "    end = datetime.datetime.now()\n",
    "    print(f\"time lapsed for cutting primary & secondary roads: {end-start}\")\n",
    "    return segments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d2a7a9c-1102-4a3c-a673-cd4cba2a204b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# primary_secondary = cut_primary_secondary_roads()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0ff123b-81fa-44b3-825d-ea9b8b68625d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cut_all_roads(date):\n",
    "    start = datetime.datetime.now()\n",
    "\n",
    "    # Segment primary and secondary roads\n",
    "    segmented_primary_secondary_rds = cut_primary_secondary_roads()\n",
    "\n",
    "    # Find local roads that intersect  with GTFS shapes, then\n",
    "    # segment them\n",
    "    local_roads_gdf = cut_local_roads(date)\n",
    "\n",
    "    # Concat\n",
    "    all_roads = pd.concat(\n",
    "        [segmented_primary_secondary_rds, segmented_local_rds], axis=0\n",
    "    )\n",
    "    all_roads.to_parquet(f\"{SHARED_GCS}segmented_all_roads.parquet\")\n",
    "\n",
    "    end = datetime.datetime.now()\n",
    "    print(f\"time lapsed: {end-start}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3482f156-a50b-4eb9-8a3e-130e9fe2b7c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_all_roads = cut_all_roads(analysis_date)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "791d8a17-f512-4a82-b465-b0f4ef2cdbbc",
   "metadata": {},
   "source": [
    "### Draft"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d45f4029-5dac-462e-98a6-4187c1e0c415",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def cut_stops(date):\n",
    "    start = datetime.datetime.now()\n",
    "    print(start)\n",
    "    \n",
    "    # Load stops\n",
    "    gtfs_stops = gtfs_stops_operators(date)\n",
    "    \n",
    "    # Load local roads - buffered\n",
    "    local_roads_buffered = load_roads([\"S1400\"], True)\n",
    "\n",
    "    # Load local roads - not buffered\n",
    "    local_roads_og = load_roads([\"S1400\"], False)\n",
    "\n",
    "    # Empty dataframe\n",
    "    sjoin_full_results = pd.DataFrame()\n",
    "\n",
    "    # Loop through and sjoin by operator\n",
    "    for operator in ['TART, North Lake Tahoe Schedule','Eastern Sierra Schedule']:\n",
    "        shapes_filtered = gtfs_stops.loc[\n",
    "            gtfs_stops.name == operator\n",
    "        ].reset_index(drop=True)\n",
    "\n",
    "        # Delete any local road linear ids that have already been found by an operator\n",
    "        try:\n",
    "            # List of linear IDS\n",
    "            linearid_to_delete = sjoin_full_results.linearid.unique().tolist()\n",
    "            \n",
    "            # Filter out the linear IDS in buffered local roads\n",
    "            local_roads_buffered = local_roads_buffered[\n",
    "                ~local_roads_buffered.linearid.isin(linearid_to_delete)\n",
    "            ].reset_index(drop=True)\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "        sjoin1 = (\n",
    "            gpd.sjoin(\n",
    "                local_roads_buffered,\n",
    "                shapes_filtered,\n",
    "                how=\"inner\",\n",
    "                predicate=\"intersects\",\n",
    "            )[[\"linearid\"]]\n",
    "            .drop_duplicates()\n",
    "            .reset_index(drop=True)\n",
    "        )\n",
    "\n",
    "        sjoin_full_results = pd.concat([sjoin_full_results, sjoin1], axis=0)\n",
    "\n",
    "    sjoin_full_results = sjoin_full_results.drop_duplicates()\n",
    "\n",
    "    # Merge back to original local roads \n",
    "    m1 = pd.merge(local_roads_og, sjoin_full_results, on=\"linearid\", how=\"inner\")\n",
    "\n",
    "    # Fill in null values for fullname\n",
    "    m1.fullname = m1.fullname.fillna(\"None\")\n",
    "    \n",
    "    # Save\n",
    "    # m1.to_parquet(f\"{SHARED_GCS}local_roads_gtfs_shapes_sjoin.parquet\")\n",
    "    \n",
    "    end = datetime.datetime.now()\n",
    "    print(f\"Done with sjoin with stops and local roads. Time lapsed: {end-start}\")\n",
    "    return m1, linearid_to_delete"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a4b03f6-3a06-42d3-bac0-96b46a900c33",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def cut_routes(date, linearid_to_filter:list):\n",
    "    start = datetime.datetime.now()\n",
    "    print(start)\n",
    "    \n",
    "    # Load stops\n",
    "    gtfs_routes = gtfs_routes_operators(date)\n",
    "    \n",
    "    # Load local roads - buffered\n",
    "    local_roads_buffered = load_roads([\"S1400\"], True)\n",
    "    # Delete out linear ids that have already been found\n",
    "    local_roads_buffered = local_roads_buffered[~local_roads_buffered.linearid.isin(linearid_to_filter)].reset_index(drop=True)\n",
    "    \n",
    "    # Load local roads - not buffered\n",
    "    local_roads_og = load_roads([\"S1400\"], False)\n",
    "    local_roads_og = local_roads_og[~local_roads_og.linearid.isin(linearid_to_filter)].reset_index(drop=True)\n",
    "\n",
    "    # Find all unique operators, ordered by largest operators first\n",
    "    # operators_list = order_operators(date)\n",
    "\n",
    "    # Empty dataframe\n",
    "    sjoin_full_results = pd.DataFrame()\n",
    "\n",
    "    # Loop through and sjoin by operator\n",
    "    for operator in ['TART, North Lake Tahoe Schedule','Eastern Sierra Schedule']:\n",
    "        shapes_filtered = gtfs_routes.loc[\n",
    "            gtfs_routes.name == operator\n",
    "        ].reset_index(drop=True)\n",
    "\n",
    "        # Delete any local road linear ids that have already been found by an operator\n",
    "        try:\n",
    "            # List of linear IDS\n",
    "            linearid_to_delete = sjoin_full_results.linearid.unique().tolist()\n",
    "            \n",
    "            # Filter out the linear IDS in buffered local roads\n",
    "            local_roads_buffered = local_roads_buffered[\n",
    "                ~local_roads_buffered.linearid.isin(linearid_to_delete)\n",
    "            ].reset_index(drop=True)\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "        sjoin1 = (\n",
    "            gpd.sjoin(\n",
    "                local_roads_buffered,\n",
    "                shapes_filtered,\n",
    "                how=\"inner\",\n",
    "                predicate=\"intersects\",\n",
    "            )[[\"linearid\"]]\n",
    "            .drop_duplicates()\n",
    "            .reset_index(drop=True)\n",
    "        )\n",
    "\n",
    "        sjoin_full_results = pd.concat([sjoin_full_results, sjoin1], axis=0)\n",
    "\n",
    "    sjoin_full_results = sjoin_full_results.drop_duplicates()\n",
    "\n",
    "    # Merge back to original local roads \n",
    "    m1 = pd.merge(local_roads_og, sjoin_full_results, on=\"linearid\", how=\"inner\")\n",
    "\n",
    "    # Fill in null values for fullname\n",
    "    m1.fullname = m1.fullname.fillna(\"None\")\n",
    "    \n",
    "    # Save\n",
    "    # m1.to_parquet(f\"{SHARED_GCS}local_roads_gtfs_shapes_sjoin.parquet\")\n",
    "    \n",
    "    end = datetime.datetime.now()\n",
    "    print(f\"Done with sjoin with stops and local roads. Time lapsed: {end-start}\")\n",
    "    return m1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7dad6db-fb7c-4b67-b783-fd7a2714365f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "def join_primary_secondary(gtfs_shape_gdf):\n",
    "    \n",
    "    # Load secondary-primary roads\n",
    "    primary_secondary_mtfcc = ['S1100','S1200']\n",
    "    primary_secondary_roads = load_roads(primary_secondary_mtfcc)\n",
    "    \n",
    "    sjoin1 = gpd.sjoin(\n",
    "        primary_secondary_roads,\n",
    "        gtfs_shape_gdf,\n",
    "        how = \"inner\",\n",
    "        predicate = \"intersects\"\n",
    "    ).drop_duplicates().reset_index(drop=True)\n",
    "    \n",
    "    # Save to GCS\n",
    "    sjoin1.to_parquet(f'{SHARED_GCS}primary_secondary_roads_gtfs_shapes.parquet')\n",
    "    print('Done with primary and secondary roads')\n",
    "    return sjoin1\n",
    "    \"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06844ac3-a82a-4aa1-89e2-e284cc1eefaa",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Test w/ LA Metro first - 25 minutes long"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc02467a-d8dd-42ba-a782-75cb4bdb379d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def la_metro(date):\n",
    "\n",
    "    # Load Shapes\n",
    "    gtfs_shape_gdf = gtfs_shapes_operators(date)\n",
    "\n",
    "    # Load local roads - buffered\n",
    "    local_roads_buffered = load_roads([\"S1400\"], True)\n",
    "\n",
    "    # Load local roads - not buffered\n",
    "    local_roads_og = load_roads([\"S1400\"], False)\n",
    "\n",
    "    # Empty dataframe\n",
    "    sjoin_full_results = pd.DataFrame()\n",
    "\n",
    "    # Loop through and join by operator\n",
    "    for operator in [\n",
    "        \"LA Metro Bus Schedule\",\n",
    "        \"LA Metro Rail Schedule\",\n",
    "    ]:\n",
    "\n",
    "        shapes_filtered = gtfs_shape_gdf.loc[\n",
    "            gtfs_shape_gdf.name == operator\n",
    "        ].reset_index(drop=True)\n",
    "\n",
    "        # Delete any local road linearids that have already been found by an operator\n",
    "        try:\n",
    "            linearid_to_delete = sjoin_full_results.linearid.unique().tolist()\n",
    "            local_roads_buffered = local_roads_buffered[\n",
    "                ~local_roads_buffered.linearid.isin(linearid_to_delete)\n",
    "            ].reset_index(drop=True)\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "        sjoin1 = (\n",
    "            gpd.sjoin(\n",
    "                local_roads_buffered,\n",
    "                shapes_filtered,\n",
    "                how=\"inner\",\n",
    "                predicate=\"intersects\",\n",
    "            )[[\"linearid\"]]\n",
    "            .drop_duplicates()\n",
    "            .reset_index(drop=True)\n",
    "        )\n",
    "\n",
    "        sjoin_full_results = pd.concat([sjoin_full_results, sjoin1], axis=0)\n",
    "\n",
    "    sjoin_full_results = sjoin_full_results.drop_duplicates()\n",
    "\n",
    "    # Merge back to original local roads -> a GDF\n",
    "    merge1 = pd.merge(local_roads_og, sjoin_full_results, on=\"linearid\", how=\"inner\")\n",
    "\n",
    "    # Fill in null values for fullname\n",
    "    merge1.fullname = merge1.fullname.fillna(\"None\")\n",
    "\n",
    "    # Save\n",
    "    print(\"Done with LA Metro\")\n",
    "\n",
    "    return merge1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63b5b9cd-11b3-4e52-b50d-809d1091626c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2023-04-10 11:20:11.194365\n",
    "# Done with LA Metro\n",
    "# time lapsed: 0:25:57.435602\n",
    "def cut_metro(date):\n",
    "    start = datetime.datetime.now()\n",
    "    print(start)\n",
    "\n",
    "    # Find local roads that intersect  with LA Metro\n",
    "    local_roads = la_metro(date)\n",
    "\n",
    "    segments = geography_utils.cut_segments(\n",
    "        local_roads, [\"linearid\", \"fullname\"], 1_000  # 1 km segments\n",
    "    )\n",
    "\n",
    "    end = datetime.datetime.now()\n",
    "    print(f\"time lapsed: {end-start}\")\n",
    "\n",
    "    return segments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4566492f-36b9-44a5-90bb-5117ebaff339",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gtfs_shapes_all_roads(date):\n",
    "\n",
    "    start = datetime.datetime.now()\n",
    "    print(start)\n",
    "\n",
    "    # Find all primary and secondary roads\n",
    "    # regardless of intersection\n",
    "    primary_secondary_mtfcc = [\"S1100\", \"S1200\"]\n",
    "    primary_secondary_roads = load_roads(primary_secondary_mtfcc)\n",
    "\n",
    "    # Find local roads that intersect  with GTFS shapes\n",
    "    local_roads_gdf = join_local_roads(date)\n",
    "\n",
    "    # Concat\n",
    "    all_roads = pd.concat([primary_secondary_roads, local_roads_gdf], axis=0)\n",
    "\n",
    "    # Save\n",
    "    all_roads.to_parquet(f\"{SHARED_GCS}intersected_local_all_primary_sec_roads.parquet\")\n",
    "\n",
    "    \"\"\"\n",
    "    # Find segments\n",
    "    segments = geography_utils.cut_segments(\n",
    "        all_roads,\n",
    "        [\"linearid\", \"fullname\"],\n",
    "        1_000 # 1 km segments\n",
    "    )\n",
    "    \n",
    "    segments.to_parquet(f\"{SHARED_GCS}census_road_segments.parquet\")\n",
    "    \"\"\"\n",
    "    end = datetime.datetime.now()\n",
    "    print(f\"time lapsed: {end-start}\")\n",
    "    return all_roads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4790b37b-1545-4637-86d3-e34e268f9315",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cut_local_roads(date):\n",
    "    start = datetime.datetime.now()\n",
    "    # Load Shapes\n",
    "    gtfs_shape_gdf = gtfs_shapes_operators(date)\n",
    "\n",
    "    # Load local roads - buffered\n",
    "    local_roads_buffered = load_roads([\"S1400\"], True)\n",
    "\n",
    "    # Load local roads - not buffered\n",
    "    local_roads_og = load_roads([\"S1400\"], False)\n",
    "\n",
    "    # Find all unique operators, ordered by\n",
    "    # largest operators first\n",
    "    operators_list = order_operators(date)\n",
    "\n",
    "    # Empty dataframe\n",
    "    sjoin_full_results = pd.DataFrame()\n",
    "\n",
    "    # Loop through and join by operator\n",
    "    for operator in operators_list:\n",
    "\n",
    "        shapes_filtered = gtfs_shape_gdf.loc[\n",
    "            gtfs_shape_gdf.name == operator\n",
    "        ].reset_index(drop=True)\n",
    "\n",
    "        # Delete any local road linearids that have already been found by an operator\n",
    "        try:\n",
    "            linearid_to_delete = sjoin_full_results.linearid.unique().tolist()\n",
    "            local_roads_buffered = local_roads_buffered[\n",
    "                ~local_roads_buffered.linearid.isin(linearid_to_delete)\n",
    "            ].reset_index(drop=True)\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "        sjoin1 = (\n",
    "            gpd.sjoin(\n",
    "                local_roads_buffered,\n",
    "                shapes_filtered,\n",
    "                how=\"inner\",\n",
    "                predicate=\"intersects\",\n",
    "            )[[\"linearid\"]]\n",
    "            .drop_duplicates()\n",
    "            .reset_index(drop=True)\n",
    "        )\n",
    "\n",
    "        sjoin_full_results = pd.concat([sjoin_full_results, sjoin1], axis=0)\n",
    "\n",
    "    sjoin_full_results = sjoin_full_results.drop_duplicates()\n",
    "    sjoin_full_results.to_parquet(f\"{SHARED_GCS}local_roads_gtfs_shapes.parquet\")\n",
    "\n",
    "    # Merge back to original local roads -> a GDF\n",
    "    merge1 = pd.merge(local_roads_og, sjoin_full_results, on=\"linearid\", how=\"inner\")\n",
    "\n",
    "    # Fill in null values for fullname\n",
    "    merge1.fullname = merge1.fullname.fillna(\"None\")\n",
    "\n",
    "    # Save\n",
    "    merge1.to_parquet(f\"{SHARED_GCS}local_roads_gtfs_shapes_sjoin.parquet\")\n",
    "    print(\"Done with sjoin local roads\")\n",
    "\n",
    "    end = datetime.datetime.now()\n",
    "    print(f\"time lapsed for cutting local roads: {end-start}\")\n",
    "    return segments"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
