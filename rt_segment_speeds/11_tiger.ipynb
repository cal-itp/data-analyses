{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "18ada7ff-048d-4c5c-9523-9f191e772137",
   "metadata": {},
   "source": [
    "## Tiger Census\n",
    "* https://www2.census.gov/geo/pdfs/maps-data/data/tiger/tgrshp2019/TGRSHP2019_TechDoc.pdf\n",
    "* S1200 - secondary road\n",
    "* S1100 - primary road\n",
    "* S1400 - local roads\n",
    "* Build off scripts/cut_road_segments.py\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e1409ea6-c44a-45af-9a4b-7752f5f8cfd5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.9/site-packages/geopandas/_compat.py:123: UserWarning: The Shapely GEOS version (3.11.1-CAPI-1.17.1) is incompatible with the GEOS version PyGEOS was compiled with (3.10.1-CAPI-1.16.0). Conversions between both will be slow.\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/dask_geopandas/backends.py:13: UserWarning: Shapely 2.0 is installed, but because PyGEOS is also installed, GeoPandas will still use PyGEOS by default for now. To force to use and test Shapely 2.0, you have to set the environment variable USE_PYGEOS=0. You can do this before starting the Python process, or in your code before importing geopandas:\n",
      "\n",
      "import os\n",
      "os.environ['USE_PYGEOS'] = '0'\n",
      "import geopandas\n",
      "\n",
      "In a future release, GeoPandas will switch to using Shapely by default. If you are using PyGEOS directly (calling PyGEOS functions on geometries from GeoPandas), this will then stop working and you are encouraged to migrate from PyGEOS to Shapely 2.0 (https://shapely.readthedocs.io/en/latest/migration_pygeos.html).\n",
      "  import geopandas\n"
     ]
    }
   ],
   "source": [
    "import datetime\n",
    "import gcsfs\n",
    "fs = gcsfs.GCSFileSystem()\n",
    "import dask.dataframe as dd\n",
    "import dask_geopandas as dg\n",
    "import geopandas\n",
    "import geopandas as gpd\n",
    "import pandas as pd\n",
    "from calitp_data_analysis.sql import to_snakecase\n",
    "from dask import compute, delayed\n",
    "from segment_speed_utils import helpers\n",
    "from segment_speed_utils.project_vars import analysis_date\n",
    "from shared_utils import dask_utils, geography_utils, utils\n",
    "\n",
    "GCS_FILE_PATH = \"gs://calitp-analytics-data/data-analyses/\"\n",
    "SHARED_GCS = f\"{GCS_FILE_PATH}shared_data/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c69abcdb-7509-47c8-b891-a99c899db38b",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.options.display.max_columns = 100\n",
    "pd.options.display.float_format = \"{:.2f}\".format\n",
    "pd.set_option(\"display.max_rows\", None)\n",
    "pd.set_option(\"display.max_colwidth\", None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af59e43d-f137-4a0a-9fb9-e6ebd0a6d030",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Tiger - Load Roads\n",
    "* TO DO: remove buffer, do it another step b/c now I have to dissolve twice and that takes too long"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "61786354-ec33-47bd-9891-cb292068c703",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_roads(road_type_wanted: list, buffer_or_not: bool = False) -> gpd.GeoDataFrame:\n",
    "    \"\"\"\n",
    "    Load roads based on what you filter.\n",
    "    Can also buffer the roads or not.\n",
    "\n",
    "    Args:\n",
    "        road_type_wanted (list): the type of roads you want.\n",
    "\n",
    "        https://www2.census.gov/geo/pdfs/maps-data/data/tiger/tgrshp2019/TGRSHP2019_TechDoc.pdf\n",
    "        buffer_or_not (bool): add a buffer of 200.\n",
    "\n",
    "    Returns:\n",
    "        GDF. As of 4/18/23, returns 953914 nunique linearid\n",
    "    \"\"\"\n",
    "    df = gpd.read_parquet(\n",
    "        f\"{SHARED_GCS}all_roads_2020_state06.parquet\",\n",
    "        filters=[(\"MTFCC\", \"in\", road_type_wanted)],\n",
    "        columns=[\"LINEARID\", \"geometry\", \"FULLNAME\"],\n",
    "    ).to_crs(geography_utils.CA_NAD83Albers)\n",
    "\n",
    "    # If a road has mutliple rows but the same\n",
    "    # linear ID, dissolve it so it becomes one row.\n",
    "    df = (\n",
    "        df.drop_duplicates()\n",
    "        .dissolve(by=[\"LINEARID\"])\n",
    "        .reset_index()\n",
    "        .drop_duplicates()\n",
    "        .reset_index(drop=True)\n",
    "    )\n",
    "\n",
    "    #if buffer_or_not:\n",
    "    #    df = df.assign(geometry=df.geometry.buffer(200))\n",
    "\n",
    "    df = to_snakecase(df)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fba78127-4a24-4be4-9c92-9a2fa8034559",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "55408ee8-7dbd-4171-9575-0cb138c8b60c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# len(og_tiger)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1c2a4b80-9bec-44bb-8a2a-63ce7a11a55c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# og_tiger.linearid.nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cd327db3-345d-48f5-b673-43edb740a210",
   "metadata": {},
   "outputs": [],
   "source": [
    "# len(og_tiger.drop_duplicates())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c868617d-b244-47f3-80e4-8c941ec2fe45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# more_than_1 = og_tiger.linearid.value_counts().loc[lambda x: x>1].reset_index()['index']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f825b43b-1f58-40bc-808c-0a62fae9c510",
   "metadata": {},
   "outputs": [],
   "source": [
    "# more_than_1 = list(more_than_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "23e094ff-b20d-4d1b-b129-8a2c830ca48c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# len(more_than_1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d935930a-93da-4609-a8d8-671a4626adde",
   "metadata": {},
   "source": [
    "#### Cesar Chavez Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4e0a190a-c26d-48cc-9a57-c9ae2e8fde2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cesar_chavez = og_tiger[og_tiger.fullname == \"Cesar Chavez\"].reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "58fc873f-bb95-4129-a154-12d64e089264",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cesar_chavez"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45bdd874-29bf-4e67-8c07-04bd9cdda0d6",
   "metadata": {},
   "source": [
    "### GTFS Shapes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a67fb46e-c33b-4dee-82c4-dcd611443963",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gtfs_stops_operators(date) -> gpd.GeoDataFrame:\n",
    "    \"\"\"\n",
    "    Load stops with operator and\n",
    "    feed key information.\n",
    "\n",
    "    Args:\n",
    "        date: date wanted for the datasets to be drawn from\n",
    "\n",
    "    Returns:\n",
    "        GDF\n",
    "    \"\"\"\n",
    "    stops = (\n",
    "        helpers.import_scheduled_stops(\n",
    "            date, (), [\"feed_key\", \"stop_id\", \"stop_key\", \"geometry\"]\n",
    "        )\n",
    "        .compute()\n",
    "        .drop_duplicates()\n",
    "    )\n",
    "\n",
    "    stops = stops.set_crs(geography_utils.CA_NAD83Albers)\n",
    "\n",
    "    # Buffer each stop by 50 feet\n",
    "    stops = stops.assign(buffered_geometry=stops.geometry.buffer(50))\n",
    "\n",
    "    # Set geometry\n",
    "    stops = stops.set_geometry(\"buffered_geometry\")\n",
    "\n",
    "    # Merge for operator information\n",
    "    trips = (\n",
    "        helpers.import_scheduled_trips(analysis_date, (), [\"name\", \"feed_key\"])\n",
    "        .compute()\n",
    "        .drop_duplicates()\n",
    "    )\n",
    "\n",
    "    m1 = pd.merge(stops, trips, on=[\"feed_key\"], how=\"left\")\n",
    "\n",
    "    # Fill in na\n",
    "    m1.name = m1.name.fillna(\"None\")\n",
    "\n",
    "    return m1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0e17d8f7-07ca-48b7-9b9d-e28cbb3b0285",
   "metadata": {},
   "outputs": [],
   "source": [
    "# stops = gtfs_stops_operators(analysis_date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "af866cf6-65d8-4d4f-96d4-a2eb8f3bf88b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gtfs_routes_operators(date) -> gpd.GeoDataFrame:\n",
    "    \"\"\"\n",
    "    Load routes with operator and feed key information.\n",
    "\n",
    "    Args:\n",
    "        date: date wanted for the datasets to be drawn from\n",
    "    \"\"\"\n",
    "    gtfs_shapes = helpers.import_scheduled_shapes(date).compute().drop_duplicates()\n",
    "\n",
    "    gtfs_shapes = gtfs_shapes.set_crs(geography_utils.CA_NAD83Albers)\n",
    "\n",
    "    trips = (\n",
    "        helpers.import_scheduled_trips(date, (), [\"name\", \"shape_array_key\"])\n",
    "        .compute()\n",
    "        .drop_duplicates()\n",
    "    )\n",
    "\n",
    "    m1 = pd.merge(gtfs_shapes, trips, how=\"left\", on=\"shape_array_key\")\n",
    "\n",
    "    return m1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ee6d2496-861a-4cbb-a3d9-98fab1735da5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def order_operators(date) -> list:\n",
    "    \"\"\"\n",
    "    Re order a list of operators the largest\n",
    "    ones will be at the top of the list.\n",
    "\n",
    "    Args:\n",
    "        date: date wanted for the datasets to be drawn from\n",
    "    \"\"\"\n",
    "    operator_list = (\n",
    "        helpers.import_scheduled_trips(date, (), [\"name\"]).compute().sort_values(\"name\")\n",
    "    )\n",
    "    operator_list = operator_list.name.unique().tolist()\n",
    "\n",
    "    # Reorder list so the biggest operators are at the beginning\n",
    "    # based on NTD services data\n",
    "    big_operators = [\n",
    "        \"LA DOT Schedule\",\n",
    "        \"LA Metro Bus Schedule\",\n",
    "        \"LA Metro Rail Schedule\",\n",
    "        \"Bay Area 511 Muni Schedule\",\n",
    "        \"Bay Area 511 AC Transit Schedule\",\n",
    "        \"Bay Area 511 Santa Clara Transit Schedule\",\n",
    "        \"Bay Area 511 BART Schedule\",\n",
    "        \"San Diego Schedule\",\n",
    "        \"OCTA Schedule\",\n",
    "        \"Sacramento Schedule\",\n",
    "        \"Bay Area 511 Sonoma-Marin Area Rail Transit Schedule\",\n",
    "        \"Bay Area 511 SFO AirTrain Schedule\",\n",
    "        \"Bay Area 511 South San Francisco Shuttle Schedule\",\n",
    "        \"Bay Area 511 Marin Schedule\",\n",
    "        \"Bay Area 511 County Connection Schedule\",\n",
    "        \"Bay Area 511 MVGO Schedule\",\n",
    "        \"Bay Area 511 Commute.org Schedule\",\n",
    "        \"Bay Area 511 Union City Transit Schedule\",\n",
    "        \"Bay Area 511 BART Schedule\",\n",
    "        \"Bay Area 511 Caltrain Schedule\",\n",
    "        \"Bay Area 511 Fairfield and Suisun Transit Schedule\",\n",
    "        \"Bay Area 511 Dumbarton Express Schedule\",\n",
    "        \"Bay Area 511 SamTrans Schedule\",\n",
    "        \"Bay Area 511 Vine Transit Schedule\",\n",
    "        \"Bay Area 511 Tri-Valley Wheels Schedule\",\n",
    "        \"Bay Area 511 Sonoma County Transit Schedule\",\n",
    "        \"Bay Area 511 Santa Rosa CityBus Schedule\",\n",
    "        \"Bay Area 511 Golden Gate Transit Schedule\",\n",
    "        \"Bay Area 511 Golden Gate Ferry Schedule\",\n",
    "        \"Bay Area 511 San Francisco Bay Ferry Schedule\",\n",
    "        \"Bay Area 511 SolTrans Schedule\",\n",
    "        \"Bay Area 511 ACE Schedule\",\n",
    "        \"Bay Area 511 Emery Go-Round Schedule\",\n",
    "        \"Bay Area 511 Tri Delta Schedule\",\n",
    "        \"Bay Area 511 Petaluma Schedule\",\n",
    "        \"Bay Area 511 Capitol Corridor Schedule\",\n",
    "    ]\n",
    "\n",
    "    # Delete off the big operators\n",
    "    operator_list = list(set(operator_list) - set(big_operators))\n",
    "\n",
    "    # Add back in the operators\n",
    "    final_list = big_operators + operator_list\n",
    "\n",
    "    return final_list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "876aad40-5b71-42b9-b899-11ecd15e2cfe",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Tiger Local Roads"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4002d6ad-0bac-45f5-a7a0-c9358fc39376",
   "metadata": {},
   "source": [
    "#### Cut all roads - stops 1st then routes \n",
    "* Use some small operators to test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "80e1cb44-e512-4de9-8a8c-d00621db840a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def loop_sjoin(date, local_roads_gdf, gdf_routes_stops) -> gpd.GeoDataFrame:\n",
    "    \"\"\"\n",
    "    By operator, sjoin its routes/stops to\n",
    "    local roads gdf. Delete off any linear ids that are joined.\n",
    "\n",
    "    Args:\n",
    "        local_roads_gdf: local roads gdf (should be buffered roads).\n",
    "        gdf_routes_stops: stops or routes gdf\n",
    "        date: date wanted for the datasets to be drawn from\n",
    "    \"\"\"\n",
    "    # Empty dataframe\n",
    "    sjoin_full_results = pd.DataFrame()\n",
    "\n",
    "    # Find all unique operators, ordered by largest operators first\n",
    "    operators_list = order_operators(date)\n",
    "\n",
    "    # Loop through and sjoin by operator\n",
    "    for operator in operators_list:\n",
    "        shapes_filtered = gdf_routes_stops.loc[\n",
    "            gdf_routes_stops.name == operator\n",
    "        ].reset_index(drop=True)\n",
    "\n",
    "        # Delete any local road linear ids that have already been found by an operator\n",
    "        try:\n",
    "            # List of linear IDS\n",
    "            linearid_to_delete = sjoin_full_results.linearid.unique().tolist()\n",
    "\n",
    "            # Filter out the linear IDS in buffered local roads\n",
    "            local_roads_gdf = local_roads_gdf[\n",
    "                ~local_roads_gdf.linearid.isin(linearid_to_delete)\n",
    "            ].reset_index(drop=True)\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "        # Do a sjoin but  keep the linearid as the only column\n",
    "        sjoin1 = (\n",
    "            gpd.sjoin(\n",
    "                local_roads_gdf,\n",
    "                shapes_filtered,\n",
    "                how=\"inner\",\n",
    "                predicate=\"intersects\",\n",
    "            )[[\"linearid\"]]\n",
    "            .drop_duplicates()\n",
    "            .reset_index(drop=True)\n",
    "        )\n",
    "\n",
    "        sjoin_full_results = pd.concat([sjoin_full_results, sjoin1], axis=0)\n",
    "\n",
    "    sjoin_full_results = sjoin_full_results.drop_duplicates()\n",
    "\n",
    "    return sjoin_full_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "98c2aa4c-1260-4b7b-b4f4-8be3c91a0bce",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sjoin_stops(buffered_roads, original_roads, date):\n",
    "    \"\"\"\n",
    "    Sjoin stops to local roads.\n",
    "\n",
    "    Returns:\n",
    "        A list of linear IDs that have already\n",
    "        been found and a GDF.\n",
    "    \"\"\"\n",
    "    start = datetime.datetime.now()\n",
    "\n",
    "    # Load stops\n",
    "    gtfs_stops = gtfs_stops_operators(date)\n",
    "\n",
    "    # Loop through and sjoin by operator\n",
    "    stops_sjoin = loop_sjoin(date, buffered_roads, gtfs_stops)\n",
    "\n",
    "    # Merge back to original local roads gdf, so we have the\n",
    "    # non buffered geometry.\n",
    "    m1 = pd.merge(original_roads, stops_sjoin, on=\"linearid\", how=\"inner\")\n",
    "\n",
    "    # Fill in null values for fullname\n",
    "    m1.fullname = m1.fullname.fillna(\"None\")\n",
    "\n",
    "    # Find linear ids to delete\n",
    "    linearid_to_delete = m1.linearid.unique().tolist()\n",
    "\n",
    "    # Save\n",
    "    m1.to_parquet(f\"{SHARED_GCS}local_roads_stops_sjoin.parquet\")\n",
    "\n",
    "    end = datetime.datetime.now()\n",
    "    print(f\"Done with sjoin with stops with local roads. Time lapsed: {end-start}\")\n",
    "\n",
    "    return m1, linearid_to_delete"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e97a8499-6db4-4f20-ae60-a4845988515e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sjoin_routes(buffered_roads, original_roads, date, linearid_to_delete: list):\n",
    "    start = datetime.datetime.now()\n",
    "\n",
    "    # Load stops\n",
    "    gtfs_routes = gtfs_routes_operators(date)\n",
    "\n",
    "    # Delete out linear ids that have already been found\n",
    "    local_roads_buffered = buffered_roads[\n",
    "        ~buffered_roads.linearid.isin(linearid_to_delete)\n",
    "    ].reset_index(drop=True)\n",
    "    local_roads_og = original_roads[\n",
    "        ~original_roads.linearid.isin(linearid_to_delete)\n",
    "    ].reset_index(drop=True)\n",
    "\n",
    "    routes_sjoin = loop_sjoin(date, local_roads_buffered, gtfs_routes)\n",
    "\n",
    "    # Merge back to original local roads, so we have the\n",
    "    # non buffered geometry.\n",
    "    m1 = pd.merge(local_roads_og, routes_sjoin, on=\"linearid\", how=\"inner\")\n",
    "\n",
    "    # Fill in null values for fullname\n",
    "    m1.fullname = m1.fullname.fillna(\"None\")\n",
    "\n",
    "    # Save\n",
    "    m1.to_parquet(f\"{SHARED_GCS}local_roads_routes_sjoin.parquet\")\n",
    "\n",
    "    end = datetime.datetime.now()\n",
    "    print(f\"Done with sjoin with routes and local roads. Time lapsed: {end-start}\")\n",
    "\n",
    "    return m1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d6c298fb-28a8-44b2-b423-e3c6ac75ff7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sjoin_local_roads(date):\n",
    "    \"\"\"\n",
    "    Sjoin local roads with stops first, then routes.\n",
    "    \"\"\"\n",
    "    start = datetime.datetime.now()\n",
    "    print(f\"sjoin {start}\")\n",
    "    \n",
    "    # Load local roads - not buffered\n",
    "    local_roads_og = load_roads([\"S1400\"], False)\n",
    "    \n",
    "    # Load local roads - buffered\n",
    "    local_roads_buffered = local_roads_og.assign(geometry=local_roads_og.geometry.buffer(200))\n",
    "    print(f\"Done buffering. {datetime.datetime.now()}-start\")\n",
    "    \n",
    "    # Deal with stops first\n",
    "    stops_sjoin, linear_id_stops = sjoin_stops(\n",
    "        local_roads_buffered, local_roads_og, date\n",
    "    )\n",
    "\n",
    "    # Move onto routes\n",
    "    routes_sjoin = sjoin_routes(\n",
    "        local_roads_buffered, local_roads_og, date, linear_id_stops\n",
    "    )\n",
    "\n",
    "    # Stack\n",
    "    all_local_roads = pd.concat([stops_sjoin, routes_sjoin], axis=0)\n",
    "\n",
    "    all_local_roads.to_parquet(\n",
    "        f\"{SHARED_GCS}local_roads_all_routes_stops_sjoin.parquet\"\n",
    "    )\n",
    "    end = datetime.datetime.now()\n",
    "\n",
    "    print(f\"Done with doing an sjoin for all local roads. Time lapsed: {end-start}\")\n",
    "    return all_local_roads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "992cd1c8-3aa0-40ba-a201-4b840cb02d95",
   "metadata": {},
   "outputs": [],
   "source": [
    "# all_ops = cut_local_roads(analysis_date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2b7ecbb8-0f54-4bd5-bd33-e0c8ed10f4f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# all_ops.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a821e63b-0878-4eb3-893a-b5aab32a5519",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  all_ops.sample()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "7f5f75cc-3514-41ff-9bc7-5f1fc98d6750",
   "metadata": {},
   "outputs": [],
   "source": [
    "# all_ops.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "6e48ae43-89d1-496e-bf64-6d7c23423156",
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunk_dask_df(gdf, chunk_row_size: int):\n",
    "    \"\"\"\n",
    "    Break up dataframes by a certain\n",
    "    number of rows, turn them into a dask\n",
    "    dataframe\n",
    "\n",
    "    Args:\n",
    "        gdf: the local roads that intersect w/ stops and routes\n",
    "        chunk_row_size(int): how many rows each dataframe should\n",
    "        be after splitting it out.\n",
    "\n",
    "    Returns:\n",
    "        List of dask dataframes. Length of how many dask dataframes\n",
    "        are returned after cutting.\n",
    "    \"\"\"\n",
    "    # Specify how many rows I want the gdf to broken into per df\n",
    "    n = chunk_row_size\n",
    "\n",
    "    # Break it out\n",
    "    list_df = [gdf[i : i + n] for i in range(0, gdf.shape[0], n)]\n",
    "\n",
    "    # Turn each dataframe to a dask one\n",
    "    my_ddfs = []\n",
    "    for df in list_df:\n",
    "        ddf = dd.from_pandas(df, npartitions=1)\n",
    "        my_ddfs.append(ddf)\n",
    "\n",
    "    return my_ddfs, len(my_ddfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "90c61251-083c-4b0c-a3e4-93448b26afcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ddfs, length = chunk_dask_df(all_ops, 10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "821ed543-0b04-474a-924d-ca4ec5786265",
   "metadata": {},
   "outputs": [],
   "source": [
    "# len(ddfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "328f7871-6dfa-4eca-bc22-df6b5e9e03ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# type(ddfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "7621c1cc-8499-46ea-85fa-0675d230a5fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# type(ddfs[0]), type(ddfs[15])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "4f8048ab-d3a9-4a33-9aeb-46879f341a7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dask_segment(ddf_list: list, ddfs_range: list) -> gpd.GeoDataFrame:\n",
    "    \"\"\"\n",
    "    Use dask to cut roads into segments. Compute the results\n",
    "    back to a GDF.\n",
    "\n",
    "    Args:\n",
    "        ddf_list: dask dataframes stored in a list.\n",
    "        ddf_list[0] will yield a ddf.\n",
    "\n",
    "        ddfs_range: how many items are in the ddf_list.\n",
    "    \"\"\"\n",
    "    # Empty dataframe\n",
    "    my_results = []\n",
    "\n",
    "    # For each dask dataframe int the list\n",
    "    # cut them and append the results into the empty df.\n",
    "    for i in ddfs_range:\n",
    "        my_df = ddf_list[i]\n",
    "        cut_geometry = delayed(geography_utils.cut_segments)(\n",
    "            my_df, [\"linearid\", \"fullname\"], 1_000\n",
    "        )\n",
    "        my_results.append(cut_geometry)\n",
    "        print(f\"done with {i}\")\n",
    "\n",
    "    # Compute results into a normal gdf\n",
    "    compute_results = [compute(i)[0] for i in my_results]\n",
    "\n",
    "    # Concat results\n",
    "    results_gdf = pd.concat(compute_results)\n",
    "\n",
    "    return results_gdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "a8245e97-95b3-4db0-aa70-49fce1925f3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test = dask_segment(ddfs, [0,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "260c7c04-6e36-4727-93ab-6ec081155448",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "7cf61650-c7e7-4575-bfe9-6e424f1a2847",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cut_local_roads(date, chunk_row_size: int) -> gpd.GeoDataFrame:\n",
    "    start = datetime.datetime.now()\n",
    "    print(f\"Cut: local roads {start}\")\n",
    "\n",
    "    # Find all local roads that intersect with\n",
    "    # stops and routes.\n",
    "    local_roads_unsegmented = sjoin_local_roads(date)\n",
    "\n",
    "    # Divide the gdf into equal sized chunks (roughly)\n",
    "    # and turn them into dask gdfs\n",
    "    ddfs, length = chunk_dask_df(local_roads_unsegmented, chunk_row_size)\n",
    "\n",
    "    # Split the list of split dask dataframes\n",
    "    # into half.\n",
    "    length_list = [*range(0, length)]\n",
    "    ddf1 = length_list[: len(length_list) // 2]\n",
    "    ddf2 = length_list[len(length_list) // 2 :]\n",
    "\n",
    "    # Cut geometry\n",
    "    part1 = dask_segment(ddfs, ddf1)\n",
    "    part1.to_parquet(f\"{SHARED_GCS}segmented_local_rds_first_pt.parquet\")\n",
    "    print(\"Done with cutting part1\")\n",
    "\n",
    "    part2 = dask_segment(ddfs, ddf2)\n",
    "    part2.to_parquet(f\"{SHARED_GCS}segmented_local_rds_second_pt.parquet\")\n",
    "    print(\"Done with cutting part2\")\n",
    "\n",
    "    segmented_local_roads = pd.concat([part1, part2])\n",
    "    segmented_local_roads.to_parquet(f\"{SHARED_GCS}segmented_local_rds.parquet\")\n",
    "\n",
    "    end = datetime.datetime.now()\n",
    "    print(f\"Done cutting local roads in {end-start} minutes\")\n",
    "    return segmented_local_roads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "63f2492d-87b9-4b1c-85c1-516f7096b5cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test = cut_local_roads(analysis_date, 10000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76fc064d-991c-4bd1-aa7d-8cf36dac1630",
   "metadata": {},
   "source": [
    "### Concat local roads and primary/secondary ones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "89d85183-516f-4b88-a6c7-5da78281fc48",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cut_primary_secondary_roads():\n",
    "    start = datetime.datetime.now()\n",
    "    print(f\"Cutting primary/secondary roads {start}\")\n",
    "\n",
    "    # Find all primary and secondary roads\n",
    "    # regardless of intersection w/ GTFS shapes\n",
    "    primary_secondary_mtfcc = [\"S1100\", \"S1200\"]\n",
    "    primary_secondary_roads = load_roads(primary_secondary_mtfcc, False)\n",
    "\n",
    "    segments = geography_utils.cut_segments(\n",
    "        primary_secondary_roads, [\"linearid\", \"fullname\"], 1_000  # 1 km segments\n",
    "    )\n",
    "\n",
    "    segments.to_parquet(f\"{SHARED_GCS}segmented_primary_secondary_roads.parquet\")\n",
    "\n",
    "    end = datetime.datetime.now()\n",
    "    print(f\"Done cutting primary & secondary roads: {end-start}\")\n",
    "    return segments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "3d2a7a9c-1102-4a3c-a673-cd4cba2a204b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# primary_secondary = cut_primary_secondary_roads()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "c0ff123b-81fa-44b3-825d-ea9b8b68625d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cut_all_roads(date, chunk_row_size):\n",
    "    \"\"\"\n",
    "    Takes about 1.5 hours.\n",
    "    \"\"\"\n",
    "    start = datetime.datetime.now()\n",
    "    print(f\"Cutting all local roads/primary/secondary roads {start}\")\n",
    "\n",
    "    # Find local roads that intersect  with GTFS shapes, then\n",
    "    # segment them\n",
    "    local_roads_gdf = cut_local_roads(date, chunk_row_size)\n",
    "\n",
    "    # Segment primary and secondary roads\n",
    "    segmented_primary_secondary_rds = cut_primary_secondary_roads()\n",
    "\n",
    "    # Concat\n",
    "    all_roads = pd.concat([segmented_primary_secondary_rds, local_roads_gdf], axis=0)\n",
    "    all_roads.to_parquet(f\"{SHARED_GCS}segmented_all_roads.parquet\")\n",
    "\n",
    "    end = datetime.datetime.now()\n",
    "    print(f\"time lapsed for cutting all roads: {end-start}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "0bfe3493-3988-4cc1-9220-a37e61dc61d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# all_roads = cut_all_roads(analysis_date, 10_000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8c75566-2320-4161-b5a3-cd4ba9c2aab2",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Dask 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "9fe405e1-fadb-4d70-b20c-41415fc69c6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find all the parquets again\n",
    "def find_files(phrase_to_find: str):\n",
    "    \"\"\"\n",
    "    Grab a list of files that contain the\n",
    "    phrase inputted. E.g. \"tmobile_no_coverage\"\n",
    "    \"\"\"\n",
    "    folder = \"gs://calitp-analytics-data/data-analyses/shared_data/daskutilstest\"\n",
    "    # Create a list of all the files in my folder\n",
    "    all_files_in_folder = fs.ls(folder)\n",
    "\n",
    "    # Grab only files with the string \"Verizon_no_coverage_\"\n",
    "    my_files = [i for i in all_files_in_folder if phrase_to_find in i]\n",
    "\n",
    "    # String to add to read the files\n",
    "    my_string = \"gs://\"\n",
    "    my_files = [my_string + i for i in my_files]\n",
    "    \n",
    "    # Extract digit of parquet \n",
    "    return my_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "922d054e-ea21-4bd7-9a43-d7672c1850d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_number(phrase_to_find: str) -> list:\n",
    "    files = find_files(phrase_to_find)\n",
    "    all_file_numbers = []\n",
    "    for file in files:\n",
    "    # https://stackoverflow.com/questions/11339210/how-to-get-integer-values-from-a-string-in-python\n",
    "        file_number = \"\".join(i for i in file if i.isdigit())\n",
    "        all_file_numbers.append(file_number)\n",
    "    return all_file_numbers "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "aa27d02b-c75b-47c5-8875-de714d4971a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunk_dask_df(gdf):\n",
    "    \"\"\"\n",
    "    Break up dataframes by a certain\n",
    "    number of rows, turn them into a dask\n",
    "    dataframe\n",
    "\n",
    "    Args:\n",
    "        gdf: the local roads that intersect w/ stops and routes\n",
    "        chunk_row_size(int): how many rows each dataframe should\n",
    "        be after splitting it out.\n",
    "\n",
    "    Returns:\n",
    "        List of dask dataframes. Length of how many dask dataframes\n",
    "        are returned after cutting.\n",
    "    \"\"\"\n",
    "   # Turn sjoin local roads to dask\n",
    "    ddf1 = dd.from_pandas(gdf, npartitions=1)\n",
    "\n",
    "    # Partition the sjoin stuff automatically\n",
    "    ddf1_partitioned = ddf1.repartition(partition_size=\"1MB\")\n",
    "    \n",
    "    #Save out to GCS\n",
    "    ddf1_partitioned.to_parquet(f\"{SHARED_GCS}partitioned_tiger\")\n",
    "    \n",
    "    # Read back all the partitioned stuff - grab the file number\n",
    "    #part0.parquet, part1.parquet\n",
    "    file_names_dask = extract_number(\"part\")\n",
    "    \n",
    "    # https://www.geeksforgeeks.org/read-multiple-csv-files-into-separate-dataframes-in-python/\n",
    "    # create empty list\n",
    "    dataframes_list = []\n",
    " \n",
    "    # append datasets into the list\n",
    "    for i in range(len(file_names_dask)):\n",
    "        gcs_file_path = \"gs://calitp-analytics-data/data-analyses/shared_data/daskutilstest/part.\"\n",
    "        temp_df = dg.read_parquet(f\"{gcs_file_path}{file_names_dask[i]}.parquet\")\n",
    "        dataframes_list.append(temp_df)\n",
    "        \n",
    "    return dataframes_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "bb37b74d-41d0-4cfc-8824-32b45ff0d5c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cut_local_roads(date) -> gpd.GeoDataFrame:\n",
    "    start = datetime.datetime.now()\n",
    "    print(f\"Cut: local roads {start}\")\n",
    "\n",
    "    # Find all local roads that intersect with\n",
    "    # stops and routes.\n",
    "    local_roads_unsegmented = sjoin_local_roads(date)\n",
    "\n",
    "    # Divide the gdf into equal sized chunks (roughly)\n",
    "    # and turn them into dask gdfs\n",
    "    print(\"Split into mulitple parquets\")\n",
    "    ddf_list = chunk_dask_df(local_roads_unsegmented)\n",
    "\n",
    "    # Cut geometry\n",
    "    cut_results = []\n",
    "    for ddf in ddf_list:\n",
    "        cut_geometry = delayed(geography_utils.cut_segments)(ddf, [\"linearid\", \"fullname\"], 1_000)\n",
    "        cut_results.append(cut_geometry)\n",
    "    print(\"Cut geometry\")\n",
    "    \n",
    "    print(f\"Begin computing: {datetime.datetime.now()}\")\n",
    "    # Compute \n",
    "    cut_results = [compute(i)[0] for i in cut_results]\n",
    "    cut_df = pd.concat(cut_results, axis=0).reset_index(drop=True)\n",
    "    cut_df.to_parquet(f\"{SHARED_GCS}segmented_local_rds.parquet\")\n",
    "    \n",
    "    end = datetime.datetime.now()\n",
    "    print(f\"Done cutting local roads in {end-start} minutes\")\n",
    "    return cut_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "eb8bccb1-8f28-4e91-b37f-6d9939813d7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cut: local roads 2023-05-04 15:39:13.050655\n",
      "sjoin 2023-05-04 15:39:13.051329\n",
      "Done buffering. 2023-05-04 15:45:10.670280-start\n",
      "Done with sjoin with stops with local roads. Time lapsed: 0:03:33.845196\n",
      "Done with sjoin with routes and local roads. Time lapsed: 0:05:41.714142\n",
      "Done with doing an sjoin for all local roads. Time lapsed: 0:15:42.049505\n",
      "Split into mulitple parquets\n",
      "Cut geometry\n",
      "Begin computing: 2023-05-04 15:55:31.592353\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'to_parquet'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[42], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m test \u001b[38;5;241m=\u001b[39m \u001b[43mcut_local_roads\u001b[49m\u001b[43m(\u001b[49m\u001b[43manalysis_date\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[41], line 24\u001b[0m, in \u001b[0;36mcut_local_roads\u001b[0;34m(date)\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[38;5;66;03m# Compute \u001b[39;00m\n\u001b[1;32m     23\u001b[0m cut_results \u001b[38;5;241m=\u001b[39m [compute(i)[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m cut_results]\n\u001b[0;32m---> 24\u001b[0m \u001b[43mcut_results\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto_parquet\u001b[49m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mSHARED_GCS\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124msegmented_local_rds.parquet\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     26\u001b[0m end \u001b[38;5;241m=\u001b[39m datetime\u001b[38;5;241m.\u001b[39mdatetime\u001b[38;5;241m.\u001b[39mnow()\n\u001b[1;32m     27\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDone cutting local roads in \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mend\u001b[38;5;241m-\u001b[39mstart\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m minutes\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'list' object has no attribute 'to_parquet'"
     ]
    }
   ],
   "source": [
    "test = cut_local_roads(analysis_date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa773662-a21c-4ac0-882a-d597b0570570",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in sjoined files\n",
    "local_roads_unsegmented = gpd.read_parquet(\n",
    "    \"gs://calitp-analytics-data/data-analyses/shared_data/local_roads_all_routes_stops_sjoin.parquet\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b08a852-2d20-4c01-bd5f-7ff8048b8c45",
   "metadata": {},
   "outputs": [],
   "source": [
    "local_roads_unsegmented.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92ff276e-dd4e-4030-9b48-eadc44b14b2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Turn sjoin local roads to dask\n",
    "ddf1 = dd.from_pandas(local_roads_unsegmented, npartitions=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a78b420-94db-40ad-9327-7d38882e579b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Partition the sjoin stuff automatically\n",
    "ddf1_partitioned = ddf1.repartition(partition_size=\"1MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79265a7a-aa75-4005-a625-8cffc4373205",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Save\n",
    "ddf1_partitioned.to_parquet(f\"{SHARED_GCS}daskutilstest\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "702a3e12-19bb-4cb0-a8c5-01b324c0634c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5042b67-0e65-4f10-a393-0da0e4240b9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_names_dask = extract_number(\"part\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e803160-d428-499a-8b54-9538d27856b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://www.geeksforgeeks.org/read-multiple-csv-files-into-separate-dataframes-in-python/\n",
    "# create empty list\n",
    "dataframes_list = []\n",
    " \n",
    "# append datasets into the list\n",
    "for i in range(len(file_names)):\n",
    "    gcs_file_path = \"gs://calitp-analytics-data/data-analyses/shared_data/daskutilstest/part.\"\n",
    "    temp_df = dg.read_parquet(f\"{gcs_file_path}{file_names[i]}.parquet\")\n",
    "    dataframes_list.append(temp_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68a5c51e-50cc-4bbf-8a18-afc078f69b76",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(dataframes_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f68a473-1ea6-4967-b37e-01d4093da01f",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframes_list[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2500651b-0560-4497-a3aa-d8f353cb1699",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Empty dataframe\n",
    "my_results = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba28efc6-3363-4333-93f0-e17bd51ee92f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For each dask dataframe int the list\n",
    "# cut them and append the results into the empty df.\n",
    "for ddf in dataframes_list:\n",
    "    cut_geometry = delayed(geography_utils.cut_segments)(ddf, [\"linearid\", \"fullname\"], 1_000)\n",
    "    my_results.append(cut_geometry)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bebed4c-383f-4e96-a77f-7859ef6422b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8e081d5-5755-481c-ad7f-0e20c951ff8e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "203a1b98-1788-4f53-b9c0-08de21301de4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Took 1:06:37.299317\n",
    "#start = datetime.datetime.now()\n",
    "#print(start)\n",
    "#results2 = [compute(i)[0] for i in my_results]  # 9;12\n",
    "#end = datetime.datetime.now()\n",
    "#print(end)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6e3ea1d-649c-47aa-8ce8-227cfdcb322a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(end-start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "223e08ec-0db3-4977-bd31-199e1f54f679",
   "metadata": {},
   "outputs": [],
   "source": [
    "# type(results2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc6d6418-d779-4eed-9f85-c65bb84512bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# testpd = pd.concat(results2, axis=0).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1d46e2f-e94b-47a7-9912-543fc67346fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# type(testpd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2cceec4-f080-467b-b3bc-106b09033aa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# testpd.to_parquet(f\"{SHARED_GCS}dask_test2.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f552ba87-85c9-4f65-9a98-e003720246ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# testpd.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "beb6c1ca-ae26-40ed-89c9-53ac2c712e9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "dask2 = gpd.read_parquet(\"gs://calitp-analytics-data/data-analyses/shared_data/dask_test2.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "042fac67-3fa5-49c7-8b5f-d2de023c4e42",
   "metadata": {},
   "outputs": [],
   "source": [
    "dask2.linearid.nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b83270a-1ec3-4e6b-91ee-4f291165f009",
   "metadata": {},
   "outputs": [],
   "source": [
    "dask2.linearid.value_counts().head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98891845-c705-4266-af51-1e931dce28ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_linearids = [ '11019653760031',\n",
    " '1106092764328',\n",
    " '11011135052903',\n",
    " '110411099535',\n",
    " '11011135055229',\n",
    " '11011135055553',\n",
    " '11011135056214',\n",
    " '11012028306122',\n",
    " '11012812027422',\n",
    " '11012812027505',\n",
    " '11012812035943',\n",
    " '11012812038881',\n",
    " '1106073054809']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0234451d-1006-4f7d-bc27-59559c8bc159",
   "metadata": {},
   "outputs": [],
   "source": [
    "og_tiger = load_roads(['S1400'], False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b78d9d13-b64e-459a-8d05-5f51b9c0bbb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "og_tiger_filtered = og_tiger[og_tiger.linearid.isin(test_linearids)].reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4374a26-ed5e-473a-acdf-95800bb04680",
   "metadata": {},
   "outputs": [],
   "source": [
    "# og_tiger_filtered.explore('linearid',cmap = \"tab20c\", style_kwds = {'weight':5})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f27bbd0-3962-4fcc-8d32-63d814c91476",
   "metadata": {},
   "outputs": [],
   "source": [
    "dask2_filtered = dask2[dask2.linearid.isin(test_linearids)].reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4174574-d5d3-4839-b669-7debb8bac4b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "dask2_filtered.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "704ee9d5-25db-49b4-ac1f-2ae9119087e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "dask2_filtered.sample()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d9c767a-1028-4ef0-9c10-96c1fa08e832",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dask2_filtered.explore('segment_sequence',cmap = \"tab20c\", style_kwds = {'weight':5})"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
