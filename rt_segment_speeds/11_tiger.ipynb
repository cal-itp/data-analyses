{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "18ada7ff-048d-4c5c-9523-9f191e772137",
   "metadata": {},
   "source": [
    "## Tiger Census\n",
    "* https://www2.census.gov/geo/pdfs/maps-data/data/tiger/tgrshp2019/TGRSHP2019_TechDoc.pdf\n",
    "* S1200 - secondary road\n",
    "* S1100 - primary road\n",
    "* S1400 - local roads\n",
    "* Build off scripts/cut_road_segments.py\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e1409ea6-c44a-45af-9a4b-7752f5f8cfd5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.9/site-packages/geopandas/_compat.py:123: UserWarning: The Shapely GEOS version (3.11.1-CAPI-1.17.1) is incompatible with the GEOS version PyGEOS was compiled with (3.10.1-CAPI-1.16.0). Conversions between both will be slow.\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/dask_geopandas/backends.py:13: UserWarning: Shapely 2.0 is installed, but because PyGEOS is also installed, GeoPandas will still use PyGEOS by default for now. To force to use and test Shapely 2.0, you have to set the environment variable USE_PYGEOS=0. You can do this before starting the Python process, or in your code before importing geopandas:\n",
      "\n",
      "import os\n",
      "os.environ['USE_PYGEOS'] = '0'\n",
      "import geopandas\n",
      "\n",
      "In a future release, GeoPandas will switch to using Shapely by default. If you are using PyGEOS directly (calling PyGEOS functions on geometries from GeoPandas), this will then stop working and you are encouraged to migrate from PyGEOS to Shapely 2.0 (https://shapely.readthedocs.io/en/latest/migration_pygeos.html).\n",
      "  import geopandas\n"
     ]
    }
   ],
   "source": [
    "import datetime\n",
    "\n",
    "import dask.dataframe as dd\n",
    "import dask_geopandas as dg\n",
    "import geopandas\n",
    "import geopandas as gpd\n",
    "import pandas as pd\n",
    "from calitp_data_analysis.sql import to_snakecase\n",
    "from dask import compute, delayed\n",
    "from segment_speed_utils import helpers\n",
    "from segment_speed_utils.project_vars import analysis_date\n",
    "from shared_utils import dask_utils, geography_utils, utils\n",
    "\n",
    "GCS_FILE_PATH = \"gs://calitp-analytics-data/data-analyses/\"\n",
    "SHARED_GCS = f\"{GCS_FILE_PATH}shared_data/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c69abcdb-7509-47c8-b891-a99c899db38b",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.options.display.max_columns = 100\n",
    "pd.options.display.float_format = \"{:.2f}\".format\n",
    "pd.set_option(\"display.max_rows\", None)\n",
    "pd.set_option(\"display.max_colwidth\", None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af59e43d-f137-4a0a-9fb9-e6ebd0a6d030",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Tiger - Load Roads\n",
    "* TO DO: dissolve by linearid because there are mulitple rows associated with one id -> the same road"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "61786354-ec33-47bd-9891-cb292068c703",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_roads(road_type_wanted: list, buffer_or_not: bool = False) -> gpd.GeoDataFrame:\n",
    "    \"\"\"\n",
    "    Load roads based on what you filter.\n",
    "    Can also buffer the roads or not.\n",
    "\n",
    "    Args:\n",
    "        road_type_wanted (list): the type of roads you want.\n",
    "\n",
    "        https://www2.census.gov/geo/pdfs/maps-data/data/tiger/tgrshp2019/TGRSHP2019_TechDoc.pdf\n",
    "        buffer_or_not (bool): add a buffer of 200.\n",
    "\n",
    "    Returns:\n",
    "        GDF. As of 4/18/23, returns 953914 nunique linearid\n",
    "    \"\"\"\n",
    "    df = gpd.read_parquet(\n",
    "        f\"{SHARED_GCS}all_roads_2020_state06.parquet\",\n",
    "        filters=[(\"MTFCC\", \"in\", road_type_wanted)],\n",
    "        columns=[\"LINEARID\", \"geometry\", \"FULLNAME\"],\n",
    "    ).to_crs(geography_utils.CA_NAD83Albers)\n",
    "\n",
    "    # If a road has mutliple rows but the same\n",
    "    # linear ID, dissolve it so it becomes one row.\n",
    "    df = (\n",
    "        df.drop_duplicates()\n",
    "        .dissolve(by=[\"LINEARID\"])\n",
    "        .reset_index()\n",
    "        .drop_duplicates()\n",
    "        .reset_index(drop=True)\n",
    "    )\n",
    "\n",
    "    if buffer_or_not:\n",
    "        df = df.assign(geometry=df.geometry.buffer(200))\n",
    "\n",
    "    df = to_snakecase(df)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0234451d-1006-4f7d-bc27-59559c8bc159",
   "metadata": {},
   "outputs": [],
   "source": [
    "# og_tiger = load_roads(['S1400'], False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "55408ee8-7dbd-4171-9575-0cb138c8b60c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# len(og_tiger)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1c2a4b80-9bec-44bb-8a2a-63ce7a11a55c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# og_tiger.linearid.nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cd327db3-345d-48f5-b673-43edb740a210",
   "metadata": {},
   "outputs": [],
   "source": [
    "# len(og_tiger.drop_duplicates())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c868617d-b244-47f3-80e4-8c941ec2fe45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# more_than_1 = og_tiger.linearid.value_counts().loc[lambda x: x>1].reset_index()['index']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f825b43b-1f58-40bc-808c-0a62fae9c510",
   "metadata": {},
   "outputs": [],
   "source": [
    "# more_than_1 = list(more_than_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "23e094ff-b20d-4d1b-b129-8a2c830ca48c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# len(more_than_1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d935930a-93da-4609-a8d8-671a4626adde",
   "metadata": {},
   "source": [
    "#### Cesar Chavez"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4e0a190a-c26d-48cc-9a57-c9ae2e8fde2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cesar_chavez = og_tiger[og_tiger.fullname == \"Cesar Chavez\"].reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "58fc873f-bb95-4129-a154-12d64e089264",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cesar_chavez"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45bdd874-29bf-4e67-8c07-04bd9cdda0d6",
   "metadata": {},
   "source": [
    "### GTFS Shapes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a67fb46e-c33b-4dee-82c4-dcd611443963",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gtfs_stops_operators(date) -> gpd.GeoDataFrame:\n",
    "    \"\"\"\n",
    "    Load stops with operator and\n",
    "    feed key information.\n",
    "\n",
    "    Args:\n",
    "        date: date wanted for the datasets to be drawn from\n",
    "\n",
    "    Returns:\n",
    "        GDF\n",
    "    \"\"\"\n",
    "    stops = (\n",
    "        helpers.import_scheduled_stops(\n",
    "            date, (), [\"feed_key\", \"stop_id\", \"stop_key\", \"geometry\"]\n",
    "        )\n",
    "        .compute()\n",
    "        .drop_duplicates()\n",
    "    )\n",
    "\n",
    "    stops = stops.set_crs(geography_utils.CA_NAD83Albers)\n",
    "\n",
    "    # Buffer each stop by 50 feet\n",
    "    stops = stops.assign(buffered_geometry=stops.geometry.buffer(50))\n",
    "\n",
    "    # Set geometry\n",
    "    stops = stops.set_geometry(\"buffered_geometry\")\n",
    "\n",
    "    # Merge for operator information\n",
    "    trips = (\n",
    "        helpers.import_scheduled_trips(analysis_date, (), [\"name\", \"feed_key\"])\n",
    "        .compute()\n",
    "        .drop_duplicates()\n",
    "    )\n",
    "\n",
    "    m1 = pd.merge(stops, trips, on=[\"feed_key\"], how=\"left\")\n",
    "\n",
    "    # Fill in na\n",
    "    m1.name = m1.name.fillna(\"None\")\n",
    "\n",
    "    return m1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0e17d8f7-07ca-48b7-9b9d-e28cbb3b0285",
   "metadata": {},
   "outputs": [],
   "source": [
    "# stops = gtfs_stops_operators(analysis_date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "af866cf6-65d8-4d4f-96d4-a2eb8f3bf88b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gtfs_routes_operators(date) -> gpd.GeoDataFrame:\n",
    "    \"\"\"\n",
    "    Load routes with operator and feed key information.\n",
    "\n",
    "    Args:\n",
    "        date: date wanted for the datasets to be drawn from\n",
    "    \"\"\"\n",
    "    gtfs_shapes = helpers.import_scheduled_shapes(date).compute().drop_duplicates()\n",
    "\n",
    "    gtfs_shapes = gtfs_shapes.set_crs(geography_utils.CA_NAD83Albers)\n",
    "\n",
    "    trips = (\n",
    "        helpers.import_scheduled_trips(date, (), [\"name\", \"shape_array_key\"])\n",
    "        .compute()\n",
    "        .drop_duplicates()\n",
    "    )\n",
    "\n",
    "    m1 = pd.merge(gtfs_shapes, trips, how=\"left\", on=\"shape_array_key\")\n",
    "\n",
    "    return m1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ee6d2496-861a-4cbb-a3d9-98fab1735da5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def order_operators(date) -> list:\n",
    "    \"\"\"\n",
    "    Re order a list of operators the largest\n",
    "    ones will be at the top of the list.\n",
    "\n",
    "    Args:\n",
    "        date: date wanted for the datasets to be drawn from\n",
    "    \"\"\"\n",
    "    operator_list = (\n",
    "        helpers.import_scheduled_trips(date, (), [\"name\"]).compute().sort_values(\"name\")\n",
    "    )\n",
    "    operator_list = operator_list.name.unique().tolist()\n",
    "\n",
    "    # Reorder list so the biggest operators are at the beginning\n",
    "    # based on NTD services data\n",
    "    big_operators = [\n",
    "        \"LA DOT Schedule\",\n",
    "        \"LA Metro Bus Schedule\",\n",
    "        \"LA Metro Rail Schedule\",\n",
    "        \"Bay Area 511 Muni Schedule\",\n",
    "        \"Bay Area 511 AC Transit Schedule\",\n",
    "        \"Bay Area 511 Santa Clara Transit Schedule\",\n",
    "        \"Bay Area 511 BART Schedule\",\n",
    "        \"San Diego Schedule\",\n",
    "        \"OCTA Schedule\",\n",
    "        \"Sacramento Schedule\",\n",
    "        \"Bay Area 511 Sonoma-Marin Area Rail Transit Schedule\",\n",
    "        \"Bay Area 511 SFO AirTrain Schedule\",\n",
    "        \"Bay Area 511 South San Francisco Shuttle Schedule\",\n",
    "        \"Bay Area 511 Marin Schedule\",\n",
    "        \"Bay Area 511 County Connection Schedule\",\n",
    "        \"Bay Area 511 MVGO Schedule\",\n",
    "        \"Bay Area 511 Commute.org Schedule\",\n",
    "        \"Bay Area 511 Union City Transit Schedule\",\n",
    "        \"Bay Area 511 BART Schedule\",\n",
    "        \"Bay Area 511 Caltrain Schedule\",\n",
    "        \"Bay Area 511 Fairfield and Suisun Transit Schedule\",\n",
    "        \"Bay Area 511 Dumbarton Express Schedule\",\n",
    "        \"Bay Area 511 SamTrans Schedule\",\n",
    "        \"Bay Area 511 Vine Transit Schedule\",\n",
    "        \"Bay Area 511 Tri-Valley Wheels Schedule\",\n",
    "        \"Bay Area 511 Sonoma County Transit Schedule\",\n",
    "        \"Bay Area 511 Santa Rosa CityBus Schedule\",\n",
    "        \"Bay Area 511 Golden Gate Transit Schedule\",\n",
    "        \"Bay Area 511 Golden Gate Ferry Schedule\",\n",
    "        \"Bay Area 511 San Francisco Bay Ferry Schedule\",\n",
    "        \"Bay Area 511 SolTrans Schedule\",\n",
    "        \"Bay Area 511 ACE Schedule\",\n",
    "        \"Bay Area 511 Emery Go-Round Schedule\",\n",
    "        \"Bay Area 511 Tri Delta Schedule\",\n",
    "        \"Bay Area 511 Petaluma Schedule\",\n",
    "        \"Bay Area 511 Capitol Corridor Schedule\",\n",
    "    ]\n",
    "\n",
    "    # Delete off the big operators\n",
    "    operator_list = list(set(operator_list) - set(big_operators))\n",
    "\n",
    "    # Add back in the operators\n",
    "    final_list = big_operators + operator_list\n",
    "\n",
    "    return final_list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "876aad40-5b71-42b9-b899-11ecd15e2cfe",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Tiger Local Roads"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4002d6ad-0bac-45f5-a7a0-c9358fc39376",
   "metadata": {},
   "source": [
    "#### Cut all roads - stops 1st then routes \n",
    "* Use some small operators to test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "80e1cb44-e512-4de9-8a8c-d00621db840a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def loop_sjoin(date, local_roads_gdf, gdf_routes_stops) -> gpd.GeoDataFrame:\n",
    "    \"\"\"\n",
    "    By operator, sjoin its routes/stops to\n",
    "    local roads gdf. Delete off any linear ids that are joined.\n",
    "\n",
    "    Args:\n",
    "        local_roads_gdf: local roads gdf (should be buffered roads).\n",
    "        gdf_routes_stops: stops or routes gdf\n",
    "        date: date wanted for the datasets to be drawn from\n",
    "    \"\"\"\n",
    "    # Empty dataframe\n",
    "    sjoin_full_results = pd.DataFrame()\n",
    "\n",
    "    # Find all unique operators, ordered by largest operators first\n",
    "    operators_list = order_operators(date)\n",
    "\n",
    "    # Loop through and sjoin by operator\n",
    "    for operator in operators_list:\n",
    "        shapes_filtered = gdf_routes_stops.loc[\n",
    "            gdf_routes_stops.name == operator\n",
    "        ].reset_index(drop=True)\n",
    "\n",
    "        # Delete any local road linear ids that have already been found by an operator\n",
    "        try:\n",
    "            # List of linear IDS\n",
    "            linearid_to_delete = sjoin_full_results.linearid.unique().tolist()\n",
    "\n",
    "            # Filter out the linear IDS in buffered local roads\n",
    "            local_roads_gdf = local_roads_gdf[\n",
    "                ~local_roads_gdf.linearid.isin(linearid_to_delete)\n",
    "            ].reset_index(drop=True)\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "        # Do a sjoin but  keep the linearid as the only column\n",
    "        sjoin1 = (\n",
    "            gpd.sjoin(\n",
    "                local_roads_gdf,\n",
    "                shapes_filtered,\n",
    "                how=\"inner\",\n",
    "                predicate=\"intersects\",\n",
    "            )[[\"linearid\"]]\n",
    "            .drop_duplicates()\n",
    "            .reset_index(drop=True)\n",
    "        )\n",
    "\n",
    "        sjoin_full_results = pd.concat([sjoin_full_results, sjoin1], axis=0)\n",
    "\n",
    "    sjoin_full_results = sjoin_full_results.drop_duplicates()\n",
    "\n",
    "    return sjoin_full_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "98c2aa4c-1260-4b7b-b4f4-8be3c91a0bce",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sjoin_stops(buffered_roads, original_roads, date):\n",
    "    \"\"\"\n",
    "    Sjoin stops to local roads.\n",
    "\n",
    "    Returns:\n",
    "        A list of linear IDs that have already\n",
    "        been found and a GDF.\n",
    "    \"\"\"\n",
    "    start = datetime.datetime.now()\n",
    "\n",
    "    # Load stops\n",
    "    gtfs_stops = gtfs_stops_operators(date)\n",
    "\n",
    "    # Loop through and sjoin by operator\n",
    "    stops_sjoin = loop_sjoin(date, buffered_roads, gtfs_stops)\n",
    "\n",
    "    # Merge back to original local roads gdf, so we have the\n",
    "    # non buffered geometry.\n",
    "    m1 = pd.merge(original_roads, stops_sjoin, on=\"linearid\", how=\"inner\")\n",
    "\n",
    "    # Fill in null values for fullname\n",
    "    m1.fullname = m1.fullname.fillna(\"None\")\n",
    "\n",
    "    # Find linear ids to delete\n",
    "    linearid_to_delete = m1.linearid.unique().tolist()\n",
    "\n",
    "    # Save\n",
    "    m1.to_parquet(f\"{SHARED_GCS}local_roads_stops_sjoin.parquet\")\n",
    "\n",
    "    end = datetime.datetime.now()\n",
    "    print(f\"Done with sjoin with stops with local roads. Time lapsed: {end-start}\")\n",
    "\n",
    "    return m1, linearid_to_delete"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e97a8499-6db4-4f20-ae60-a4845988515e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sjoin_routes(buffered_roads, original_roads, date, linearid_to_delete: list):\n",
    "    start = datetime.datetime.now()\n",
    "\n",
    "    # Load stops\n",
    "    gtfs_routes = gtfs_routes_operators(date)\n",
    "\n",
    "    # Delete out linear ids that have already been found\n",
    "    local_roads_buffered = buffered_roads[\n",
    "        ~buffered_roads.linearid.isin(linearid_to_delete)\n",
    "    ].reset_index(drop=True)\n",
    "    local_roads_og = original_roads[\n",
    "        ~original_roads.linearid.isin(linearid_to_delete)\n",
    "    ].reset_index(drop=True)\n",
    "\n",
    "    routes_sjoin = loop_sjoin(date, local_roads_buffered, gtfs_routes)\n",
    "\n",
    "    # Merge back to original local roads, so we have the\n",
    "    # non buffered geometry.\n",
    "    m1 = pd.merge(local_roads_og, routes_sjoin, on=\"linearid\", how=\"inner\")\n",
    "\n",
    "    # Fill in null values for fullname\n",
    "    m1.fullname = m1.fullname.fillna(\"None\")\n",
    "\n",
    "    # Save\n",
    "    m1.to_parquet(f\"{SHARED_GCS}local_roads_routes_sjoin.parquet\")\n",
    "\n",
    "    end = datetime.datetime.now()\n",
    "    print(f\"Done with sjoin with routes and local roads. Time lapsed: {end-start}\")\n",
    "\n",
    "    return m1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d6c298fb-28a8-44b2-b423-e3c6ac75ff7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sjoin_local_roads(date):\n",
    "    \"\"\"\n",
    "    Sjoin local roads with stops first, then routes.\n",
    "    \"\"\"\n",
    "    start = datetime.datetime.now()\n",
    "    print(start)\n",
    "\n",
    "    # Load local roads - buffered\n",
    "    local_roads_buffered = load_roads([\"S1400\"], True)\n",
    "\n",
    "    # Load local roads - not buffered\n",
    "    local_roads_og = load_roads([\"S1400\"], False)\n",
    "\n",
    "    # Deal with stops first\n",
    "    stops_sjoin, linear_id_stops = sjoin_stops(\n",
    "        local_roads_buffered, local_roads_og, date\n",
    "    )\n",
    "\n",
    "    # Move onto routes\n",
    "    routes_sjoin = sjoin_routes(\n",
    "        local_roads_buffered, local_roads_og, date, linear_id_stops\n",
    "    )\n",
    "\n",
    "    # Stack\n",
    "    all_local_roads = pd.concat([stops_sjoin, routes_sjoin], axis=0)\n",
    "\n",
    "    all_local_roads.to_parquet(\n",
    "        f\"{SHARED_GCS}local_roads_all_routes_stops_sjoin.parquet\"\n",
    "    )\n",
    "    end = datetime.datetime.now()\n",
    "\n",
    "    print(f\"Done with doing an sjoin for all local roads. Time lapsed: {end-start}\")\n",
    "    return all_local_roads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "992cd1c8-3aa0-40ba-a201-4b840cb02d95",
   "metadata": {},
   "outputs": [],
   "source": [
    "# all_ops = cut_local_roads(analysis_date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "2b7ecbb8-0f54-4bd5-bd33-e0c8ed10f4f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# all_ops.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a821e63b-0878-4eb3-893a-b5aab32a5519",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  all_ops.sample()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "7f5f75cc-3514-41ff-9bc7-5f1fc98d6750",
   "metadata": {},
   "outputs": [],
   "source": [
    "# all_ops.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "6e48ae43-89d1-496e-bf64-6d7c23423156",
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunk_dask_df(gdf, chunk_row_size: int):\n",
    "    \"\"\"\n",
    "    Break up dataframes by a certain\n",
    "    number of rows, turn them into a dask\n",
    "    dataframe\n",
    "\n",
    "    Args:\n",
    "        gdf: the local roads that intersect w/ stops and routes\n",
    "        chunk_row_size(int): how many rows each dataframe should\n",
    "        be after splitting it out.\n",
    "\n",
    "    Returns:\n",
    "        List of dask dataframes. Length of how many dask dataframes\n",
    "        are returned after cutting.\n",
    "    \"\"\"\n",
    "    # Specify how many rows I want the gdf to broken into per df\n",
    "    n = chunk_row_size\n",
    "\n",
    "    # Break it out\n",
    "    list_df = [gdf[i : i + n] for i in range(0, gdf.shape[0], n)]\n",
    "\n",
    "    # Turn each dataframe to a dask one\n",
    "    my_ddfs = []\n",
    "    for df in list_df:\n",
    "        ddf = dd.from_pandas(df, npartitions=1)\n",
    "        my_ddfs.append(ddf)\n",
    "\n",
    "    return my_ddfs, len(my_ddfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "90c61251-083c-4b0c-a3e4-93448b26afcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ddfs, length = chunk_dask_df(all_ops, 10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "821ed543-0b04-474a-924d-ca4ec5786265",
   "metadata": {},
   "outputs": [],
   "source": [
    "# len(ddfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "328f7871-6dfa-4eca-bc22-df6b5e9e03ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# type(ddfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "7621c1cc-8499-46ea-85fa-0675d230a5fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# type(ddfs[0]), type(ddfs[15])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "4f8048ab-d3a9-4a33-9aeb-46879f341a7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dask_segment(ddf_list: list, ddfs_range: list) -> gpd.GeoDataFrame:\n",
    "    \"\"\"\n",
    "    Use dask to cut roads into segments. Compute the results\n",
    "    back to a GDF.\n",
    "\n",
    "    Args:\n",
    "        ddf_list: dask dataframes stored in a list.\n",
    "        ddf_list[0] will yield a ddf.\n",
    "\n",
    "        ddfs_range: how many items are in the ddf_list.\n",
    "    \"\"\"\n",
    "    # Empty dataframe\n",
    "    my_results = []\n",
    "\n",
    "    # For each dask dataframe int the list\n",
    "    # cut them and append the results into the empty df.\n",
    "    for i in ddfs_range:\n",
    "        my_df = ddf_list[i]\n",
    "        cut_geometry = delayed(geography_utils.cut_segments)(\n",
    "            my_df, [\"linearid\", \"fullname\"], 1_000\n",
    "        )\n",
    "        my_results.append(cut_geometry)\n",
    "        print(f\"done with {i}\")\n",
    "\n",
    "    # Compute results into a normal gdf\n",
    "    compute_results = [compute(i)[0] for i in my_results]\n",
    "\n",
    "    # Concat results\n",
    "    results_gdf = pd.concat(compute_results)\n",
    "\n",
    "    return results_gdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "a8245e97-95b3-4db0-aa70-49fce1925f3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test = dask_segment(ddfs, [0,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "260c7c04-6e36-4727-93ab-6ec081155448",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "7cf61650-c7e7-4575-bfe9-6e424f1a2847",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cut_local_roads(date, chunk_row_size: int) -> gpd.GeoDataFrame:\n",
    "    start = datetime.datetime.now()\n",
    "    print(f\"Cut: local roads {start}\")\n",
    "\n",
    "    # Find all local roads that intersect with\n",
    "    # stops and routes.\n",
    "    local_roads_unsegmented = sjoin_local_roads(date)\n",
    "\n",
    "    # Divide the gdf into equal sized chunks (roughly)\n",
    "    # and turn them into dask gdfs\n",
    "    ddfs, length = chunk_dask_df(local_roads_unsegmented, chunk_row_size)\n",
    "\n",
    "    # Split the list of split dask dataframes\n",
    "    # into half.\n",
    "    length_list = [*range(0, length)]\n",
    "    ddf1 = length_list[: len(length_list) // 2]\n",
    "    ddf2 = length_list[len(length_list) // 2 :]\n",
    "\n",
    "    # Cut geometry\n",
    "    part1 = dask_segment(ddfs, ddf1)\n",
    "    part1.to_parquet(f\"{SHARED_GCS}segmented_local_rds_first_pt.parquet\")\n",
    "    print(\"Done with cutting part1\")\n",
    "\n",
    "    part2 = dask_segment(ddfs, ddf2)\n",
    "    part2.to_parquet(f\"{SHARED_GCS}segmented_local_rds_second_pt.parquet\")\n",
    "    print(\"Done with cutting part2\")\n",
    "\n",
    "    segmented_local_roads = pd.concat([part1, part2])\n",
    "    segmented_local_roads.to_parquet(f\"{SHARED_GCS}segmented_local_rds.parquet\")\n",
    "\n",
    "    end = datetime.datetime.now()\n",
    "    print(f\"Done cutting local roads in {end-start} minutes\")\n",
    "    return segmented_local_roads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "63f2492d-87b9-4b1c-85c1-516f7096b5cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test = cut_local_roads(analysis_date, 10000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76fc064d-991c-4bd1-aa7d-8cf36dac1630",
   "metadata": {},
   "source": [
    "### Concat local roads and primary/secondary ones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "89d85183-516f-4b88-a6c7-5da78281fc48",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cut_primary_secondary_roads():\n",
    "    start = datetime.datetime.now()\n",
    "    print(f\"Cutting primary/secondary roads {start}\")\n",
    "\n",
    "    # Find all primary and secondary roads\n",
    "    # regardless of intersection w/ GTFS shapes\n",
    "    primary_secondary_mtfcc = [\"S1100\", \"S1200\"]\n",
    "    primary_secondary_roads = load_roads(primary_secondary_mtfcc, False)\n",
    "\n",
    "    segments = geography_utils.cut_segments(\n",
    "        primary_secondary_roads, [\"linearid\", \"fullname\"], 1_000  # 1 km segments\n",
    "    )\n",
    "\n",
    "    segments.to_parquet(f\"{SHARED_GCS}segmented_primary_secondary_roads.parquet\")\n",
    "\n",
    "    end = datetime.datetime.now()\n",
    "    print(f\"Done cutting primary & secondary roads: {end-start}\")\n",
    "    return segments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "3d2a7a9c-1102-4a3c-a673-cd4cba2a204b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# primary_secondary = cut_primary_secondary_roads()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "c0ff123b-81fa-44b3-825d-ea9b8b68625d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cut_all_roads(date, chunk_row_size):\n",
    "    \"\"\"\n",
    "    Takes about 1.5 hours.\n",
    "    \"\"\"\n",
    "    start = datetime.datetime.now()\n",
    "    print(f\"Cutting all local roads/primary/secondary roads {start}\")\n",
    "\n",
    "    # Find local roads that intersect  with GTFS shapes, then\n",
    "    # segment them\n",
    "    local_roads_gdf = cut_local_roads(date, chunk_row_size)\n",
    "\n",
    "    # Segment primary and secondary roads\n",
    "    segmented_primary_secondary_rds = cut_primary_secondary_roads()\n",
    "\n",
    "    # Concat\n",
    "    all_roads = pd.concat([segmented_primary_secondary_rds, local_roads_gdf], axis=0)\n",
    "    all_roads.to_parquet(f\"{SHARED_GCS}segmented_all_roads.parquet\")\n",
    "\n",
    "    end = datetime.datetime.now()\n",
    "    print(f\"time lapsed for cutting all roads: {end-start}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "0bfe3493-3988-4cc1-9220-a37e61dc61d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# all_roads = cut_all_roads(analysis_date, 10_000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8c75566-2320-4161-b5a3-cd4ba9c2aab2",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Dask "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "fa773662-a21c-4ac0-882a-d597b0570570",
   "metadata": {},
   "outputs": [],
   "source": [
    "local_roads_unsegmented = gpd.read_parquet(\n",
    "    \"gs://calitp-analytics-data/data-analyses/shared_data/local_roads_all_routes_stops_sjoin.parquet\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "1b08a852-2d20-4c01-bd5f-7ff8048b8c45",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(322720, 3)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "local_roads_unsegmented.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "92ff276e-dd4e-4030-9b48-eadc44b14b2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Turn sjoin local roads to dask\n",
    "ddf1 = dd.from_pandas(local_roads_unsegmented, npartitions=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "7a78b420-94db-40ad-9327-7d38882e579b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Partition the cut stuff automatically\n",
    "ddf1_partitioned = ddf1.repartition(partition_size=\"1MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "79265a7a-aa75-4005-a625-8cffc4373205",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Save\n",
    "ddf1_partitioned.to_parquet(f\"{SHARED_GCS}daskutilstest\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "702a3e12-19bb-4cb0-a8c5-01b324c0634c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gcsfs\n",
    "fs = gcsfs.GCSFileSystem()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "9fe405e1-fadb-4d70-b20c-41415fc69c6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find all the parquets again\n",
    "def open_parquets(phrase_to_find: str):\n",
    "    \"\"\"\n",
    "    Grab a list of files that contain the\n",
    "    phrase inputted. E.g. \"tmobile_no_coverage\"\n",
    "    \"\"\"\n",
    "    folder = \"gs://calitp-analytics-data/data-analyses/shared_data/daskutilstest\"\n",
    "    # Create a list of all the files in my folder\n",
    "    all_files_in_folder = fs.ls(folder)\n",
    "\n",
    "    # Grab only files with the string \"Verizon_no_coverage_\"\n",
    "    my_files = [i for i in all_files_in_folder if phrase_to_find in i]\n",
    "\n",
    "    # String to add to read the files\n",
    "    my_string = \"gs://\"\n",
    "    my_files = [my_string + i for i in my_files]\n",
    "    \n",
    "    # Extract digit of parquet \n",
    "    return my_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "7a4edebd-9b6a-4683-8983-03ab2d410e6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "partitioned_ddf = find_specific_files(\"part\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "b863ac0e-63ca-4420-b923-afd0ee1fa79b",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['calitp-analytics-data/data-analyses/shared_data/daskutilstest/part.0.parquet',\n",
       " 'calitp-analytics-data/data-analyses/shared_data/daskutilstest/part.1.parquet',\n",
       " 'calitp-analytics-data/data-analyses/shared_data/daskutilstest/part.10.parquet',\n",
       " 'calitp-analytics-data/data-analyses/shared_data/daskutilstest/part.11.parquet',\n",
       " 'calitp-analytics-data/data-analyses/shared_data/daskutilstest/part.12.parquet',\n",
       " 'calitp-analytics-data/data-analyses/shared_data/daskutilstest/part.13.parquet',\n",
       " 'calitp-analytics-data/data-analyses/shared_data/daskutilstest/part.14.parquet',\n",
       " 'calitp-analytics-data/data-analyses/shared_data/daskutilstest/part.15.parquet',\n",
       " 'calitp-analytics-data/data-analyses/shared_data/daskutilstest/part.16.parquet',\n",
       " 'calitp-analytics-data/data-analyses/shared_data/daskutilstest/part.17.parquet',\n",
       " 'calitp-analytics-data/data-analyses/shared_data/daskutilstest/part.18.parquet',\n",
       " 'calitp-analytics-data/data-analyses/shared_data/daskutilstest/part.19.parquet',\n",
       " 'calitp-analytics-data/data-analyses/shared_data/daskutilstest/part.2.parquet',\n",
       " 'calitp-analytics-data/data-analyses/shared_data/daskutilstest/part.20.parquet',\n",
       " 'calitp-analytics-data/data-analyses/shared_data/daskutilstest/part.21.parquet',\n",
       " 'calitp-analytics-data/data-analyses/shared_data/daskutilstest/part.22.parquet',\n",
       " 'calitp-analytics-data/data-analyses/shared_data/daskutilstest/part.23.parquet',\n",
       " 'calitp-analytics-data/data-analyses/shared_data/daskutilstest/part.24.parquet',\n",
       " 'calitp-analytics-data/data-analyses/shared_data/daskutilstest/part.25.parquet',\n",
       " 'calitp-analytics-data/data-analyses/shared_data/daskutilstest/part.26.parquet',\n",
       " 'calitp-analytics-data/data-analyses/shared_data/daskutilstest/part.27.parquet',\n",
       " 'calitp-analytics-data/data-analyses/shared_data/daskutilstest/part.28.parquet',\n",
       " 'calitp-analytics-data/data-analyses/shared_data/daskutilstest/part.29.parquet',\n",
       " 'calitp-analytics-data/data-analyses/shared_data/daskutilstest/part.3.parquet',\n",
       " 'calitp-analytics-data/data-analyses/shared_data/daskutilstest/part.30.parquet',\n",
       " 'calitp-analytics-data/data-analyses/shared_data/daskutilstest/part.31.parquet',\n",
       " 'calitp-analytics-data/data-analyses/shared_data/daskutilstest/part.32.parquet',\n",
       " 'calitp-analytics-data/data-analyses/shared_data/daskutilstest/part.33.parquet',\n",
       " 'calitp-analytics-data/data-analyses/shared_data/daskutilstest/part.34.parquet',\n",
       " 'calitp-analytics-data/data-analyses/shared_data/daskutilstest/part.35.parquet',\n",
       " 'calitp-analytics-data/data-analyses/shared_data/daskutilstest/part.36.parquet',\n",
       " 'calitp-analytics-data/data-analyses/shared_data/daskutilstest/part.37.parquet',\n",
       " 'calitp-analytics-data/data-analyses/shared_data/daskutilstest/part.38.parquet',\n",
       " 'calitp-analytics-data/data-analyses/shared_data/daskutilstest/part.39.parquet',\n",
       " 'calitp-analytics-data/data-analyses/shared_data/daskutilstest/part.4.parquet',\n",
       " 'calitp-analytics-data/data-analyses/shared_data/daskutilstest/part.40.parquet',\n",
       " 'calitp-analytics-data/data-analyses/shared_data/daskutilstest/part.41.parquet',\n",
       " 'calitp-analytics-data/data-analyses/shared_data/daskutilstest/part.42.parquet',\n",
       " 'calitp-analytics-data/data-analyses/shared_data/daskutilstest/part.43.parquet',\n",
       " 'calitp-analytics-data/data-analyses/shared_data/daskutilstest/part.44.parquet',\n",
       " 'calitp-analytics-data/data-analyses/shared_data/daskutilstest/part.45.parquet',\n",
       " 'calitp-analytics-data/data-analyses/shared_data/daskutilstest/part.46.parquet',\n",
       " 'calitp-analytics-data/data-analyses/shared_data/daskutilstest/part.47.parquet',\n",
       " 'calitp-analytics-data/data-analyses/shared_data/daskutilstest/part.48.parquet',\n",
       " 'calitp-analytics-data/data-analyses/shared_data/daskutilstest/part.49.parquet',\n",
       " 'calitp-analytics-data/data-analyses/shared_data/daskutilstest/part.5.parquet',\n",
       " 'calitp-analytics-data/data-analyses/shared_data/daskutilstest/part.6.parquet',\n",
       " 'calitp-analytics-data/data-analyses/shared_data/daskutilstest/part.7.parquet',\n",
       " 'calitp-analytics-data/data-analyses/shared_data/daskutilstest/part.8.parquet',\n",
       " 'calitp-analytics-data/data-analyses/shared_data/daskutilstest/part.9.parquet']"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "partitioned_ddf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "c0ceeb06-815b-47a1-ac66-36480d201fc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_names = []\n",
    "for file in partitioned_ddf:\n",
    "    # https://stackoverflow.com/questions/11339210/how-to-get-integer-values-from-a-string-in-python\n",
    "    file_number = \"\".join(i for i in file if i.isdigit())\n",
    "    file_names.append(file_number)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c022022d-0be5-4b85-b8a3-d554416f77a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"all_dfs = [] \n",
    "for file in partitioned_ddf:\n",
    "    file_number = \"\".join(i for i in file if i.isdigit())\n",
    "    exec(f\"df_{file_number} = dg.read_parquet('{file}')\")\n",
    "    all_dfs.append(f\"df_{file_number}\")\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "5e803160-d428-499a-8b54-9538d27856b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://www.geeksforgeeks.org/read-multiple-csv-files-into-separate-dataframes-in-python/\n",
    "# create empty list\n",
    "dataframes_list = []\n",
    " \n",
    "# append datasets into the list\n",
    "for i in range(len(file_names)):\n",
    "    gcs_file_path = \"gs://calitp-analytics-data/data-analyses/shared_data/daskutilstest/part.\"\n",
    "    temp_df = dg.read_parquet(f\"{gcs_file_path}{file_names[i]}.parquet\")\n",
    "    dataframes_list.append(temp_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "68a5c51e-50cc-4bbf-8a18-afc078f69b76",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dataframes_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "4f68a473-1ea6-4967-b37e-01d4093da01f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><strong>Dask-GeoPandas GeoDataFrame Structure:</strong></div>\n",
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>linearid</th>\n",
       "      <th>geometry</th>\n",
       "      <th>fullname</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>npartitions=1</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <td>object</td>\n",
       "      <td>geometry</td>\n",
       "      <td>object</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>\n",
       "<div>Dask Name: read-parquet, 1 graph layer</div>"
      ],
      "text/plain": [
       "Dask GeoDataFrame Structure:\n",
       "              linearid  geometry fullname\n",
       "npartitions=1                            \n",
       "                object  geometry   object\n",
       "                   ...       ...      ...\n",
       "Dask Name: read-parquet, 1 graph layer"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataframes_list[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2500651b-0560-4497-a3aa-d8f353cb1699",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Empty dataframe\n",
    "my_results = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba28efc6-3363-4333-93f0-e17bd51ee92f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For each dask dataframe int the list\n",
    "# cut them and append the results into the empty df.\n",
    "for df in all_dfs:\n",
    "    cut_geometry = delayed(geography_utils.cut_segments)(df, [\"linearid\", \"fullname\"], 1_000)\n",
    "    my_results.append(cut_geometry)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bebed4c-383f-4e96-a77f-7859ef6422b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "203a1b98-1788-4f53-b9c0-08de21301de4",
   "metadata": {},
   "outputs": [],
   "source": [
    "start = datetime.datetime.now()\n",
    "print(start)\n",
    "results2 = [compute(i)[0] for i in my_results]  # 9;12\n",
    "end = datetime.datetime.now()\n",
    "print(end)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "223e08ec-0db3-4977-bd31-199e1f54f679",
   "metadata": {},
   "outputs": [],
   "source": [
    "type(results2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc6d6418-d779-4eed-9f85-c65bb84512bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "testpd = pd.concat(results2, axis=0).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1d46e2f-e94b-47a7-9912-543fc67346fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "type(testpd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2cceec4-f080-467b-b3bc-106b09033aa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "testpd.shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
