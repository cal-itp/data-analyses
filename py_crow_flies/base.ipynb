{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2874583-4b8b-47b7-b9e4-019cbb578d8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import dask.dataframe as dd\n",
    "import dask_geopandas as dg\n",
    "import geopandas as gpd\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "GCS_FILE_PATH = \"gs://calitp-analytics-data/data-analyses/py_crow_flies/\"\n",
    "\n",
    "from dask import delayed, compute\n",
    "\n",
    "files = [\"CentralCal_POIs\", \"Mojave_POIs\", \"NorCal_POIs\", \"SoCal_POIs\"]\n",
    "CRS = \"EPSG:3857\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a69cac6-7bb7-4e7b-9a1e-3b3e4eb0c0de",
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunk_origin_points(n: int) -> gpd.GeoDataFrame: \n",
    "    # https://stackoverflow.com/questions/33367142/split-dataframe-into-relatively-even-chunks-according-to-length\n",
    "\n",
    "    keep_cols = [\"region\", \"poi_index\", \"geometry\"]\n",
    "    \n",
    "    df = gpd.read_parquet(\n",
    "        f\"./all_pois.parquet\", \n",
    "        columns = keep_cols\n",
    "    )\n",
    "\n",
    "    list_df = [delayed(df[i:i+n]) for i in range(0, df.shape[0], n)]\n",
    "\n",
    "    return list_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02da28a0-ba5c-43cf-b793-e3b23ea01299",
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 15_000\n",
    "list_df = chunk_origin_points(n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17aafeed-8a9e-4468-b66c-a9eecd376943",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = gpd.read_parquet(f\"{GCS_FILE_PATH}all_pois.parquet\", \n",
    "                     filters = [[(\"region\", \"==\", \"Mojave\")]],\n",
    "                      columns = [\"poi_index\", \"geometry\"]\n",
    "                     ).head(100_000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6807a830-4114-4ff9-92c9-b0cfbc318516",
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_destinations = gpd.read_parquet(\n",
    "    f\"{GCS_FILE_PATH}all_pois.parquet\",\n",
    "    filters = [[(\"grid_code\", \">\", 0)]],\n",
    "    columns = [\"poi_index\", \"grid_code\", \"geometry\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "679cdd8a-3156-4308-8eb8-55e4ab9f2f83",
   "metadata": {},
   "outputs": [],
   "source": [
    "def buffer_origin_and_sjoin(\n",
    "    origin_gdf: dg.GeoDataFrame,\n",
    "    destination_gdf: gpd.GeoDataFrame,\n",
    "    buffer_miles: int = 20\n",
    ") -> dd.DataFrame:\n",
    "    \"\"\"\n",
    "    Draw 20 mi buffer around origin point.\n",
    "    Note: Our CRS is in meters, need to convert 20 miles into ___ meters.\n",
    "    \"\"\"\n",
    "    METERS_IN_MILES = 1609.34\n",
    "    \n",
    "    origin_gdf = dg.from_geopandas(origin_gdf, npartitions=1)\n",
    "    origin_gdf = origin_gdf.repartition(partition_size=\"50MB\")\n",
    "    \n",
    "    origin_buffered = origin_gdf.assign(\n",
    "        geometry = origin_gdf.geometry.buffer(\n",
    "            buffer_miles * METERS_IN_MILES)\n",
    "    )\n",
    "            \n",
    "    sjoin_to_destination = dg.sjoin(\n",
    "        origin_buffered, \n",
    "        destination_gdf,\n",
    "        how = \"inner\",\n",
    "        predicate = \"intersects\"\n",
    "    )[[\"poi_index_left\", \n",
    "       \"poi_index_right\", \"grid_code\"]].drop_duplicates()\n",
    "    \n",
    "    sjoin_results = (sjoin_to_destination.rename(\n",
    "        columns = {\n",
    "            \"poi_index_left\": \"origin_poi_index\", \n",
    "            \"poi_index_right\": \"destination_poi_index\"}\n",
    "        ).reset_index(drop=True)\n",
    "        .repartition(npartitions=1)\n",
    "    )\n",
    "    \n",
    "    # Merge point geometry back in\n",
    "    with_origin_point_geom = dd.merge(\n",
    "        origin_gdf,\n",
    "        sjoin_results,\n",
    "        left_on = \"poi_index\",\n",
    "        right_on = \"origin_poi_index\",\n",
    "        how = \"inner\"\n",
    "    ).drop(columns = \"poi_index\")\n",
    "    \n",
    "    with_destin_point_geom = dd.merge(\n",
    "        with_origin_point_geom,\n",
    "        origin_gdf,\n",
    "        left_on = \"destination_poi_index\",\n",
    "        right_on = \"poi_index\",\n",
    "        how = \"inner\"\n",
    "    ).drop(columns = \"poi_index\")\n",
    "    \n",
    "    with_distance = calculate_distance(\n",
    "        with_destin_point_geom, \n",
    "        \"geometry_x\", \n",
    "        \"geometry_y\"\n",
    "    )\n",
    "    \n",
    "    return with_distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d55e773-ed7e-4b43-a344-f1aa0a647a35",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_distance(\n",
    "    gdf: dg.GeoDataFrame,\n",
    "    origin_col: str, \n",
    "    destination_col: str\n",
    ")-> dg.GeoDataFrame: \n",
    "                                       \n",
    "    origin_geom = gdf.set_geometry(origin_col)[origin_col]\n",
    "    destin_geom = gdf.set_geometry(destination_col)[destination_col]\n",
    "    \n",
    "    distance = origin_geom.distance(destin_geom)\n",
    "    \n",
    "    gdf2 = gdf.drop(columns = [origin_col, destination_col])\n",
    "    gdf2 = gdf2.assign(\n",
    "        dist = distance\n",
    "    )\n",
    "    \n",
    "    return gdf2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ddedd1a-5086-4e31-a253-22874091e9cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "sjoin_pairs = buffer_origin_and_sjoin(\n",
    "    df,\n",
    "    valid_destinations,\n",
    "    buffer_miles = 20\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65234983-b578-4ccf-917b-38a4602984cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "sjoin_pairs[sjoin_pairs.origin_poi_index==\n",
    "           sjoin_pairs.destination_poi_index].compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d24aa30-20ee-4b81-8cbf-d258d3d4aed6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def decay_weighted_opportunities(df: pd.DataFrame):\n",
    "        \n",
    "    SPEED = 10\n",
    "    # Define time cutoff (in minutes)\n",
    "    CUTOFF = 60  \n",
    "    \n",
    "    df = df.assign(\n",
    "        decay_weighted_opps = (df.grid_code * np.exp(np.log(0.5)) / \n",
    "                               (CUTOFF * 60) * \n",
    "                               (((60 * df.dist * 0.000621371) / SPEED) * 60)\n",
    "                              )\n",
    "    )\n",
    "    \n",
    "    # Adjust own opportunities to have full value\n",
    "    df = df.assign(\n",
    "        decay_weighted_opps = df.apply(\n",
    "            lambda x: \n",
    "            x.grid_code if x.origin_poi_index == x.destination_poi_index\n",
    "            else x.decay_weighted_opps, axis=1, \n",
    "            meta=('decay_weighted_opps', 'float')\n",
    "        )\n",
    "    )\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63638954-d8fc-4f73-a8e5-cd4a208c3ff3",
   "metadata": {},
   "outputs": [],
   "source": [
    "decay_df = decay_weighted_opportunities(sjoin_pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdf808bf-a713-4aa4-8bef-eaa9ce607119",
   "metadata": {},
   "outputs": [],
   "source": [
    "decay_df.compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05fa939b-4830-4298-9f51-b81213549cc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "decay_df[decay_df.origin_poi_index==\n",
    "         decay_df.destination_poi_index].compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "055d2ac7-9ccf-4cbe-925a-6f99538d1eb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def aggregate_by_origin(\n",
    "    df: pd.DataFrame\n",
    ") -> pd.DataFrame:\n",
    "\n",
    "    access = (df.groupby(\"origin_poi_index\")\n",
    "              .decay_weighted_opps\n",
    "              .sum()\n",
    "              .reset_index()\n",
    "             ).rename(columns = {\"origin_poi_index\": \"poi_index\"})\n",
    "        \n",
    "    return access"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e37160b-37a4-4082-8947-d1adaff7800b",
   "metadata": {},
   "outputs": [],
   "source": [
    "access = aggregate_by_origin(decay_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f985ce5a-2ad2-4a1c-a6c8-c6af3ea429fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "access.compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc9ad133-4661-4bf6-b030-2a4f83274cb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# also a left join at the end because\n",
    "# we want to track all the zeroes\n",
    "def full_merge_onto_all_pois(df: gpd.GeoDataFrame, \n",
    "                             results: dd.DataFrame):\n",
    "    # combine all the results\n",
    "    final = dd.merge(\n",
    "        df,\n",
    "        results,\n",
    "        on = \"poi_index\",\n",
    "        how = \"left\",\n",
    "    )\n",
    "\n",
    "    final = final.assign(\n",
    "        decay_weighted_opps = results.decay_weighted_opps.fillna(0)\n",
    "    ).repartition(npartitions=1)\n",
    "    \n",
    "    return final"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "858170bb-8572-4254-86c5-191be5c42655",
   "metadata": {},
   "source": [
    "Original R script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc06c058-4238-40a8-b97f-974baf88aff4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in Shapefile of grid (or origin) points. Points must have two required columns:\n",
    "  #1. grid_code: The value of the opportunities being measured. In this case, the number of opportunities within the grid cell.\n",
    "  #2. Point_ID: A unique id for each grid in character format.\n",
    "grid_points <- st_read(\"Path to shapefile\")\n",
    "\n",
    "# Transform the grid points to your preferred CRS\n",
    "grid_points <- st_as_sf(grid_points) %>%\n",
    "  st_transform(crs = 3857)\n",
    "\n",
    "# Create a SF dataset for destination points by filtering grid points to only those with opportunities > 0 (this reduces computing time)\n",
    "Dest_Points <- grid_points %>%\n",
    "  filter(grid_code > 0)\n",
    "\n",
    "# Define origin points for analysis. If dataset is large, it may be useful to limit these or break them into chunks\n",
    "origins <- grid_points\n",
    "\n",
    "# For loop to perform crows fly access calcualtions for each origin in the defined dataset\n",
    "out = NULL\n",
    "for(i in 1:nrow(origins)) {\n",
    "  \n",
    "  # Create a buffer around the origin point and select points within that buffer\n",
    "  # Define buffer distance (in miles)\n",
    "  buffer_dis <- 20\n",
    "  buffer <- st_buffer(origins[i, ], (buffer_dis * 1609.34))\n",
    "  intersected_points <- st_intersection(Dest_Points, buffer)\n",
    "  \n",
    "  # Select origin point ID and remove spatial data\n",
    "  origin <- origins[i, ] %>%\n",
    "    select(Point_ID) %>%\n",
    "    st_drop_geometry()\n",
    "  \n",
    "  # If the sum of opportunities within the buffer is > 1, perform access calculations\n",
    "    if(nrow(intersected_points) >= 1) {\n",
    "  \n",
    "      # Calculate distance matrix between origin and point and all destination points within buffer\n",
    "      dist <- st_distance(origins[i, ], intersected_points)\n",
    "      dist <- matrix(dist, ncol = 1)\n",
    "      # Add distance to intersected points DF\n",
    "      intersected_points$dist <- dist\n",
    "      \n",
    "      # Decay-weight opportunities by travel time\n",
    "      # Define travel speed (in MPH)\n",
    "      speed <- 10\n",
    "      # Define time cutoff (in minutes)\n",
    "      cutoff <- 60\n",
    "      intersected_points <- as.data.frame(intersected_points) %>%\n",
    "      mutate(decay_weighted_opps = grid_code * exp(log(0.5) / (cutoff * 60) * (((60 * dist * 0.000621371) / speed) * 60)))\n",
    "      \n",
    "      # Sum decay-weighted opportunities by origin\n",
    "      access <- sum(intersected_points$decay_weighted_opps)\n",
    "      access_df <- data.frame(origin, access)\n",
    "  \n",
    "      out <- rbind.data.frame(access_df, out)\n",
    "      \n",
    "      # Print % progress (optional)\n",
    "      print(i / nrow(origins))\n",
    "    \n",
    "    # If the sum of opportunities within the buffer is zero, access is zero\n",
    "    } else {\n",
    "      access <- 0\n",
    "      access_df <- data.frame(origin, access)\n",
    "      out <- rbind.data.frame(access_df, out)\n",
    "      print(i / nrow(origins))\n",
    "    }\n",
    "}\n",
    "\n",
    "# Write output to CSV\n",
    "write.csv(out, \"/Users/Username/Downloads/CrowsFlyWeighted.csv\", na = \"\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
