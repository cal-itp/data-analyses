{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d44d6ec8-c903-4d33-a3d0-2b655558ef3c",
   "metadata": {},
   "source": [
    "# Get around `maxRowCount` limit for ArcGIS REST Service\n",
    "\n",
    "* [StackOverflow example](https://gis.stackexchange.com/questions/266897/how-to-get-around-the-1000-objectids-limit-on-arcgis-server)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5e64852-3d66-4b9a-8c1a-e5d01ec7b84a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import urllib.parse\n",
    "import requests\n",
    "\n",
    "def query_arcgis_feature_server(url_feature_server=''):\n",
    "    '''\n",
    "    This function downloads all of the features available on a given ArcGIS \n",
    "    feature server. The function is written to bypass the limitations imposed\n",
    "    by the online service, such as only returning up to 1,000 or 2,000 featues\n",
    "    at a time.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    url_feature_server : string\n",
    "        Sting containing the URL of the service API you want to query. It should \n",
    "        end in a forward slash and look something like this:\n",
    "        'https://services.arcgis.com/P3ePLMYs2RVChkJx/arcgis/rest/services/USA_Counties/FeatureServer/0/'\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    geodata_final : gpd.GeoDataFrame\n",
    "        This is a GeoDataFrame that contains all of the features from the \n",
    "        Feature Server. After calling this function, the `geodata_final` object \n",
    "        can be used to store the data on disk in several different formats \n",
    "        including, but not limited to, Shapefile (.shp), GeoJSON (.geojson), \n",
    "        GeoPackage (.gpkg), or PostGIS.\n",
    "        See https://geopandas.org/en/stable/docs/user_guide/io.html#writing-spatial-data\n",
    "        for more details.\n",
    "\n",
    "    '''\n",
    "    if url_feature_server == '':\n",
    "        geodata_final = gpd.GeoDataFrame()\n",
    "        return geodata_final\n",
    "\n",
    "    # Fixing last character in case the URL provided didn't end in a \n",
    "    # forward slash\n",
    "    if url_feature_server[-1] != '/':\n",
    "        url_feature_server = url_feature_server + '/'\n",
    "    \n",
    "    # Getting the layer definitions. This contains important info such as the \n",
    "    # name of the column used as feature_ids/object_ids, among other things.\n",
    "    layer_def = requests.get(url_feature_server + '?f=pjson').json()\n",
    "    \n",
    "    # The `objectIdField` is the column name used for the \n",
    "    # feature_ids/object_ids\n",
    "    fid_colname = layer_def['objectIdField']\n",
    "    \n",
    "    # The `maxRecordCount` tells us the maximum number of records this REST \n",
    "    # API service can return at once. The code below is written such that we \n",
    "    # perform multiple calls to the API, each one being short enough never to \n",
    "    # go beyond this limit.\n",
    "    record_count_max = layer_def['maxRecordCount']\n",
    "    \n",
    "    # Part of the URL that specifically requests only the object IDs\n",
    "    url_query_get_ids = (f'query?f=geojson&returnIdsOnly=true'\n",
    "                         f'&where={fid_colname}+is+not+null')\n",
    "    \n",
    "    url_comb = url_feature_server + url_query_get_ids\n",
    "    \n",
    "    # Getting all the object IDs\n",
    "    service_request = requests.get(url_comb)\n",
    "    all_objectids = np.sort(service_request.json()['properties']['objectIds'])\n",
    "    \n",
    "    # This variable will store all the parts of the multiple queries. These \n",
    "    # parts will, at the end, be concatenated into one large GeoDataFrame.\n",
    "    geodata_parts = []\n",
    "    \n",
    "    # This part of the query is fixed and never actually changes\n",
    "    url_query_fixed = ('query?f=geojson&outFields=*&where=')\n",
    "    \n",
    "    # Identifying the largest query size allowed per request. This will dictate \n",
    "    # how many queries will need to be made. We start the search at\n",
    "    # the max record count, but that generates errors sometimes - the query \n",
    "    # might time out because it's too big. If the test query times out, we try \n",
    "    # shrink the query size until the test query goes through without \n",
    "    # generating a time-out error.\n",
    "    block_size = min(record_count_max, len(all_objectids))\n",
    "    worked = False\n",
    "    while not worked:\n",
    "        # Moving the \"cursors\" to their appropriate locations\n",
    "        id_start = all_objectids[0]\n",
    "        id_end = all_objectids[block_size-1]\n",
    "\n",
    "        readable_query_string = (f'{fid_colname}>={id_start} '\n",
    "                                 f'and {fid_colname}<={id_end}')\n",
    "        \n",
    "        url_query_variable =  urllib.parse.quote(readable_query_string)\n",
    "    \n",
    "        url_comb = url_feature_server + url_query_fixed + url_query_variable\n",
    "        \n",
    "        url_get = requests.get(url_comb)\n",
    "        \n",
    "        if 'error' in url_get.json():\n",
    "            block_size = int(block_size/2)+1\n",
    "        else:\n",
    "            geodata_part = gpd.read_file(url_get.text)\n",
    "            \n",
    "            geodata_parts.append(geodata_part.copy())\n",
    "            worked = True\n",
    "    \n",
    "    # Performing the actual query to the API multiple times. This skips the \n",
    "    # first few rows/features in the data because those rows were already \n",
    "    # captured in the query performed in the code chunk above.\n",
    "    for i in range(block_size, len(all_objectids), block_size):\n",
    "        # Moving the \"cursors\" to their appropriate locations and finding the \n",
    "        # limits of each block\n",
    "        sub_list = all_objectids[i:i + block_size]\n",
    "        id_start = sub_list[0]\n",
    "        id_end = sub_list[-1]\n",
    "\n",
    "        readable_query_string = (f'{fid_colname}>={id_start} '\n",
    "                                 f'and {fid_colname}<={id_end}')\n",
    "        \n",
    "        # Encoding from readable text to URL\n",
    "        url_query_variable =  urllib.parse.quote(readable_query_string)\n",
    "    \n",
    "        # Constructing the full request URL\n",
    "        url_comb = url_feature_server + url_query_fixed + url_query_variable\n",
    "        \n",
    "        # Actually performing the query and storing its results in a \n",
    "        # GeoDataFrame\n",
    "        geodata_part =  (gpd.read_file(url_comb, \n",
    "                                       driver='GeoJSON'))\n",
    "        \n",
    "        # Appending the result to `geodata_parts`\n",
    "        if geodata_part.shape[0] > 0:\n",
    "            geodata_parts.append(geodata_part)\n",
    "\n",
    "    # Concatenating all of the query parts into one large GeoDataFrame\n",
    "    geodata_final = (pd.concat(geodata_parts, \n",
    "                               ignore_index=True)\n",
    "                     .sort_values(by=fid_colname)\n",
    "                     .reset_index(drop=True))\n",
    "    \n",
    "    # Checking if any object ID is missing\n",
    "    ids_queried = set(geodata_final[fid_colname])\n",
    "    for i,this_id in enumerate(all_objectids):\n",
    "        if this_id not in ids_queried:\n",
    "            print('WARNING! The following ObjectID is missing from the final '\n",
    "                  f'GeoDataFrame: ObjectID={this_id}')\n",
    "            pass\n",
    "    \n",
    "    # Checking if any object ID is included twice\n",
    "    geodata_temp = geodata_final[[fid_colname]].copy()\n",
    "    geodata_temp['temp'] = 1\n",
    "    geodata_temp = (geodata_temp\n",
    "                    .groupby(fid_colname)\n",
    "                    .agg({'temp':'sum'})\n",
    "                    .reset_index())\n",
    "    geodata_temp = geodata_temp.loc[geodata_temp['temp']>1].copy()\n",
    "    for i,this_id in enumerate(geodata_temp[fid_colname].values):\n",
    "        n_times = geodata_temp['temp'].values[i]\n",
    "        print('WARNING! The following ObjectID is included multiple times in'\n",
    "              f'the final GeoDataFrame: ObjectID={this_id}\\tOccurrences={n_times}')\n",
    "    \n",
    "    return geodata_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c46065a3-55f5-44c5-b50b-6bab3058f035",
   "metadata": {},
   "outputs": [],
   "source": [
    "QUERY = (\"https://caltrans-gis.dot.ca.gov/arcgis/rest/services/\"\n",
    "         \"CHhighway/SHN_Postmiles_Tenth/FeatureServer/0/\"\n",
    "        )\n",
    "\n",
    "gdf = query_arcgis_feature_server(QUERY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb7e6075-7316-44ea-b19f-5cfbe3bd249a",
   "metadata": {},
   "outputs": [],
   "source": [
    "gdf.to_parquet(\"./postmile_feature_server.parquet\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
