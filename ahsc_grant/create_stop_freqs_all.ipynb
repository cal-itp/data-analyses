{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bbcf5856-259a-4f51-8058-ee9c23e97db5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/geopandas/_compat.py:123: UserWarning: The Shapely GEOS version (3.10.3-CAPI-1.16.1) is incompatible with the GEOS version PyGEOS was compiled with (3.10.1-CAPI-1.16.0). Conversions between both will be slow.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"CALITP_BQ_MAX_BYTES\"] = str(800_000_000_000) ## 800GB?\n",
    "\n",
    "import branca\n",
    "import folium\n",
    "import shared_utils\n",
    "\n",
    "from siuba import *\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import dask.dataframe as dd\n",
    "\n",
    "pd.set_option('display.max_columns', None) \n",
    "\n",
    "import datetime as dt\n",
    "import time\n",
    "\n",
    "from calitp import get_engine\n",
    "from calitp.tables import tbls\n",
    "\n",
    "engine = get_engine()\n",
    "connection = engine.connect()\n",
    "\n",
    "GCS_FILE_PATH = 'gs://calitp-analytics-data/data-analyses/ahsc_grant/'\n",
    "\n",
    "import gcsfs\n",
    "fs = gcsfs.GCSFileSystem()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9c84e5a-6c1e-46dd-be1e-bfab99eb0fe8",
   "metadata": {},
   "source": [
    "# Creating trips per am peak / midday / pm / weekend by stop\n",
    "This is assembles the by-stop analytical file. This version loops over all operators with GTFS Schedule data, for recent analysis days. It then joins ACS characteristics.\n",
    "\n",
    "## File Assembly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5c806291-b1f3-4d8d-9a71-5f8193453257",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set date parameters\n",
    "analysis_wkd = dt.date(2022,11,2)\n",
    "analysis_sat = dt.date(2022,11,5)\n",
    "analysis_sun = dt.date(2022,11,6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1100ffc3-e535-4178-b1a7-84435258edef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in ACS data\n",
    "acs_ca = gpd.read_parquet(f\"{GCS_FILE_PATH}acs_tbl_ca.parquet\")\n",
    "\n",
    "# join to job data\n",
    "jobdata=(pd.read_parquet(f\"{GCS_FILE_PATH}job_density\")\n",
    "         >> select(_.geo_id,_.jobs_total)\n",
    "        )\n",
    "\n",
    "acs_ca = (acs_ca\n",
    "          >> inner_join(_,jobdata)\n",
    "         )\n",
    "\n",
    "#project\n",
    "acs_ca = acs_ca.to_crs(shared_utils.geography_utils.CA_NAD83Albers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "17f13fa8-1f14-4556-959a-27b4ea1d61e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get all operators\n",
    "airtable_itp_ids = (\n",
    "    tbls.airtable.california_transit_organizations()\n",
    "    >> filter(_.at_least_one_gtfs_feed_for_any_service==1)\n",
    "    >> select(_.itp_id, _.name, _.caltrans_district)\n",
    "    >> collect()\n",
    ").dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "45736fce-37b6-429d-8813-d2ea5db4f9b0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>itp_id</th>\n",
       "      <th>name</th>\n",
       "      <th>caltrans_district</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>42</td>\n",
       "      <td>Blue Lake Rancheria</td>\n",
       "      <td>01 - Eureka</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>18</td>\n",
       "      <td>City of Arcata</td>\n",
       "      <td>01 - Eureka</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>108</td>\n",
       "      <td>City of Eureka</td>\n",
       "      <td>01 - Eureka</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>261</td>\n",
       "      <td>Redwood Coast Transit Authority</td>\n",
       "      <td>01 - Eureka</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100</th>\n",
       "      <td>135</td>\n",
       "      <td>Humboldt Transit Authority</td>\n",
       "      <td>01 - Eureka</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>154</td>\n",
       "      <td>City of Laguna Beach</td>\n",
       "      <td>12 - Irvine</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57</th>\n",
       "      <td>14</td>\n",
       "      <td>Anaheim Transportation Network</td>\n",
       "      <td>12 - Irvine</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>111</th>\n",
       "      <td>235</td>\n",
       "      <td>Orange County Transportation Authority</td>\n",
       "      <td>12 - Irvine</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>150</th>\n",
       "      <td>394</td>\n",
       "      <td>City of San Juan Capistrano</td>\n",
       "      <td>12 - Irvine</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>168</th>\n",
       "      <td>137</td>\n",
       "      <td>City of Huntington Park</td>\n",
       "      <td>12 - Irvine</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>169 rows Ã— 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     itp_id                                    name caltrans_district\n",
       "4        42                     Blue Lake Rancheria       01 - Eureka\n",
       "13       18                          City of Arcata       01 - Eureka\n",
       "20      108                          City of Eureka       01 - Eureka\n",
       "98      261         Redwood Coast Transit Authority       01 - Eureka\n",
       "100     135              Humboldt Transit Authority       01 - Eureka\n",
       "..      ...                                     ...               ...\n",
       "22      154                    City of Laguna Beach       12 - Irvine\n",
       "57       14          Anaheim Transportation Network       12 - Irvine\n",
       "111     235  Orange County Transportation Authority       12 - Irvine\n",
       "150     394             City of San Juan Capistrano       12 - Irvine\n",
       "168     137                 City of Huntington Park       12 - Irvine\n",
       "\n",
       "[169 rows x 3 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "airtable_itp_ids = (airtable_itp_ids \n",
    "                    >> mutate(itp_id = _.itp_id.astype('int64'))\n",
    "                    >> arrange(_.caltrans_district)\n",
    "                   )\n",
    "\n",
    "airtable_itp_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "354e6eb8-33b6-4b4b-a289-49d07dbab637",
   "metadata": {},
   "outputs": [],
   "source": [
    "#get table of trip keys for different days\n",
    "def daily_trips(day_type, analysis_dt):\n",
    "    trips_df = (shared_utils.gtfs_utils.get_trips(\n",
    "        selected_date = analysis_dt,\n",
    "        itp_id_list = [itp_id],\n",
    "        trip_cols = [\"calitp_itp_id\",\"trip_id\",\"route_id\",\"shape_id\",\"direction_id\"]\n",
    "    ) \n",
    "                    ) \n",
    "    \n",
    "    if trips_df.empty:\n",
    "        return pd.DataFrame()\n",
    "    \n",
    "    #routes contains route type - filter to bus\n",
    "    routes_df = (shared_utils.gtfs_utils.get_route_info(\n",
    "        selected_date = analysis_dt,\n",
    "        itp_id_list = [itp_id],\n",
    "        route_cols = [\"calitp_itp_id\",\"route_type\",\"route_id\"]\n",
    "    ) \n",
    "                    ) \n",
    "\n",
    "    trips_df = (trips_df\n",
    "                     >> left_join(_,routes_df)\n",
    "                     >> filter(_.route_type==\"3\")\n",
    "               )\n",
    "    # stop times contains how many trips go by a stop\n",
    "    stoptimes_df = (shared_utils.gtfs_utils.get_stop_times(\n",
    "        selected_date = analysis_dt,\n",
    "        itp_id_list = [itp_id],\n",
    "        get_df = True\n",
    "    )\n",
    "        >> left_join(_,trips_df)\n",
    "        >> group_by(_.calitp_itp_id,_.stop_id)\n",
    "        >> summarize(n_trips = _.trip_id.nunique(),\n",
    "                     n_routes = _.route_id.nunique()\n",
    "                    )\n",
    "        >> ungroup()\n",
    "        >> mutate(daytype = day_type,\n",
    "                  analysis_date = analysis_dt\n",
    "                 )\n",
    "                       )\n",
    "    #stop geometry\n",
    "    stops_geo = shared_utils.gtfs_utils.get_stops(\n",
    "                     selected_date = analysis_dt, \n",
    "                     itp_id_list = [itp_id]\n",
    "    )\n",
    "    \n",
    "    stoptimes_geo = (stops_geo.merge(stoptimes_df, on = [\"calitp_itp_id\",\"stop_id\"])\n",
    "                     >> select (_.calitp_itp_id,_.stop_desc,_.stop_name,_.stop_id,\n",
    "                                _.geometry,_.n_trips,_.n_routes,_.daytype,_.analysis_date\n",
    "                               )\n",
    "                     >> mutate (point_geometry = _.geometry)\n",
    "                    ).to_crs(shared_utils.geography_utils.CA_NAD83Albers)\n",
    "    \n",
    "    #replace geometry with a .25mi buffer\n",
    "    stoptimes_geo.geometry = stoptimes_geo.buffer(402.336)\n",
    "    \n",
    "    #join w/ acs data\n",
    "    stops_acs_joined = stoptimes_geo.sjoin(acs_ca, how='left', predicate='intersects')\n",
    "    \n",
    "    #roll back up to stop level\n",
    "    stops_acs_rollup = (stops_acs_joined\n",
    "                    >> group_by(_.calitp_itp_id,_.stop_id, _.stop_name,\n",
    "                                _.n_trips,_.n_routes,_.daytype,_.analysis_date)\n",
    "                    >> summarize(sum_tracts = _.geo_id.nunique(),\n",
    "                                 sum_total_pop = _.total_pop.sum(),\n",
    "                                 sum_households = _.households.sum(),\n",
    "                                 sum_not_us_citizen_pop = _.not_us_citizen_pop.sum(),\n",
    "                                 sum_youth_pop = _.youth_pop.sum(),\n",
    "                                 sum_seniors_pop = _.seniors_pop.sum(),\n",
    "                                 sum_pop_determined_poverty_status = _.pop_determined_poverty_status.sum(), #denominator for poverty rate\n",
    "                                 sum_poverty = _.poverty.sum(),\n",
    "                                 sum_no_car = _.no_car.sum(), #workers without access to car\n",
    "                                 sum_no_cars = _.no_cars.sum(), #households without car\n",
    "                                 sum_land_area = _.ALAND.sum(),\n",
    "                                 sum_jobs=_.jobs_total.sum()\n",
    "                                )\n",
    "                    >> ungroup()\n",
    "                    >> mutate(land_area_sqkm=_.sum_land_area/1000000,\n",
    "                           pop_density = _.sum_total_pop/_.land_area_sqkm,\n",
    "                           job_density = _.sum_jobs/_.land_area_sqkm,\n",
    "                           pct_not_us_citizen_pop = (_.sum_not_us_citizen_pop/_.sum_total_pop)*100,\n",
    "                           pct_youth_pop = (_.sum_youth_pop/_.sum_total_pop)*100,\n",
    "                           pct_seniors_pop = (_.sum_seniors_pop/_.sum_total_pop)*100,\n",
    "                           pct_poverty = (_.sum_poverty/_.sum_pop_determined_poverty_status)*100,\n",
    "                           pct_pop_workers_no_car = (_.sum_no_car/_.sum_total_pop)*100,\n",
    "                           pct_hh_no_cars = (_.sum_no_cars/_.sum_households)*100\n",
    "                          )\n",
    "                       )\n",
    "    \n",
    "    #put back point geometry\n",
    "    stops_acs_rollup_gpd = (stops_geo \n",
    "                            >> select(_.calitp_itp_id,_.stop_name,_.stop_id, _.geometry)\n",
    "                            >> right_join(_,stops_acs_rollup)\n",
    "                           )\n",
    "    \n",
    "    return stops_acs_rollup_gpd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6ec0219b-9dc0-4226-b180-7a4456d89fd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "fs_list = fs.ls(f\"{GCS_FILE_PATH}tool_data/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ac271b81-8e32-4436-81c4-e17f3390c0ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "ran_operators = [\n",
    "        int(path.split(\"tool_data/\")[1].split(\"_\")[2].split(\".\")[0])\n",
    "        for path in fs_list\n",
    "        if \".parquet\" in path.split(\"tool_data/\")[1]\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "eb5211a4-1928-4b35-9d4d-a9520698ecc8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "already ran: 42\n",
      "already ran: 18\n",
      "already ran: 108\n",
      "already ran: 261\n",
      "already ran: 135\n",
      "already ran: 198\n",
      "already ran: 159\n",
      "already ran: 344\n",
      "already ran: 329\n",
      "already ran: 254\n",
      "already ran: 259\n",
      "already ran: 83\n",
      "already ran: 334\n",
      "already ran: 162\n",
      "already ran: 331\n",
      "already ran: 271\n",
      "already ran: 351\n",
      "already ran: 23\n",
      "already ran: 221\n",
      "already ran: 47\n",
      "already ran: 372\n",
      "already ran: 251\n",
      "already ran: 101\n",
      "already ran: 376\n",
      "already ran: 273\n",
      "already ran: 81\n",
      "already ran: 13\n",
      "already ran: 194\n",
      "already ran: 315\n",
      "already ran: 246\n",
      "already ran: 356\n",
      "already ran: 61\n",
      "already ran: 279\n",
      "already ran: 280\n",
      "already ran: 110\n",
      "already ran: 247\n",
      "already ran: 350\n",
      "already ran: 167\n",
      "already ran: 310\n",
      "already ran: 314\n",
      "already ran: 336\n",
      "already ran: 282\n",
      "already ran: 4\n",
      "already ran: 127\n",
      "already ran: 301\n",
      "already ran: 368\n",
      "already ran: 294\n",
      "already ran: 218\n",
      "already ran: 290\n",
      "already ran: 15\n",
      "already ran: 41\n",
      "already ran: 324\n",
      "already ran: 338\n",
      "already ran: 201\n",
      "already ran: 257\n",
      "already ran: 106\n",
      "already ran: 481\n",
      "already ran: 365\n",
      "already ran: 281\n",
      "already ran: 56\n",
      "already ran: 76\n",
      "already ran: 199\n",
      "already ran: 216\n",
      "already ran: 33\n",
      "already ran: 379\n",
      "already ran: 287\n",
      "already ran: 293\n",
      "already ran: 129\n",
      "already ran: 289\n",
      "already ran: 274\n",
      "already ran: 312\n",
      "already ran: 296\n",
      "already ran: 298\n",
      "already ran: 169\n",
      "already ran: 208\n",
      "already ran: 213\n",
      "already ran: 116\n",
      "already ran: 188\n",
      "already ran: 21\n",
      "already ran: 330\n",
      "already ran: 91\n",
      "already ran: 126\n",
      "already ran: 148\n",
      "already ran: 146\n",
      "already ran: 117\n",
      "already ran: 361\n",
      "already ran: 474\n",
      "already ran: 71\n",
      "already ran: 188\n",
      "already ran: 179\n",
      "already ran: 186\n",
      "processing agency 323 starting 2022-11-28 10:06:55.881714...\n",
      "processing agency 112 starting 2022-11-28 10:07:41.588924...\n",
      "processing agency 97 starting 2022-11-28 10:09:03.136312...\n",
      "processing agency 231 starting 2022-11-28 10:09:07.792874...\n",
      "processing agency 239 starting 2022-11-28 10:10:11.059153...\n",
      "processing agency 29 starting 2022-11-28 10:10:28.426837...\n",
      "processing agency 120 starting 2022-11-28 10:11:10.574816...\n",
      "processing agency 118 starting 2022-11-28 10:11:57.007881...\n",
      "processing agency 75 starting 2022-11-28 10:12:01.993751...\n",
      "processing agency 35 starting 2022-11-28 10:12:48.470316...\n",
      "processing agency 36 starting 2022-11-28 10:13:15.510487...\n",
      "processing agency 77 starting 2022-11-28 10:13:43.636405...\n",
      "processing agency 37 starting 2022-11-28 10:14:11.763999...\n",
      "processing agency 182 starting 2022-11-28 10:14:28.108692...\n",
      "processing agency 16 starting 2022-11-28 10:20:53.726452...\n",
      "processing agency 45 starting 2022-11-28 10:21:50.014502...\n",
      "processing agency 260 starting 2022-11-28 10:22:06.711280...\n",
      "processing agency 6 starting 2022-11-28 10:22:51.218427...\n",
      "processing agency 320 starting 2022-11-28 10:23:19.934708...\n",
      "processing agency 17 starting 2022-11-28 10:23:47.407474...\n",
      "processing agency 243 starting 2022-11-28 10:24:26.868517...\n",
      "processing agency 49 starting 2022-11-28 10:25:15.770399...\n",
      "processing agency 54 starting 2022-11-28 10:25:45.277069...\n",
      "processing agency 300 starting 2022-11-28 10:25:50.002411...\n",
      "processing agency 121 starting 2022-11-28 10:26:50.998039...\n",
      "processing agency 228 starting 2022-11-28 10:27:07.582820...\n",
      "processing agency 95 starting 2022-11-28 10:28:02.190337...\n",
      "processing agency 367 starting 2022-11-28 10:28:19.118580...\n",
      "processing agency 102 starting 2022-11-28 10:28:58.925946...\n",
      "processing agency 87 starting 2022-11-28 10:29:41.969659...\n",
      "processing agency 366 starting 2022-11-28 10:30:33.243304...\n",
      "processing agency 210 starting 2022-11-28 10:30:49.341396...\n",
      "processing agency 24 starting 2022-11-28 10:31:05.131322...\n",
      "processing agency 339 starting 2022-11-28 10:31:44.513612...\n",
      "processing agency 207 starting 2022-11-28 10:32:39.556181...\n",
      "processing agency 308 starting 2022-11-28 10:32:56.985784...\n",
      "processing agency 170 starting 2022-11-28 10:33:27.189885...\n",
      "processing agency 123 starting 2022-11-28 10:34:49.177964...\n",
      "processing agency 380 starting 2022-11-28 10:35:44.098296...\n",
      "processing agency 337 starting 2022-11-28 10:36:44.513897...\n",
      "processing agency 206 starting 2022-11-28 10:37:12.837676...\n",
      "processing agency 295 starting 2022-11-28 10:38:07.321114...\n",
      "processing agency 183 starting 2022-11-28 10:39:00.682409...\n",
      "processing agency 165 starting 2022-11-28 10:40:34.378465...\n",
      "processing agency 152 starting 2022-11-28 10:41:14.489360...\n",
      "processing agency 305 starting 2022-11-28 10:41:53.885279...\n",
      "processing agency 103 starting 2022-11-28 10:42:09.763103...\n",
      "processing agency 79 starting 2022-11-28 10:42:25.869895...\n",
      "processing agency 360 starting 2022-11-28 10:42:53.670101...\n",
      "processing agency 269 starting 2022-11-28 10:43:56.892740...\n",
      "processing agency 327 starting 2022-11-28 10:45:24.724065...\n",
      "processing agency 232 starting 2022-11-28 10:46:18.949252...\n",
      "processing agency 30 starting 2022-11-28 10:47:41.190607...\n",
      "processing agency 34 starting 2022-11-28 10:48:22.325657...\n",
      "processing agency 214 starting 2022-11-28 10:48:52.252046...\n",
      "processing agency 212 starting 2022-11-28 10:49:34.263631...\n",
      "processing agency 99 starting 2022-11-28 10:49:39.404338...\n",
      "processing agency 10 starting 2022-11-28 10:50:23.046447...\n",
      "processing agency 374 starting 2022-11-28 10:50:39.426377...\n",
      "processing agency 192 starting 2022-11-28 10:51:20.777890...\n",
      "processing agency 107 starting 2022-11-28 10:51:50.561148...\n",
      "processing agency 341 starting 2022-11-28 10:52:07.129798...\n",
      "processing agency 343 starting 2022-11-28 10:52:11.874476...\n",
      "processing agency 349 starting 2022-11-28 10:52:59.168131...\n",
      "processing agency 11 starting 2022-11-28 10:53:30.395177...\n",
      "processing agency 168 starting 2022-11-28 10:53:47.521959...\n",
      "processing agency 284 starting 2022-11-28 10:54:16.490362...\n",
      "processing agency 484 starting 2022-11-28 10:55:05.747838...\n",
      "processing agency 386 starting 2022-11-28 10:56:06.956064...\n",
      "processing agency 278 starting 2022-11-28 10:56:36.512429...\n",
      "processing agency 226 starting 2022-11-28 10:59:22.856683...\n",
      "processing agency 32 starting 2022-11-28 11:00:37.144203...\n",
      "processing agency 277 starting 2022-11-28 11:00:41.975608...\n",
      "processing agency 154 starting 2022-11-28 11:00:48.693698...\n",
      "processing agency 14 starting 2022-11-28 11:01:31.131594...\n",
      "processing agency 235 starting 2022-11-28 11:02:34.127417...\n",
      "processing agency 394 starting 2022-11-28 11:04:54.168121...\n",
      "processing agency 137 starting 2022-11-28 11:04:59.382326...\n"
     ]
    }
   ],
   "source": [
    "for itp_id in airtable_itp_ids['itp_id'].tolist():\n",
    "    if itp_id in ran_operators:\n",
    "        print(f\"already ran: {itp_id}\")\n",
    "        continue\n",
    "    else:\n",
    "        print(f\"processing agency {itp_id} starting {dt.datetime.now()}...\")\n",
    "        stops_weekday = daily_trips(\"Weekday\",analysis_wkd)\n",
    "        stops_saturday = daily_trips(\"Saturday\",analysis_sat)\n",
    "        stops_sunday = daily_trips(\"Sunday\",analysis_sun)\n",
    "        stoptimes_all = pd.concat([stops_weekday,stops_saturday,stops_sunday], ignore_index=True)\n",
    "        shared_utils.utils.geoparquet_gcs_export(stoptimes_all, f\"{GCS_FILE_PATH}tool_data/\", f\"trips_perstop_{itp_id}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e66cdbd3-9fa6-450b-989f-cb8d2c5f9219",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
