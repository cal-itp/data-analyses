{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "04af490c-5230-4b28-aeb4-d636f44a28ba",
   "metadata": {},
   "source": [
    "## Exercise 2: Merging and Deriving New Columns\n",
    "\n",
    "Skills: \n",
    "* Merge 2 dataframes\n",
    "* F-strings!\n",
    "* Markdown cells\n",
    "* Build on groupby/agg knowledge, derive new columns, exporting\n",
    "* Practice committing on GitHub\n",
    "\n",
    "References: \n",
    "* https://docs.calitp.org/data-infra/analytics_new_analysts/01-data-analysis-intro.html\n",
    "* https://docs.calitp.org/data-infra/analytics_tools/saving_code.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "940e12c5-6775-4aa1-b7df-c00ca89732af",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4487520-d005-4262-b19f-4433203b12df",
   "metadata": {},
   "source": [
    "Use of f-strings. [Read more on this](https://realpython.com/python-f-strings/#f-strings-a-new-and-improved-way-to-format-strings-in-python).\n",
    "\n",
    "Also, click on this Markdown cell and see how to do different formatting syntax within Markdown. [Reference this](https://www.datacamp.com/community/tutorials/markdown-in-jupyter-notebook).\n",
    "\n",
    "If you don't have access to Google Cloud Storage, change the path to pull from our truncated sample parquets stored in the repo.\n",
    "\n",
    "We use [relative paths](https://towardsthecloud.com/get-relative-path-python) rather than absolute paths. Since we are in the `starter_kit` directory, we just need to go one more level in to the `data` subfolder. To get one level outside of `starter_kit`, use `../` and you'll end up in `data-analyses`. \n",
    "\n",
    "```\n",
    "FOLDER = \"./data/\"\n",
    "FILE_NAME = \"exercise_2_3_ntd_metrics_2019.parquet\"\n",
    "df = pd.read_parquet(f\"{FOLDER}{FILE_NAME}\")\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "287cd877-e8a9-4db8-846d-3ba384e7979a",
   "metadata": {},
   "outputs": [],
   "source": [
    "GCS_FILE_PATH = \"gs://calitp-analytics-data/data-analyses/bus_service_increase/\"\n",
    "FILE_NAME = \"ntd_metrics_2019.csv\"\n",
    "\n",
    "df = pd.read_csv(f\"{GCS_FILE_PATH}{FILE_NAME}\")\n",
    "df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fa36e5f-7ccd-4af0-9edb-f6e0ae96d542",
   "metadata": {},
   "outputs": [],
   "source": [
    "FILE_NAME = \"ntd_vehicles_2019.csv\"\n",
    "df2 = pd.read_csv(f\"{GCS_FILE_PATH}{FILE_NAME}\")\n",
    "\n",
    "df2.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cfc1e31-12d8-490e-9473-1dffe0710047",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3994dc75-80b3-4094-ad3f-d37c415587d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"NTD ID\"].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5b19028-cf65-4cd8-9bbb-e74076365374",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df2)\n",
    "df2[\"NTD ID\"].nunique()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3aac90ed-cbf2-4ba8-bd2a-a603cb1c2b26",
   "metadata": {},
   "source": [
    "### To do:\n",
    "\n",
    "* Start with the `ntd_metrics_2019.csv` dataset.\n",
    "* Merge in the `ntd_vehicles_2019.csv` dataset from the same location within the GCS bucket, but only keep a couple of columns.\n",
    "* Print out what states there are using `value_counts`\n",
    "* Subset and only keep the following states: NY, CA, TX, ID, MS\n",
    "* Calculate some aggregate statistics grouping by states (the point of the exercise is to aggregate, less so on whether the stats make sense):\n",
    "    * Include: sum, mean, count (of operators), nunique (of city)\n",
    "    * Challenge: give a per capita measure, such as total service vehicles per 100,000 residents\n",
    "* Plot the per capita measure across the 5 states (some states are very populous and some are not...per capita hopefully normalizes pop differences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d8ee58a-a478-4ca0-931d-513729dff275",
   "metadata": {},
   "outputs": [],
   "source": [
    "list(df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f668dfe-406d-4786-96bd-ea5df4810119",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.Agency.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62261f84-c908-4285-9a8c-a0e95f425629",
   "metadata": {},
   "source": [
    "### Helpful Hints and Best Practices\n",
    "\n",
    "* Start with comprehensive approach in writing down all the lines of code to clean data. \n",
    "* Once the data cleaning process is done, work on refining the code and tidying it to see what steps can be chained together, what steps are done repeatedly (use a function!), etc.\n",
    "\n",
    "#### Chaining\n",
    "Similar to **piping** in R, where you can pipe multiple operations in 1 line of code with `>>`, you can do a similar method of chaining in Python. There is also a `df.pipe` function, but that's slightly different.\n",
    "\n",
    "Make use of parentheses to do this. Also, use `df.assign` (see below) so you don't run into the `SettingWithCopyWarning`, which may pop up if you decide to subset your data. \n",
    "\n",
    "#### Assign \n",
    "You can create new columns in place, and the warning that comes up is mostly harmless. But, `assign` also lets you chain more operations after. [More clarification.](https://pythonguides.com/add-a-column-to-a-dataframe-in-python-pandas/)\n",
    "```\n",
    "states_clean = (states_clean\n",
    "    # Assign is similar to R dplyr's mutate\n",
    "    .assign(\n",
    "        # Strip leading or trailing blanks (slightly different than replace)\n",
    "        # Decide if you want to replace all blanks or just leading/trailing\n",
    "        Agency = (states_clean.Agency.str.strip()\n",
    "                .str.replace('(', '').str.replace(')', '')\n",
    "        ),\n",
    "        # Do something similar for City as above\n",
    "        City = states_clean.City.str.strip(),\n",
    "        # Replace blanks with nothing\n",
    "        State = states_clean.State.str.replace(' ', '')\n",
    "    ).astype({\n",
    "        \"Population\": int, \n",
    "        \"Fare_Revenues\": int,\n",
    "    })\n",
    ")\n",
    "```\n",
    "\n",
    "Alternatively, try it with a loop:\n",
    "\n",
    "```\n",
    "for c in [\"Agency\", \"City\"]:\n",
    "    df[c] = (df[c].str.strip()\n",
    "            .str.replace('(', '')\n",
    "            .str.replace(')', '')\n",
    "            .astype(int)\n",
    "            )\n",
    "```\n",
    "\n",
    "#### Using `str.contains` with some special characters\n",
    "Use backslash `\\` to \"escape\". [StackOverflow explanation](https://stackoverflow.com/questions/48699907/error-unbalanced-parenthesis-while-checking-if-an-item-presents-in-a-pandas-d)\n",
    "`states_clean[states_clean.Fare_Revenues.str.contains(\"\\(\")]`\n",
    "\n",
    "\n",
    "#### Merging\n",
    "If your merge results produces a `col_x` and `col_y`, add more columns to your list of merge columns, with `on = [\"col1\", \"col2\"]`.\n",
    "\n",
    "#### Use `isin` to filter by multiple conditions\n",
    "\n",
    "```\n",
    "keep_me = [\"CA\", \"NY\", \"TX\"]\n",
    "df2 = df[df.State.isin(keep_me)]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2db26245-2188-41cd-bc31-4444f11162e2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
