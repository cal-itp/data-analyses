{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "247e773f-0e29-4ed6-ab4d-5856325611b4",
   "metadata": {},
   "source": [
    "# Exercise 1: Familiarize yourself with `pandas`\n",
    "If you are new to Python, check out the introductory Python courses available through Caltrans's LinkedIn Learning Library:\n",
    "* https://www.linkedin.com/learning/search?keywords=python&u=36029164\n",
    "\n",
    "Skills: \n",
    "* `pandas` is one of the base Python packages for working with tabular data.\n",
    "* Do some grouping and aggregation. Many ways to do this!\n",
    "* F-strings\n",
    "* Export to Google Cloud Storage\n",
    "* Practice committing on GitHub\n",
    "\n",
    "References: \n",
    "* https://docs.calitp.org/data-infra/analytics_new_analysts/01-data-analysis-intro.html\n",
    "* https://docs.calitp.org/data-infra/analytics_tools/saving_code.html\n",
    "\n",
    "## What are we working with today? \n",
    "* Today we will be working on Caltrans System Investment Strategy (CSIS) today. Per this [description](https://dot.ca.gov/programs/transportation-planning/division-of-transportation-planning/corridor-and-system-planning/csis)\n",
    "> The California Department of Transportation (Caltrans) is committed to leading climate action and advancing social equity in the transportation sector set forth by the California State Transportation Agency (CalSTA) Climate Action Plan for Transportation Infrastructure (CAPTI, 2021)...Caltrans is in a significant leadership role to carry out meaningful measures that advance state’s goals and priorities through the development and implementation of the Caltrans System Investment Strategy (CSIS). The CSIS, which implements one of CAPTI’s key actions, is envisioned to be an investment framework through a data and performance-driven approach that guides transportation investments and decisions.\n",
    "* One way DDS is working on CSIS is by automating the scoring of projects using Python. We score each project based on how well they do in various categories, aka metrics such as Zero Emmission Vehicles, Vehicle Miles Traveled, and more. \n",
    "* While the values in we are working with today are all <i>fake</i>, the exercise really is based on actual datasets and assignments. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "50199af7-04a8-43c5-ba1b-4127940749bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import altair as alt\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc30cb7d-77d3-465b-9831-8810096af9b1",
   "metadata": {},
   "source": [
    "## Check out the data \n",
    "* Download the Excel workbook containing all the CSIS data from Google Cloud Storage [here](https://console.cloud.google.com/storage/browser/_details/calitp-analytics-data/data-analyses/starter_kit/starter_kit_csis_scoring_workbook.xlsx;tab=live_object?project=cal-itp-data-infra). \n",
    "    * Open it up in Excel and take a look.\n",
    "### Read in the data\n",
    "* We are reading our Excel Workbook into a Pandas dataframe.\n",
    "* While there is a very [technical definition](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.html) of what a dataframe is, you can think of it as an Excel sheet that holds your data. A pandas dataframe merely allows you to clean the data programatically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5950cb87-75ab-4871-ab4b-a8f1c41f0a4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"gs://calitp-analytics-data/data-analyses/starter_kit/starter_kit_csis_scoring_workbook.xlsx\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e09456e0-dfd2-4388-85de-eb9e95f983fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_excel(url)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "179960a3-6c9b-42af-a8f1-d6156c4be2d2",
   "metadata": {},
   "source": [
    "### Previewing Data \n",
    "* Often, you want to get a sneak preview of your data. \n",
    "* Thankfully, Python provides many methods for you to do so. \n",
    "* Below are a couple of very common methods we use. \n",
    "    * `.head()` shows the first five rows, while `.tail()` shows the last five.\n",
    "    * `.sample()` shows you a random row.\n",
    "    * Want to see or less than five? Specify it in the parantheses: `.head(10)`.\n",
    "* Try everything yourself below."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3386e9d8-15cd-48bc-8b1f-cf6f95512ad5",
   "metadata": {},
   "source": [
    "### Reviewing the Data - More Methods!\n",
    "* `df.shape` gives you the number of rows and columns in your dataset.\n",
    "* `df.columns` returns all of the column names.\n",
    "* `df.info()` per the [pandas docs](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.info.html#pandas.DataFrame.info) <i>prints information about a DataFrame including the index dtype and columns, non-null values and memory usage.</i>\n",
    "* Experiment below. \n",
    "* More food for thought:\n",
    "    * `Dtype` is critical. There are integers, objects, booleans, floats...\n",
    "    * Does the `dtype` of each column below make sense to you? \n",
    "    * The `dtype` of object is a catchall term."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7f55b33e-d402-473b-815a-92ad935d35d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 29 entries, 0 to 28\n",
      "Data columns (total 3 columns):\n",
      " #   Column         Non-Null Count  Dtype \n",
      "---  ------         --------------  ----- \n",
      " 0   ct_district    29 non-null     object\n",
      " 1   project_name   29 non-null     object\n",
      " 2   Scope of Work  29 non-null     object\n",
      "dtypes: object(3)\n",
      "memory usage: 824.0+ bytes\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d117908f-af05-4e95-8042-39a3e0557d6f",
   "metadata": {},
   "source": [
    "### Deeper Dive\n",
    "* We now know a good amount about our dataset, but the # of rows and columns are not always so thrilling. \n",
    "* Let's take a look at each column."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55cece73-c3d5-4cd7-8896-f97d43fc1114",
   "metadata": {},
   "source": [
    "#### `.value_counts()` helps you see how many times the same value appears. \n",
    "* Notice something repetitive? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "63f21ab5-0920-4310-afce-2ea657556912",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10          5\n",
       "11          4\n",
       "1           3\n",
       "9           3\n",
       "2           3\n",
       "5           2\n",
       "3           2\n",
       "7           2\n",
       "4 and 12    1\n",
       "4           1\n",
       "three       1\n",
       "12          1\n",
       "nine        1\n",
       "Name: ct_district, dtype: int64"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.ct_district.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55baf38e-3776-4448-b375-9e124030bae2",
   "metadata": {},
   "source": [
    "#### `.nunique()` displays the number of distinct values in your column\n",
    "* This is particulary useful because there are many times when the number of unique values of a column should match the number of rows of your dataset <b>exactly</b>.\n",
    "* In our case, our dataframe has 29 rows and we should have 29 unique project names and scope of work descriptions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "1d832308-a425-404d-83a0-53ce8bfae279",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "29"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.project_name.nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "4e232324-f75f-46a0-962d-76ed9273dac7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "29"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Notice that when you have spaces in between each string of your column name,\n",
    "# you need to refer the column using brackets []. \n",
    "df[\"Scope of Work\"].nunique()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06ee15f6-ee2e-4e3e-91b2-115875292042",
   "metadata": {},
   "source": [
    "## Something missing? \n",
    "* Open up our dataset using Excel. \n",
    "* Take a look at the sheets: how many are there in the Excel worbook? \n",
    "* Which sheet is loaded into `df` above? "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5302dd99-acb2-40d7-b00d-4f0493ee5e09",
   "metadata": {},
   "source": [
    "### Lists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "02380fb6-c55b-477f-acfb-8b483e83beac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enter in all the sheets you are interested in loading into Python.\n",
    "# By the way, they always need to be strings.\n",
    "my_sheets = [\"projects_auto\",\n",
    "            \"overall_score\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a9a1a3e-e10d-4447-96dd-92ecb2fe6357",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(my_sheets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3be037d-b21b-4192-9099-25bfcb660f01",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Index\n",
    "my_sheets[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebf91535-a466-446a-9f7a-606503d78b6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_sheets[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2e2578bc-db1f-41f5-bc07-3cb82998420e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open the workbook in a dictionary\n",
    "df2 = pd.read_excel(\n",
    "    url,\n",
    "    sheet_name=my_sheets,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6059f491-3966-4343-b000-0830fa3559d6",
   "metadata": {},
   "source": [
    "### Specificity is beautiful.\n",
    "* Grab out each individual sheet into its own dataframe using `df2.get(my_sheets[enter in the number])`. \n",
    "* Make sure your `dataframe` is titled descriptively.\n",
    "* `df` is not exactly very telling. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4c6f8fdb-33d3-4c44-bb00-6d1447d49feb",
   "metadata": {},
   "outputs": [],
   "source": [
    "projects_df = df2.get(my_sheets[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "167af2f1-b09d-476d-87b4-b9374ad445c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "scores_df = df2.get(my_sheets[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd0d51ea-b7da-41d0-bb03-5432b4de1a1b",
   "metadata": {},
   "source": [
    "## Add a new column\n",
    "* Oops! Us analysts were so wrapped up in scoring, we forgot to to total up all the metrics to find the overall_score for the project. \n",
    "* Do so and place your results in a column called `\"overall_score\"`\n",
    "* There are a couple of ways to do this.\n",
    "* More food for thought:\n",
    "    * What does `axis = 1` mean?\n",
    "    * What happens if you do `.sum(axis=0)`?\n",
    "    * Try everything once.\n",
    "    * You don't always have to save everything into a dataframe. You can do something like `df.sum(axis=0)` just to see what happens. \n",
    "        * Just make sure your dataframe isn't too large or else you will run out of memory!\n",
    "    * What happens when you create a new column with `scores_df.overall_score`? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e9321f90-8c99-46fb-9d50-8571f3d94fc8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_8038/1686633536.py:1: FutureWarning: Dropping of nuisance columns in DataFrame reductions (with 'numeric_only=None') is deprecated; in a future version this will raise TypeError.  Select only valid columns before calling the reduction.\n",
      "  scores_df[\"overall_score\"] = scores_df.sum(axis = 1)\n"
     ]
    }
   ],
   "source": [
    "# scores_df[\"overall_score\"] = scores_df.sum(axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "246437eb-f284-49b8-960d-d601a66f6362",
   "metadata": {},
   "source": [
    "## Less is More\n",
    "* Your manager asks for the `project_name` and `overall_score`. \n",
    "* Subset the dataframe and save it into a new dataframe.\n",
    "* There are many ways to do the same thing in Python. \n",
    "    * While this isn't always true, the best way is usually the one with the least amount of text and code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "812207f4-7448-45a9-8d9f-7652aeaa7fd9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['project_name', 'accessibility_score', 'dac_accessibility_score',\n",
       "       'dac_traffic_impacts_score', 'freight_efficiency_score',\n",
       "       'freight_sustainability_score', 'mode_shift_score',\n",
       "       'lu_natural_resources_score', 'safety_score', 'vmt_score', 'zev_score',\n",
       "       'public_engagement_score', 'climate_resilience_score',\n",
       "       'program_fit_score', 'overall_score', 'sample_sentence'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e6d8e70-ae57-46c5-a5aa-9972be77f415",
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_to_keep = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c64cdcf-9598-4f4a-b077-5caec0cfe264",
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_to_drop = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47a96b86-e5d1-4fcd-ba73-7db5badae28b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(columns = columns_to_drop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48ee899b-3db9-464f-802f-d431189176b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[columns_to_keep]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e641185d-295d-4c42-ace1-16d33f2da0fa",
   "metadata": {},
   "source": [
    "## Export to Google Cloud Storage (GCS)\n",
    "* Our original Excel workbook's file path is `\"gs://calitp-analytics-data/data-analyses/starter_kit/starter_kit_csis_scoring_workbook.xlsx\"`\n",
    "* Save your subsetted dataframe from above back into the `starter_kit` folder. \n",
    "* Sure you could do `\"gs://calitp-analytics-data/data-analyses/starter_kit/aggregated_csis.xlsx\"` but that is an eyesore.\n",
    "* Essentially, the only difference between these two file paths are `aggregated_csis.xlsx` and `starter_kit_csis_scoring_workbook.xlsx` because the file_path `gs://calitp-analytics-data/data-analyses/starter_kit/` remains the same. \n",
    "* This is where f-strings come in. What are f-strings? \n",
    "> Python f-strings provide a quick way to interpolate and format strings. They’re readable, concise, and less prone to error than traditional string interpolation and formatting tools...\n",
    "    * Read more about them [here](https://realpython.com/python-f-strings/#f-strings-a-new-and-improved-way-to-format-strings-in-python).\n",
    "* <b> Let's practice </b>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "4c9c53a5-dbf3-4dc0-aea0-832f3a91414d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# My file_path is always going to be `gs://calitp-analytics-data/data-analyses/starter_kit/`.\n",
    "GCS_FILE_PATH = \"gs://calitp-analytics-data/data-analyses/starter_kit/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "db111f34-08b8-42f9-96fe-6852c4af50ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# However my file is going to change.\n",
    "# I want to name my subsetted dataframe as \"aggregated\" and I want it to be saved as an Excel workbook.\n",
    "AGG_FILE = \"aggregated.xlsx\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "edff403c-ef37-48d8-8c7a-60b388752a51",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'gs://calitp-analytics-data/data-analyses/starter_kit/amanda_aggregated'"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Put them together using a f-string\n",
    "f\"{GCS_FILE_PATH}{AGG_FILE}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf37fc2d-ac6c-4134-94de-79a9a4141ffc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# What if I wanted to read back the original file using f-strings? \n",
    "OLD_FILE = \n",
    "f\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17c17adb-404e-4e54-bdb4-c3295e0e2be2",
   "metadata": {},
   "source": [
    "### So many options!\n",
    "* Export using `df.to_parquet()`. We typically use prefer saving to `parquets` and you can read why [here](https://docs.calitp.org/data-infra/analytics_new_analysts/03-data-management.html#parquet).\n",
    "* Export `df.to_excel()`. Open up your new Excel workbook and see if it's what you expect.\n",
    "    * Hint: you will probably get a very annoying extra column! \n",
    "    * Try out some of the arguments [listed](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.to_excel.html#pandas.DataFrame.to_excel)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69d211b4-89f0-4b2c-9093-1118114ba649",
   "metadata": {},
   "source": [
    "## You're almost done!\n",
    "* Name this notebook `YOURNAME_exercise1.ipynb`\n",
    "    * If you need to rename because you already named it, do it within the terminal.\n",
    "    * `git mv OLDNAME.ipynb NEWNAME.ipynb`. \n",
    "    * The `mv` stands for move, and renaming a file is basically \"moving\" its path. Doing it this way retains the git history associated with the notebook. If you rename directly with right click, rename, you destroy the git history.\n",
    "* Use a descriptive commit message (ex: adding chart, etc). GitHub already tracks who makes the commit, the date, the timestamp of it, the files being affected, so your commit message should be more descriptive than the metadata already stored."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
