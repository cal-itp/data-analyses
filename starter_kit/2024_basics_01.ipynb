{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "247e773f-0e29-4ed6-ab4d-5856325611b4",
   "metadata": {},
   "source": [
    "# Exercise 1: `Git`, `pandas`,`python`, `f-strings`, Importing and Exporting data.\n",
    "If you are new to Python, there are many resources to help you! Below is just a small sample of what is available.\n",
    "* There are introductory Python courses available through [Caltrans's LinkedIn Learning Library](https://www.linkedin.com/learning/search?keywords=python&u=36029164).\n",
    "* [Practical Python for Data Science](https://www.practicalpythonfordatascience.com/00_python_crash_course) is an incredibly helpful resource. Material from it is linked throughout.\n",
    "\n",
    "## How to use these tutorials\n",
    "* The tutorials are divided by skills/concepts we are going to learn.\n",
    "* There are hints and instructions on the top.\n",
    "* There are links to references. \n",
    "**It is highly recommended to read through them and practice them in this notebook.**\n",
    "\n",
    "## What are we working with today? \n",
    "* Today we will be working on Caltrans System Investment Strategy (CSIS) data. Per this [description](https://dot.ca.gov/programs/transportation-planning/division-of-transportation-planning/corridor-and-system-planning/csis)\n",
    "> <i>The California Department of Transportation (Caltrans) is committed to leading climate action and advancing social equity in the transportation sector set forth by the California State Transportation Agency (CalSTA) Climate Action Plan for Transportation Infrastructure (CAPTI, 2021)...Caltrans is in a significant leadership role to carry out meaningful measures that advance state’s goals and priorities through the development and implementation of the Caltrans System Investment Strategy (CSIS). The CSIS, which implements one of CAPTI’s key actions, is envisioned to be an investment framework through a data and performance-driven approach that guides transportation investments and decisions.</i>\n",
    "* The Data Science Branch is working on CSIS is by automating the scoring of projects using Python. We score each project based on how well they do on various  metrics such as Zero Emmission Vehicles, Vehicle Miles Traveled Reduction, and more. \n",
    "* While the values in we are working with today are all <i>fake</i>, the exercise is based on the actual data and work we've done. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41f1a8ae-23c0-42ba-8b50-5a1014d75fd0",
   "metadata": {},
   "source": [
    "## GitHub - Making a Branch\n",
    "* You are probably on the `main` branch of our `data-analyses` repo. \n",
    "* The `main` branch is [here](https://github.com/cal-itp/data-analyses).\n",
    "* We never work on the `main` branch. \n",
    "* You can think of the `main` branch as an area that contains our work only when it's at a good stopping point.\n",
    "* We typically save (via `merging` a `pull request`) our work to the `main` branch at the end of the work week.\n",
    "* The rest of the time, we work on our own branches, making frequent `commits` to save and document our work along the way. \n",
    "* Let's make (or rather `check out`) our own branch.\n",
    "\n",
    "**Steps**\n",
    "1. Go to the terminal.\n",
    "2. Paste `git pull origin main` which pulls down the main branch with the latest work. \n",
    "3. Paste `git switch -c your-branch` in the terminal. Swap out `your-branch` with something else.\n",
    "     * We typically name branches with all lowercase, and dashes instead of underscores. Instead of `Amanda_Branch`, write `amanda-branch`.\n",
    "4. Your terminal should now show `jovyan@jupyter-your_name ~/data-analyses (your-branch) $ ` which means you successfully made your new branch!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dd32eed-55a4-4fd1-874b-02f9b4bd94a7",
   "metadata": {},
   "source": [
    "## Import Packages\n",
    "* Before doing some data cleaning and analyzing, we need to equip ourselves with the right tools.\n",
    "* Part of our \"toolbox\" are importing packages. \n",
    "* **Resource**: [Importing Dependencies via Practical Python for Data Science](https://www.practicalpythonfordatascience.com/05_data_exploration.html?highlight=dependencies#importing-our-dependencies)\n",
    "\n",
    "### `Pandas`\n",
    "* Below, you are importing the package `pandas` that is the backbone of our data analysis work. \n",
    "* Other packages DDS commonly uses are `geopandas` for geospatial data work and `altair` for making charts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50199af7-04a8-43c5-ba1b-4127940749bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19b42c5d-4f2b-4d66-a7a7-98ab74a6591e",
   "metadata": {},
   "source": [
    "* This block of code below adjusts the notebook's settings.\n",
    "* I am setting the maximum number of columns to be displayed to be 100 because the default number of columns shown is much smaller.\n",
    "* I want any `float` columns to be rounded to 2 decimal points.\n",
    "* I want all of the rows in the dataframe to display. \n",
    "* I don't want my string columns to be truncated.\n",
    "    * A long string value will display like this <i>The California Department of Transportation (Caltrans) is committed to leading climate action and advancing social equity...</i> would be displayed something like this <i>The California Department of Transportation (Caltrans) is...</i> without this line of code.\n",
    "* Adjust some of these settings if you wish to make this notebook the proper environment for you."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e18d8d7-2cce-4854-b6c4-56a7e7bdf636",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.options.display.max_columns = 100\n",
    "pd.options.display.float_format = \"{:.2f}\".format\n",
    "pd.set_option(\"display.max_rows\", None)\n",
    "pd.set_option(\"display.max_colwidth\", None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f14077a3-2882-46eb-8cd2-27c08e4705a9",
   "metadata": {},
   "source": [
    "### `calitp_data_analysis`\n",
    "* DDS also has our own [internal library of functions](https://docs.calitp.org/data-infra/analytics_tools/python_libraries.html#calitp-data-analysis).\n",
    "    * You can check out all the functions [here](https://github.com/cal-itp/data-infra/tree/main/packages/calitp-data-analysis/calitp_data_analysis).\n",
    "* Below, we are importing only one function called `to_snakecase` from the python submodule `sql` in our package `calitp_data_analysis`. \n",
    "* `to_snakecase` allows us to change the column names of our dataset from something like `Project Description` to `project_description`. \n",
    "    * Turning the column names to lower case and replacing the spaces with underscores, this makes referencing specific columns much easier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd388d88-d2d6-4dd6-9870-22c14db7a44a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from calitp_data_analysis.sql import to_snakecase"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff74b143-6ff2-46e9-ae88-4a208155e990",
   "metadata": {},
   "source": [
    "## Jupyter Notebook\n",
    "* You're using a Jupyter Notebook right now.\n",
    "* There are many benefits of using a notebook for our analysis, which you can read about here in our [DDS Docs](https://docs.calitp.org/data-infra/analytics_new_analysts/04-notebooks.html).\n",
    "* Take some time to get used to this interface. \n",
    "    * Press ctrl+enter to run a cell\n",
    "    * Go up to the Kernel and rerun all the cells.\n",
    "    * Use the scissors at the top to cut out the cell.\n",
    "    * Adjust your settings to be dark instead of light.\n",
    "* There are many tutorials available on Youtube, just skip the installation portion. \n",
    "    * [This one looks promising](https://youtu.be/LW2Rye_l8L0?si=B8kojobCe3OIF3xg)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fbf9a7c-5c72-488c-972a-0c85a237aa3b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "cc30cb7d-77d3-465b-9831-8810096af9b1",
   "metadata": {},
   "source": [
    "## Check out the data \n",
    "* Download the Excel workbook containing all the CSIS data from Google Cloud Storage [here](https://console.cloud.google.com/storage/browser/_details/calitp-analytics-data/data-analyses/starter_kit/starter_kit_csis_scoring_workbook.xlsx;tab=live_object?project=cal-itp-data-infra). \n",
    "* Open the workbook up in Excel and take a look at how many sheets it contains.\n",
    "\n",
    "### Read in the data\n",
    "* We are reading our Excel Workbook into a Pandas dataframe.\n",
    "* While there is a very [technical definition](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.html) of what a dataframe is, you can think of it as an Excel sheet that holds your data. \n",
    "* <b> Resource</b>: [Practical Python for Data Science](https://www.practicalpythonfordatascience.com/02_loading_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5950cb87-75ab-4871-ab4b-a8f1c41f0a4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"gs://calitp-analytics-data/data-analyses/starter_kit/starter_kit_csis_scoring_workbook.xlsx\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88d79cea-c017-454e-a2aa-85c0bf511d85",
   "metadata": {},
   "source": [
    "* Read in the dataframe without the function `to_snakecase()` first to see what happens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67ba9264-65d9-453b-a800-a91bd365e43e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_no_snakecase = (pd.read_excel(url))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2d886b4-c207-41e5-8325-7275619b60e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_no_snakecase.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f959563e-7fa2-444a-b2b3-6c539dce802b",
   "metadata": {},
   "source": [
    "* Read in the dataframe with `to_snakecase()` now and compare the difference between the column names. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e09456e0-dfd2-4388-85de-eb9e95f983fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = to_snakecase(pd.read_excel(url))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54c718b3-eeff-4ec5-b012-1cc612543c60",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "179960a3-6c9b-42af-a8f1-d6156c4be2d2",
   "metadata": {},
   "source": [
    "### Previewing Data \n",
    "* Often, you want to get a sneak preview of your data. \n",
    "* Below are a couple of very common methods we use. \n",
    "    * `.head()` shows the first five rows, while `.tail()` shows the last five.\n",
    "    * `.sample()` shows you a random row.\n",
    "    * Want to see or less than five? Specify it in the parantheses: `.head(10)` allows you to see the first 10 rows and `.sample(2)` allows you to see two random rows.\n",
    "* **Resource**: [Practical Python for Data Science: Data Inspection](https://www.practicalpythonfordatascience.com/02_loading_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e966250-47b1-4f14-802b-c795e44330dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3386e9d8-15cd-48bc-8b1f-cf6f95512ad5",
   "metadata": {},
   "source": [
    "### More Methods!\n",
    "* `df.shape` gives you the number of rows and columns in your dataset.\n",
    "* `df.columns` returns all of the column names.\n",
    "* `df.info()` per the [pandas docs](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.info.html#pandas.DataFrame.info) <i>prints information about a DataFrame including the index dtype and columns, non-null values and memory usage.</i>\n",
    "* **Experiment below.** \n",
    "* More food for thought:\n",
    "    * `Dtype` is critical. There are integers, objects, booleans, floats...\n",
    "    * Does the `dtype` of each column below make sense to you? \n",
    "    * The `dtype` of `object` is a catchall term. It can either contain all string values like \"muffins\" and \"apples\" or a mix of string and other data types like \"6 muffins\" and \"3 apples.\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d117908f-af05-4e95-8042-39a3e0557d6f",
   "metadata": {},
   "source": [
    "### Deeper Dive\n",
    "* Let's take a closer look at some columns.\n",
    "* `.value_counts()` helps you see how many times the same value appears. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63f21ab5-0920-4310-afce-2ea657556912",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.ct_district.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55baf38e-3776-4448-b375-9e124030bae2",
   "metadata": {},
   "source": [
    "* `.nunique()` displays the number of distinct values in your column\n",
    "    * This is  useful because often the number of unique values of a column should match the number of rows of your dataset <b>exactly</b>.\n",
    "    * In our case, our dataframe has 44 rows and we should have 44 unique project names and scope of work descriptions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d832308-a425-404d-83a0-53ce8bfae279",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.project_name.nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55d2140f-feab-496b-b9b1-90bbe5701a9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c0c499e-fa7b-4f01-a357-db7b0ec41416",
   "metadata": {},
   "source": [
    "* You can preview a column with brackets [] as well with the column name encased in quotation marks.\n",
    "* However, simply using a period . is much easier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e232324-f75f-46a0-962d-76ed9273dac7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"scope_of_work\"].nunique()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfbb9b16-b7cd-44d5-af92-6c6351a35022",
   "metadata": {},
   "source": [
    "* Describe() gives you some descriptive statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83536bb4-1939-4ceb-8d62-7aeab1473993",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "497ae835-e7eb-45b8-b593-eaf44cb5858e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.project_cost.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06ee15f6-ee2e-4e3e-91b2-115875292042",
   "metadata": {},
   "source": [
    "## Something missing? \n",
    "* Open up our dataset using Excel. \n",
    "* Take a look at the bottom: how many sheets are there in the Excel worbook? \n",
    "* Which sheet is loaded into `df` above? "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5302dd99-acb2-40d7-b00d-4f0493ee5e09",
   "metadata": {},
   "source": [
    "### Lists: An Introduction\n",
    "* We can load in all of the sheets in an Excel workbook using a list\n",
    "* Per [Practical Python for Data Science](https://www.practicalpythonfordatascience.com/00_python_crash_course_datatypes.html?highlight=dictionary#list): <i>\"lists represent a collection of objects and are constructed with square brackets, separating items with commas. A list can contain a collection of one datatype...It can also contain a collection of mixed datatypes</i>\".\n",
    "    * **Play around with some of the examples in the link above in this notebook.**\n",
    "    * You will be using lists often in your work, so it is best to be familiar with this datatype."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7f41842-853e-4ad0-ae9e-0da0955d4352",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "21a32ab4-bfb2-4e7a-b90a-6fa05b7ceb89",
   "metadata": {},
   "source": [
    "### Application of Lists\n",
    "* I am placing all of the sheets in our Excel Workbook in a list.\n",
    "* Notice that the items in this list are <i>strings</i>. \n",
    "    * Read about strings [here](https://www.practicalpythonfordatascience.com/00_python_crash_course_datatypes.html?highlight=dictionary#string).\n",
    "* You can access each element of the list using an index.\n",
    "    * An index represents the location of an element with a number.\n",
    "    * The index always starts at 0. What we consider the first item is not index \"1\", it's index \"0\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02380fb6-c55b-477f-acfb-8b483e83beac",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_sheets = [\"projects_auto\",\n",
    "            \"overall_score\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a9a1a3e-e10d-4447-96dd-92ecb2fe6357",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(my_sheets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3be037d-b21b-4192-9099-25bfcb660f01",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Index 0 is projects_auto\n",
    "my_sheets[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75df89d0-92fb-4e4e-aaa3-54f4944c55c3",
   "metadata": {},
   "source": [
    "* Read the in the Excel workbook into a dataframe.\n",
    "* Using the argument `sheet_name` you can open up a specific sheet in an Excel workbook or multiple sheets that is held in a list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e2578bc-db1f-41f5-bc07-3cb82998420e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = pd.read_excel(\n",
    "    url,\n",
    "    sheet_name=my_sheets,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6059f491-3966-4343-b000-0830fa3559d6",
   "metadata": {},
   "source": [
    "### Specificity is beautiful.\n",
    "* Grab out each individual sheet into its own dataframe using `df2.get(my_sheets[enter in the index number])`. \n",
    "* Make sure your `dataframe` is titled descriptively.\n",
    "* `df` is not exactly very telling. \n",
    "* Use the function `to_snakecase` to clean up your column names"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd0d51ea-b7da-41d0-bb03-5432b4de1a1b",
   "metadata": {},
   "source": [
    "## Add a new column\n",
    "* Oops! Us analysts were so wrapped up in scoring, we forgot to to total up all the metrics to find the overall_score for the project. \n",
    "* Using the dataframe you read in from the Excel sheet \"Overall Score\", sum up all the metric columns into a column called `overall_score`\n",
    "* There are a couple of ways to do this: experiment! \n",
    "* Here are some resources:\n",
    "    * [Stackoverflow](https://stackoverflow.com/questions/22342285/summing-two-columns-in-a-pandas-dataframe)\n",
    "    * [Statology](https://www.statology.org/pandas-sum-specific-columns/)\n",
    "* Food for thought:\n",
    "    * What happens when you create a new column with `scores_df.overall_score` instead of `scores_df[\"overall_score\"]`? \n",
    "    * What does `axis = 1` mean?\n",
    "    * What happens if you do `.sum(axis=0)`?\n",
    "    * You don't always have to save everything into a dataframe. You can do something like `df.sum(axis=0)` just to see what happens. \n",
    "        * Just make sure your dataframe isn't too large or else you will run out of memory!\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93085a1c-d479-424d-b2a1-d8e6cba150ab",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "246437eb-f284-49b8-960d-d601a66f6362",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Subsetting\n",
    "* Your manager asks for the `overall_score` for each project in Excel format. \n",
    "* They do not want to see the other metrics, only the project's name and its `overall_score`\n",
    "* Subset the dataframe and saveit into a new dataframe.\n",
    "* <b>Method 1:</b> Enter in all the columns you want to keep in a list and place the list in another set of brackets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e6d8e70-ae57-46c5-a5aa-9972be77f415",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enter in the columns you want to keep\n",
    "columns_to_keep = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48ee899b-3db9-464f-802f-d431189176b7",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "subsetted_df1 = scores_df[columns_to_keep]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56865911-994c-4fb5-afe4-1fdc1d752d8b",
   "metadata": {},
   "source": [
    "* <b>Method 2</b>: You can enter in all the columns in a list you want to drop and use `.drop()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c64cdcf-9598-4f4a-b077-5caec0cfe264",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enter in the columns you want to drop\n",
    "columns_to_drop = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47a96b86-e5d1-4fcd-ba73-7db5badae28b",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "subsetted_df2 = scores_df.drop(columns = columns_to_drop)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e641185d-295d-4c42-ace1-16d33f2da0fa",
   "metadata": {},
   "source": [
    "## F-Strings\n",
    "* Save your <b>subsetted dataframe</b> from above back into the `starter_kit` folder. \n",
    "* The file path should be something like this `\"gs://calitp-analytics-data/data-analyses/starter_kit/your_file_name_here.xlsx\"`.\n",
    "* However, remember our original Excel workbook's file path? It was`\"gs://calitp-analytics-data/data-analyses/starter_kit/starter_kit_csis_scoring_workbook.xlsx\"`\n",
    "* The **only** difference between these two file paths are `your_file_name_here.xlsx` and `starter_kit_csis_scoring_workbook.xlsx` because the folder path `gs://calitp-analytics-data/data-analyses/starter_kit/` remains the same. \n",
    "* This is where f-strings come in. \n",
    "> Python f-strings provide a quick way to interpolate and format strings. They’re readable, concise, and less prone to error than traditional string interpolation and formatting tools...\n",
    "    * Excerpt from [here](https://realpython.com/python-f-strings/#f-strings-a-new-and-improved-way-to-format-strings-in-python).\n",
    "#### Application of F-Strings\n",
    "* My file_path is always going to be `gs://calitp-analytics-data/data-analyses/starter_kit/` so I'll set that in its own variable.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c9c53a5-dbf3-4dc0-aea0-832f3a91414d",
   "metadata": {},
   "outputs": [],
   "source": [
    "GCS_FILE_PATH = \"gs://calitp-analytics-data/data-analyses/starter_kit/\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11a088a5-e8e2-4a12-9736-44ae46c2d771",
   "metadata": {},
   "source": [
    "* However the file is going to change.\n",
    "* Save the file name in a new variable called `FILE`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db111f34-08b8-42f9-96fe-6852c4af50ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "FILE = "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf96d0cf-7225-4a44-9955-988d982a0f7f",
   "metadata": {},
   "source": [
    "* Using a `f-string`, combine `GCS_FILE_PATH` and `FILE` together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edff403c-ef37-48d8-8c7a-60b388752a51",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Put them together using a f-string\n",
    "f\"{GCS_FILE_PATH}{FILE}\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5504c416-b65b-4c74-a2ba-95688cf8e77a",
   "metadata": {},
   "source": [
    "* Now go open up your new Excel workbook and see if it's what you expect.\n",
    "    * Hint: you will probably get a very annoying extra column! \n",
    "    * Try out some of the arguments [here](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.to_excel.html#pandas.DataFrame.to_excel) and save your file again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf37fc2d-ac6c-4134-94de-79a9a4141ffc",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "scores_df[[\"project_name\",\"overall_score\"]].to_excel(f\"{GCS_FILE_PATH}{FILE}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17c17adb-404e-4e54-bdb4-c3295e0e2be2",
   "metadata": {},
   "source": [
    "### Parquets\n",
    "* Export the entire (not subsetted) dataframe with the new `overall_score` column using `df.to_parquet()`. \n",
    "    * We typically  prefer saving to `parquets`. Why? Read below. Text taken from [here](https://docs.calitp.org/data-infra/analytics_new_analysts/03-data-management.html#parquet).\n",
    "    * <i>Parquet is an “open source columnar storage format for use in data analysis systems.” Columnar storage is more efficient as it is easily compressed and the data is more homogenous. CSV files utilize a row-based storage format which is harder to compress, a reason why Parquets files are preferable for larger datasets. Parquet files are faster to read than CSVs, as they have a higher querying speed and preserve datatypes (i.e. Number, Timestamps, Points). They are best for intermediate data storage and large datasets (1GB+) on most any on-disk storage. This file format is also good for passing dataframes between Python and R. A similar option is feather.</i>\n",
    "* <b> Reference</b>\n",
    "    *  [DDS Docs: Saving Code](https://docs.calitp.org/data-infra/analytics_tools/saving_code.html)\n",
    "* Make sure you use a f-string."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69d211b4-89f0-4b2c-9093-1118114ba649",
   "metadata": {},
   "source": [
    "## Git - `Committing` Code\n",
    "* In the terminal, paste `git mv 2024_basics_01.ipynb your_new_notebook.ipynb`. \n",
    "    * This renames your notebook.\n",
    "    * You can't right click and rename the file, since this notebook is tracked with Git. \n",
    "    * The `mv` stands for move, and renaming a file is basically \"moving\" its path. \n",
    "    *  If you rename directly with right click, rename, you destroy the git history.\n",
    "    * Doing it this way retains the git history associated with the notebook.\n",
    "* In the terminal, paste `your_new_notebook.ipynb`. \n",
    "    * This adds your new notebook.\n",
    "    * To add all files with a certain extension, write `git add *ipynb`.\n",
    "* Continuing in the terminal, paste `git commit -m 'write a message here'`\n",
    "    * This details the work you did this particular coding session. \n",
    "    * A typical message would be: `git commit -m 'added charts'` or `git commit -m 'worked on exercise 1'`\n",
    "    * GitHub already tracks the change's date and timestamp, the files being affected, who made the change, and more so you don't need to include details like these details.\n",
    "* Finally, in the terminal, paste `git push origin your_branch`.\n",
    "    * This pushes up your change to the remote `data-analyses` repo onto your own branch.\n",
    "    * Now, all your work is safely stored on and recorded by GitHub."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
