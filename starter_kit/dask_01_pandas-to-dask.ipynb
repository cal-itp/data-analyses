{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "df45f43a-94de-421b-b0df-54158d0196ef",
   "metadata": {},
   "source": [
    "# Dask Exercise 1: switching from `pandas`\n",
    "\n",
    "**Dask intro**\n",
    "* [Why Dask?](https://docs.dask.org/en/stable/why.html)\n",
    "* [10 min to Dask](https://docs.dask.org/en/stable/10-minutes-to-dask.html)\n",
    "* [Dask dataframe](https://docs.dask.org/en/stable/dataframe.html) -- scope of what is easily ported from `pandas` vs what might be slow / not implemented in Dask.\n",
    "* [Dask and parquets](https://docs.dask.org/en/stable/dataframe-parquet.html)\n",
    "\n",
    "The first and easiest step in starting to use `dask` is to make the switch from `pandas` dataframes (dfs) to `dask` dataframes (ddfs) and find the equivalent methods. The look and feel of this should be very familiar.\n",
    "\n",
    "The **major** difference between `pandas` dfs and `dask` ddfs is that ddfs are not read into memory. The schema and certain attributes of the df are there, but to actually get computations, you have call `.compute()`. Dask uses a lazy evaluation, which means it's storing the steps and the order you want to do it in, and evaluating it all at once when you say you want it.\n",
    "\n",
    "The `dask` equivalent for Python's `numpy` arrays is `dask arrays`. \n",
    "\n",
    "Skills:\n",
    "* equivalent methods for dask dataframes\n",
    "* task graphs\n",
    "* concatenation\n",
    "* merges\n",
    "* partitioned parquets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6284954c-5581-4498-989e-f047bd2f02a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import dask.dataframe as dd\n",
    "import dask_geopandas as dg\n",
    "import geopandas as gpd\n",
    "import pandas as pd\n",
    "\n",
    "TAXI_DATA = (\"https://raw.githubusercontent.com/mwaskom/\"\n",
    "             \"seaborn-data/master/taxis.csv\"\n",
    "            )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dd7279d-83e2-46c2-af45-458cea9d6a91",
   "metadata": {},
   "source": [
    "### Basics\n",
    "\n",
    "Most `pandas` methods can be called, but you need to add a `.compute()`.\n",
    "\n",
    "To convert a ddf to a df, simply use `ddf.compute()`. \n",
    "\n",
    "Alternatively, you can change any df to a ddf."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "511ab8cc-3790-4021-a763-5b819fb1a824",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(TAXI_DATA)\n",
    "\n",
    "ddf = dd.from_pandas(df, npartitions=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e819e1a-a839-47cd-8255-75bbbefa3ca9",
   "metadata": {},
   "outputs": [],
   "source": [
    "ddf = dd.read_csv(TAXI_DATA)\n",
    "ddf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a591cce-2318-486a-a4d4-5ba157b5fcce",
   "metadata": {},
   "outputs": [],
   "source": [
    "ddf.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60a06af4-0477-4381-b3de-8d96268b7569",
   "metadata": {},
   "outputs": [],
   "source": [
    "ddf.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5da83f9f-2e33-4e1e-bf0b-d90350b6c53a",
   "metadata": {},
   "outputs": [],
   "source": [
    "ddf.describe().compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c6a990e-e485-464d-b5a8-764cc469daf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.shape or len(df) would not work\n",
    "# to find how many rows there are, find the length of the index\n",
    "len(ddf.index)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c9d2318-d659-4119-9956-c98b0978f21a",
   "metadata": {},
   "source": [
    "### Cleaning Columns / Apply Row-Wise Functions  \n",
    "\n",
    "For the most part, this is intact. Sometimes there is a `dask` equivalent of the `pandas` methods. Always look to see if there is one first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5ae0144-a8bd-49b8-b030-140e64949c6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lambda functions in pandas\n",
    "df = df.assign(\n",
    "    same_borough = df.apply(\n",
    "        lambda x: \n",
    "        1 if x.pickup_borough == x.dropoff_borough\n",
    "        else 0, axis=1)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01e2187e-6358-4fdf-b3e7-d40f29191ae2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the same lambda function for dask df\n",
    "# Make sure to add the metadata argument to specify the data type\n",
    "ddf = ddf.assign(\n",
    "    same_borough = ddf.apply(\n",
    "        lambda x:\n",
    "        1 if x.pickup_borough == x.dropoff_borough\n",
    "        else 0, axis=1, meta=('same_borough', 'int')\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8311206e-5e85-4463-81b9-e6cc0dc75eee",
   "metadata": {},
   "outputs": [],
   "source": [
    "ddf.same_borough.value_counts().compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eff92765-e5ca-4623-9078-1cccdfcb069b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def type_of_trip(row) -> str: \n",
    "    if ((row.passengers == 1) and \n",
    "        (row.distance >= 5) and \n",
    "        (row.pickup_borough != row.dropoff_borough)\n",
    "       ):\n",
    "        return \"individual_long_trips\"\n",
    "    elif ((row.passengers > 1) and \n",
    "          (row.distance >= 5) and \n",
    "          (row.pickup_borough != row.dropoff_borough)\n",
    "    ):\n",
    "        return \"group_long_trips\"\n",
    "    \n",
    "    else:\n",
    "        return \"short_trips\"\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de8b492f-6dc6-4855-a372-0a3e6a62c660",
   "metadata": {},
   "outputs": [],
   "source": [
    "ddf = ddf.assign(\n",
    "    trip_type = ddf.apply(\n",
    "        lambda x: type_of_trip(x), \n",
    "        axis=1, meta=(\"trip_type\", \"str\"))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff2d1672-0d1e-4b0d-8b6e-f9d34ff9d595",
   "metadata": {},
   "outputs": [],
   "source": [
    "ddf.groupby(\"trip_type\").agg(\n",
    "    {\"total\": \"sum\", \n",
    "     \"passengers\": \"sum\"}).reset_index().compute()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac1042b6-e02d-4028-b676-2f180621e1bb",
   "metadata": {},
   "source": [
    "### DateTimes\n",
    "\n",
    "A lot of the methods are similar here!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03eeb72e-9266-4d1c-9d35-7f3e9d93163e",
   "metadata": {},
   "outputs": [],
   "source": [
    "ddf = ddf.assign(\n",
    "    pickup_time = dd.to_datetime(ddf.pickup),\n",
    "    pickup_hour = dd.to_datetime(ddf.pickup).dt.hour\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad1ba0ff-8f97-472a-ac4f-db8e1d990ccc",
   "metadata": {},
   "outputs": [],
   "source": [
    "ddf.pickup_hour.value_counts().compute()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce060ea6-6ea6-4c37-9f53-f831e6ea9c6b",
   "metadata": {},
   "source": [
    "## Task Graphs\n",
    "\n",
    "Since Dask is lazily evaluated, it's really just storing the order of operations for you. To see all the transformations you're doing to the dataframe, look at the task graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "976c771a-1c7f-4a57-a6d8-1dd5e3929f14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split up the ddf into multiple partitions\n",
    "ddf2 = ddf.repartition(npartitions=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dce51709-534b-47cc-a5a0-69e50e9a061b",
   "metadata": {},
   "outputs": [],
   "source": [
    "ddf2.visualize()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ae7849d-44e6-49be-a5a5-4ad58bc5c325",
   "metadata": {},
   "source": [
    "## Concatenation\n",
    "\n",
    "Instead of `pd.concat`, use `dd.multi.concat`. [Docs](https://docs.dask.org/en/stable/generated/dask.dataframe.multi.concat.html).\n",
    "\n",
    "The fact that we are concatenating a list of dfs or ddfs is a very useful concept to use in `dask.delayed`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da667fac-388e-4783-81c0-a4291d1fd0db",
   "metadata": {},
   "outputs": [],
   "source": [
    "yellow = df[df.color==\"yellow\"].reset_index(drop=True)\n",
    "green = df[df.color==\"green\"].reset_index(drop=True)\n",
    "\n",
    "yellow_ddf = dd.from_pandas(yellow, npartitions=1)\n",
    "green_ddf = dd.from_pandas(green, npartitions=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3da5c638-e605-4fca-87a2-15951fe26397",
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_df = pd.concat([yellow, green], axis=0)\n",
    "combined_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1120e512-a46a-4d8b-8af4-4fa3032c2485",
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_ddf = dd.multi.concat([yellow_ddf, green_ddf], axis=0)\n",
    "len(combined_ddf.index)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0df2114d-333a-4d88-be09-b031fa777563",
   "metadata": {},
   "source": [
    "## Merges\n",
    "\n",
    "This isn't a very meaningful merge, but we'll use it to demonstrate anyway.\n",
    "\n",
    "Let's say that there's a column called `manhattan_flag` in `yellow_ddf` and we want to bring that column in for `green_ddf`. \n",
    "\n",
    "We cannot use the `validate` parameter in the merge, but most of the other arguments are present. [Docs](https://docs.dask.org/en/stable/generated/dask.dataframe.DataFrame.merge.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60d460e0-8c18-4a7e-a62f-11ed026db48e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Not that this is a meaningful merge, but, we can!\n",
    "yellow_ddf = yellow_ddf.assign(\n",
    "    manhattan_flag = yellow_ddf.apply(\n",
    "        lambda x: \n",
    "        1 if (x.pickup_borough == \"Manhattan\") or \n",
    "        (x.dropoff_borough==\"Manhattan\") \n",
    "         else 0, axis=1, meta=(\"manhattan_flag\", \"int\")\n",
    "    )\n",
    ")\n",
    "\n",
    "yellow_ddf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6327cda7-cb98-445d-b6e1-f0a11e4b8b55",
   "metadata": {},
   "outputs": [],
   "source": [
    "yellow_ddf_flag = yellow_ddf[[\"pickup_borough\", \"dropoff_borough\", \n",
    "                              \"manhattan_flag\"]].drop_duplicates()\n",
    "\n",
    "yellow_ddf_flag.visualize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "012b7541-36aa-478a-ae9b-a2394df6d37b",
   "metadata": {},
   "outputs": [],
   "source": [
    "m1 = dd.merge(\n",
    "    green_ddf,\n",
    "    yellow_ddf_flag,\n",
    "    on = [\"pickup_borough\", \"dropoff_borough\"],\n",
    "    how = \"inner\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43dd765f-af41-4606-9161-f514f37e6f1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "m1.manhattan_flag.value_counts().compute()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10711abb-18e6-4890-8958-6719b425ed2a",
   "metadata": {},
   "source": [
    "## Partitioned Parquets\n",
    "\n",
    "We already use parquets because it's a lot faster than csv or geojson. We can also use partitioned parquets (a folder of lots of smaller parquet files).\n",
    "\n",
    "The folder of partitioned parquets can be easily read back in or filtered against.\n",
    "\n",
    "If you find that the `.compute()` step in bringing a very large ddf into memory is holding you back, consider exporting it out as a partitioned parquet. Reading a partitioned parquet back in and exporting as a single parquet is faster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6187d41-d168-4bf8-95e9-43c3153bf493",
   "metadata": {},
   "outputs": [],
   "source": [
    "type(ddf.compute())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58afbe4d-97cf-484c-90ec-77878f0d8cfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "type(ddf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "616f44cb-0f2c-4f31-ae0d-da2435b1511f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We don't have trouble computing and bringing this back into memory\n",
    "ddf.compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "552e5d04-3c36-4a2d-8363-c551d180620a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Look at this task graph\n",
    "ddf3 = dd.read_csv(TAXI_DATA).repartition(npartitions=3)\n",
    "ddf3 = ddf3.assign(\n",
    "    trip_type = ddf3.apply(\n",
    "        lambda x: type_of_trip(x), \n",
    "        axis=1, meta=(\"trip_type\", \"str\"))\n",
    ")\n",
    "\n",
    "ddf3.visualize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91d6c146-62dc-4aee-88e0-5e7e4c36ee1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "ddf3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50348562-6ed4-49bf-92f8-07b3bcc24f2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# look at the format of this partitioned parquet\n",
    "ddf3.to_parquet(\"dask1_multipart\", overwrite=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "835e4614-aed8-45ba-ae7e-33e674afcdad",
   "metadata": {},
   "outputs": [],
   "source": [
    "read_in_ddf3 = dd.read_parquet(\"dask1_multipart/\")\n",
    "read_in_ddf3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bf9bee3-0ea2-408f-990c-8e673fa9cdd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# task graph just reads in each part\n",
    "read_in_ddf3.visualize()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
