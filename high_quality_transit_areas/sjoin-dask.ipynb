{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e0392646-c15d-44d9-a99b-ab91da56e8a4",
   "metadata": {},
   "source": [
    "# Speed up spatial join"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bee712cf-3cfb-42fc-88af-0f8e0b9cd33c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/geopandas/_compat.py:123: UserWarning: The Shapely GEOS version (3.10.3-CAPI-1.16.1) is incompatible with the GEOS version PyGEOS was compiled with (3.10.1-CAPI-1.16.0). Conversions between both will be slow.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import dask.dataframe as dd\n",
    "import dask_geopandas as dg\n",
    "import geopandas as gpd\n",
    "import pandas as pd\n",
    "\n",
    "import calitp.magics\n",
    "\n",
    "DASK_GCS = \"gs://calitp-analytics-data/data-analyses/dask_test/\"\n",
    "ALL_BUS = f\"{DASK_GCS}all_bus.parquet\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f79354cc-5621-4483-8ad6-48c1d6277c66",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sjoin_keep_orthogonal(left_df: dg.GeoDataFrame, \n",
    "                          right_df: dg.GeoDataFrame) -> dd.DataFrame:\n",
    "    route_cols = [\"hqta_segment_id\", \"route_direction\"]\n",
    "\n",
    "    s1 = dg.sjoin(\n",
    "        left_df[route_cols + [\"geometry\"]], \n",
    "        right_df[route_cols  + [\"geometry\"]],\n",
    "        how = \"inner\",\n",
    "        predicate = \"intersects\"\n",
    "    ).drop(columns = [\"index_right\", \"geometry\"])\n",
    "\n",
    "    # Only allow orthogonal!\n",
    "    s2 = (s1[s1.route_direction_left != s1.route_direction_right]\n",
    "                   [[\"hqta_segment_id_left\", \"hqta_segment_id_right\"]]\n",
    "                  )\n",
    "    route_pairs = (s2.rename(columns = {\n",
    "                    \"hqta_segment_id_left\": \"hqta_segment_id\", \n",
    "                    \"hqta_segment_id_right\": \"intersect_hqta_segment_id\"})\n",
    "          .drop_duplicates()\n",
    "          .reset_index(drop=True)\n",
    "         )\n",
    "\n",
    "    return route_pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "92806713-ef7b-46b4-b86f-98a700cd9c90",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compile_within_operator_intersections(\n",
    "    operator_df: dg.GeoDataFrame) -> dd.DataFrame:\n",
    "    \"\"\"\n",
    "    Grab one operator's routes, and do sjoin against the other routes \n",
    "    of same operator.\n",
    "    Look WITHIN operators.\n",
    "    \n",
    "    Concatenate all the small dask dfs into 1 dask df by the end.\n",
    "    \n",
    "    https://stackoverflow.com/questions/56072129/scale-and-concatenate-pandas-dataframe-into-a-dask-dataframe\n",
    "    \"\"\"\n",
    "    results = []\n",
    "\n",
    "    operator_routes = operator_df.route_identifier.unique()\n",
    "    \n",
    "    for r in operator_routes:\n",
    "        this_route = operator_df[operator_df.route_identifier == r]\n",
    "        other_routes = operator_df[operator_df.route_identifier != r]\n",
    "\n",
    "        results.append(sjoin_keep_orthogonal(\n",
    "            this_route, other_routes)) \n",
    "\n",
    "    # Concatenate all the dask dfs in the list and get it into one dask df\n",
    "    ddf = dd.multi.concat(results, axis=0).drop_duplicates()\n",
    "\n",
    "    return ddf  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c01a31aa-b7e0-497f-8808-65c513dda909",
   "metadata": {},
   "source": [
    "### Set up dataset\n",
    "* Keep just 5 LA area transit agencies.\n",
    "* Repartition it, do it in a way where multiple transit operator are in the same partition.\n",
    "\n",
    "\n",
    "#### Method 1: Loop, but save results in a list and unpack at the same time\n",
    "\n",
    "Ex: https://stackoverflow.com/questions/56072129/scale-and-concatenate-pandas-dataframe-into-a-dask-dataframe\n",
    "\n",
    "\n",
    "#### Method 2: try to use map partitions\n",
    "If I set the index to be the transit operator, but expect multiple transit operators in the same partition, how can I make sure the spatial join is only operating only within an operator?\n",
    "\n",
    "Check within the partition to see what operators are present, and introduce the loop there?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6fa9edd0-78b6-4584-9702-d5bebc161da4",
   "metadata": {},
   "outputs": [],
   "source": [
    "bus_segments = dg.read_parquet(ALL_BUS)\n",
    "\n",
    "keep_cols = [\"calitp_itp_id\", \"route_identifier\", \"route_direction\",\n",
    "             \"hqta_segment_id\", \"geometry\"]\n",
    "\n",
    "test_ids = [\n",
    "    182, # LA Metro\n",
    "    183, # DASH\n",
    "    300, # Big Blue Bus \n",
    "    17, # Arcadia transit\n",
    "    45, # Burbank Bus\n",
    "]\n",
    "\n",
    "gdf = (bus_segments[(bus_segments.hq_transit_corr==True) & \n",
    "                    (bus_segments.calitp_itp_id.isin(test_ids))]\n",
    "       [keep_cols].reset_index(drop=True)\n",
    "      )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70da1c25-b100-44a8-8e90-8472d8ca01cb",
   "metadata": {},
   "source": [
    "### Method 1: loop and append results in list, concat and compute at the end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "27e40fae-03be-4693-ac6c-b7d6fa0b5414",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1min 39s ± 1.26 s per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "gdf1 = gdf.repartition(npartitions=3)\n",
    "\n",
    "results = []\n",
    "\n",
    "IDS_TO_LOOP = gdf1.calitp_itp_id.unique()\n",
    "\n",
    "for itp_id in IDS_TO_LOOP:\n",
    "    operator_results = compile_within_operator_intersections(\n",
    "        gdf1[gdf1.calitp_itp_id==itp_id]) \n",
    "    results.append(operator_results)\n",
    "    \n",
    "    \n",
    "full = dd.multi.concat(results, axis=0).drop_duplicates()\n",
    "full.compute()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8321181f-017e-4a11-91d6-dfee6658a152",
   "metadata": {},
   "source": [
    "### Method 2: map_partitions\n",
    "\n",
    "Rewrite the function to account for multiple operators within a partition?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2207c08a-47c0-45ff-8619-2c56329ed0d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# use map_partitions\n",
    "gdf2 = gdf.set_index(\"calitp_itp_id\").repartition(npartitions=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "86dcd0e8-846f-45b4-bd36-038b8c87a108",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compile_within_operator_intersections_partitioned(\n",
    "    operator_df: dg.GeoDataFrame) -> dd.DataFrame:\n",
    "    \"\"\"\n",
    "    Grab one operator's routes, and do sjoin against the other routes \n",
    "    of same operator.\n",
    "    Look WITHIN operators.\n",
    "    \n",
    "    Concatenate all the small dask dfs into 1 dask df by the end.\n",
    "    \n",
    "    https://stackoverflow.com/questions/56072129/scale-and-concatenate-pandas-dataframe-into-a-dask-dataframe\n",
    "    \"\"\"\n",
    "    results = []\n",
    "    \n",
    "    these_operators_present = operator_df.index.unique()\n",
    "    \n",
    "    for itp_id in these_operators_present:\n",
    "        one_operator_df = operator_df.loc[itp_id]\n",
    "        operator_routes = one_operator_df.route_identifier.unique()\n",
    "    \n",
    "        for r in operator_routes:\n",
    "            this_route = one_operator_df[one_operator_df.route_identifier == r]\n",
    "            other_routes = one_operator_df[one_operator_df.route_identifier != r]\n",
    "\n",
    "        results.append(sjoin_keep_orthogonal(\n",
    "            this_route, other_routes)) \n",
    "\n",
    "    # Concatenate all the dask dfs in the list and get it into one dask df\n",
    "    ddf = dd.multi.concat(results, axis=0).drop_duplicates()\n",
    "\n",
    "    return ddf.persist() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "adeb6434-ae0b-44e6-9533-14bbc00aa5ae",
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [8], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m partition_full_results \u001b[38;5;241m=\u001b[39m \u001b[43mgdf2\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmap_partitions\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompile_within_operator_intersections_partitioned\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgdf2\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m                    \u001b[49m\u001b[43mmeta\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mhqta_segment_id\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mint\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m                           \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mintersect_hqta_segment_id\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mint\u001b[39;49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/dask/dataframe/core.py:867\u001b[0m, in \u001b[0;36m_Frame.map_partitions\u001b[0;34m(self, func, *args, **kwargs)\u001b[0m\n\u001b[1;32m    740\u001b[0m \u001b[38;5;129m@insert_meta_param_description\u001b[39m(pad\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m12\u001b[39m)\n\u001b[1;32m    741\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mmap_partitions\u001b[39m(\u001b[38;5;28mself\u001b[39m, func, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    742\u001b[0m     \u001b[38;5;124;03m\"\"\"Apply Python function on each DataFrame partition.\u001b[39;00m\n\u001b[1;32m    743\u001b[0m \n\u001b[1;32m    744\u001b[0m \u001b[38;5;124;03m    Note that the index and divisions are assumed to remain unchanged.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    865\u001b[0m \u001b[38;5;124;03m    None as the division.\u001b[39;00m\n\u001b[1;32m    866\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 867\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmap_partitions\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/dask/dataframe/core.py:6497\u001b[0m, in \u001b[0;36mmap_partitions\u001b[0;34m(func, meta, enforce_metadata, transform_divisions, align_dataframes, *args, **kwargs)\u001b[0m\n\u001b[1;32m   6494\u001b[0m name \u001b[38;5;241m=\u001b[39m kwargs\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtoken\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m   6495\u001b[0m parent_meta \u001b[38;5;241m=\u001b[39m kwargs\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparent_meta\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[0;32m-> 6497\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m callable(func)\n\u001b[1;32m   6498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   6499\u001b[0m     token \u001b[38;5;241m=\u001b[39m tokenize(meta, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "\u001b[0;31mAssertionError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "partition_full_results = gdf2.map_partitions(\n",
    "    compile_within_operator_intersections_partitioned(gdf2),\n",
    "                    meta = {\"hqta_segment_id\": int,\n",
    "                           \"intersect_hqta_segment_id\": int})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87440eaf-38fb-4aa6-8ab6-fc400484c914",
   "metadata": {},
   "outputs": [],
   "source": [
    "partition_full_results.compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1412f985-3475-4710-b676-a401a582f012",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
