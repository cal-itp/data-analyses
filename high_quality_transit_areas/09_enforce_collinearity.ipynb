{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38262382-8e2c-4eb9-86ce-aa4aa251d6d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CALITP_BQ_MAX_BYTES\"] = str(1_000_000_000_000) ## 1TB?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bbd8694-444f-48c0-89a0-00c5c0e542c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import geopandas as gpd\n",
    "import pandas as pd\n",
    "from siuba import *\n",
    "import numpy as np\n",
    "\n",
    "from segment_speed_utils import helpers, gtfs_schedule_wrangling\n",
    "from shared_utils import rt_dates, gtfs_utils_v2\n",
    "import folium\n",
    "import itertools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "994a67bc-1cbe-4477-ac3b-cbe0f7e4822e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from update_vars import (analysis_date, AM_PEAK, PM_PEAK, EXPORT_PATH, GCS_FILE_PATH, PROJECT_CRS,\n",
    "SEGMENT_BUFFER_METERS, AM_PEAK, PM_PEAK, HQ_TRANSIT_THRESHOLD, MS_TRANSIT_THRESHOLD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4706f9e9-03d9-49d7-9b3a-d5044018127e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import create_aggregate_stop_frequencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f494bb98-09d9-472c-882c-158c35d391f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib\n",
    "importlib.reload(create_aggregate_stop_frequencies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd373f3b-6a3a-40c4-8f59-52ad745ae1d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "am_peak_hrs = list(range(AM_PEAK[0].hour, AM_PEAK[1].hour))\n",
    "pm_peak_hrs = list(range(PM_PEAK[0].hour, PM_PEAK[1].hour))\n",
    "both_peaks_hrs = am_peak_hrs + pm_peak_hrs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd40c5c1-6d79-4bb0-87bd-18c9053af24b",
   "metadata": {},
   "outputs": [],
   "source": [
    "analysis_date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe9470ff-40a3-40d0-aaed-913784f105ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_times = helpers.import_scheduled_stop_times(\n",
    "    analysis_date,\n",
    "    get_pandas = True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3263a32-1140-41bf-8181-e07be8a4cc5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_times = create_aggregate_stop_frequencies.add_route_dir(stop_times, analysis_date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55413cd4-bf8e-4313-8f17-a80a7b4806dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "st_prepped = stop_times.pipe(create_aggregate_stop_frequencies.prep_stop_times)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54fd1d50-74a2-403a-b8a7-9d018d9926d6",
   "metadata": {
    "tags": []
   },
   "source": [
    "## multi logic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5e95afa-9078-4f4f-a7d3-f4a6dad36cac",
   "metadata": {},
   "outputs": [],
   "source": [
    "multi_test2 = create_aggregate_stop_frequencies.stop_times_aggregation_max_by_stop(st_prepped, analysis_date, single_route_dir=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff21037c-95e1-46f7-a1f7-e77c6b9e3f30",
   "metadata": {
    "tags": []
   },
   "source": [
    "## single logic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69bd9d0f-997d-4a04-a12b-a63750d5a1b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "single_test2 = create_aggregate_stop_frequencies.stop_times_aggregation_max_by_stop(st_prepped, analysis_date, single_route_dir=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69a031d0-c805-4ef1-b558-7e52749c9b95",
   "metadata": {},
   "source": [
    "## create count of shared stops between each route_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad2fa6a1-5447-4c87-a3c8-6354ac47b480",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_explode_multiroute_only(\n",
    "    single_route_aggregation: pd.DataFrame,\n",
    "    multi_route_aggregation: pd.DataFrame,\n",
    "    frequency_thresholds: tuple,\n",
    ") -> pd.DataFrame:\n",
    "    '''\n",
    "    Shrink the problem space for the compute-intensive collinearity screen.\n",
    "    First, get stops with any chance of qualifying as a major stop/hq corr for\n",
    "    both single and multi-route aggregations.\n",
    "    Then get stops that appear in multi-route qualifiers only, these will go to\n",
    "    further processing.\n",
    "    '''\n",
    "    #  note this is max -- still evaluate stops meeting the lower threshold as single-route in case they meet the higher threshold as multi\n",
    "    single_qual = single_route_aggregation >> filter(_.am_max_trips_hr >= max(frequency_thresholds), _.pm_max_trips_hr >= max(frequency_thresholds))\n",
    "    multi_qual = multi_route_aggregation >> filter(_.am_max_trips_hr >= min(frequency_thresholds), _.pm_max_trips_hr >= min(frequency_thresholds))\n",
    "    multi_only = multi_qual >> anti_join(_, single_qual, on=['schedule_gtfs_dataset_key', 'stop_id'])\n",
    "    #  only consider route_dir that run at least hourly when doing multi-route aggregation, should reduce edge cases\n",
    "    single_hourly = single_route_aggregation >> filter(_.am_max_trips_hr >= 1, _.pm_max_trips_hr >= 1)\n",
    "    single_hourly = single_hourly.explode('route_dir')[['route_dir', 'schedule_gtfs_dataset_key', 'stop_id']]\n",
    "    multi_only_explode = (multi_only[['schedule_gtfs_dataset_key', 'stop_id', 'route_dir']].explode('route_dir'))\n",
    "    multi_only_explode = multi_only_explode.merge(single_hourly, on = ['route_dir', 'schedule_gtfs_dataset_key', 'stop_id'])\n",
    "    multi_only_explode = multi_only_explode.sort_values(['schedule_gtfs_dataset_key','stop_id', 'route_dir']) #  sorting crucial for next step\n",
    "    print(f'{multi_only_explode.stop_id.nunique()} stops may qualify with multi-route aggregation')\n",
    "    return multi_only_explode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "609c507c-ecc4-411f-973b-31dfb150f317",
   "metadata": {},
   "outputs": [],
   "source": [
    "multi_only_explode = get_explode_multiroute_only(single_test2, multi_test2, (HQ_TRANSIT_THRESHOLD, MS_TRANSIT_THRESHOLD))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f581e78-f05b-4713-a65d-c27d28bdd148",
   "metadata": {},
   "outputs": [],
   "source": [
    "def accumulate_share_count(route_dir_exploded: pd.DataFrame):\n",
    "    '''\n",
    "    For use via pd.DataFrame.groupby.apply\n",
    "    Accumulate the number of times each route_dir shares stops with\n",
    "    each other in a dictionary (share_counts)\n",
    "    '''\n",
    "    global share_counts\n",
    "    rt_dir = route_dir_exploded.route_dir.to_numpy()\n",
    "    schedule_gtfs_dataset_key = route_dir_exploded.schedule_gtfs_dataset_key.iloc[0]\n",
    "    for route_dir in rt_dir:\n",
    "        route = route_dir.split('_')[0] #  don't compare opposite dirs of same route, leads to edge cases like AC Transit 45\n",
    "        other_dirs = [x for x in rt_dir if x != route_dir and x.split('_')[0] != route]\n",
    "        for other_dir in other_dirs:\n",
    "            key = schedule_gtfs_dataset_key+'__'+route_dir+'__'+other_dir\n",
    "            if key in share_counts.keys():\n",
    "                share_counts[key] += 1\n",
    "            else:\n",
    "                share_counts[key] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a117a2b-1ca5-4c7c-ae33-62763f5db66b",
   "metadata": {},
   "outputs": [],
   "source": [
    "share_counts = {}\n",
    "multi_only_explode.groupby(['schedule_gtfs_dataset_key', 'stop_id']).apply(accumulate_share_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37cec4a9-757f-4a7e-a980-5a2052f31b8c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# share_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bc40fa7-825d-416a-ad3e-e5a081bb1524",
   "metadata": {},
   "outputs": [],
   "source": [
    "s = pd.Series(share_counts.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53d2e6d7-2adc-4b14-b290-af783d707b14",
   "metadata": {},
   "outputs": [],
   "source": [
    "(s[s<11]).hist()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d839566e-7cc0-4345-b835-ce5a77ab70cc",
   "metadata": {},
   "source": [
    "### Which threshold?\n",
    "\n",
    "* 8 catches Muni 48 and 66, which are somewhat marginal but not an edge case per se"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92b8ffc8-b4f8-4aec-bc2c-a0b027be1d3a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# qualify"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3faa0af-7b24-402d-9f68-3d61acc6cb9e",
   "metadata": {},
   "source": [
    "## lookup function/filtering steps\n",
    "\n",
    "1. If a feed has no route_direction pairs qualifying, by definition no stops will qualify. Can exclude feed from next steps.\n",
    "1. Get a list of unique feeds where at least one route_directions pair qualifies to evaluate.\n",
    "1. Get stop_times filtered to that feed, and filter that to stops that only qualify with multiple routes, and route directions that pair with at least one other route_direction. Do not consider pairs between the same route in one direction and the same route in the opposite direction.\n",
    "1. After that filtering, check again if stop_times includes the minimum frequency to qualify at each stop. Exclude stops where it doesn't.\n",
    "1. Then... evaluate which route_directions can be aggregated at each remaining stop. From the full list of route_directions (sorted by frequency) serving the stop, use `list(itertools.combinations(this_stop_route_dirs, 2))` to get each unique pair of route_directions. Check each of those unique pairs to see if it meets the `SHARED_STOP_THRESHOLD`. If they all do, keep all stop_times entries for that stop, different route_directions can be aggregated together at that stop. If any do not, remove the least frequent route_direction and try again, until a subset passes (only keep stop_times for that subset) or until all are eliminated. Currently implemented recursively as below:\n",
    "\n",
    "    ```\n",
    "    attempting ['103_1', '101_1', '102_1', '104_1']... subsetting...\n",
    "    attempting ['103_1', '101_1', '102_1']... subsetting...\n",
    "    attempting ['103_1', '101_1']... matched!\n",
    "\n",
    "    attempting ['103_1', '101_0', '101_1', '103_0']... subsetting...\n",
    "    attempting ['103_1', '101_0', '101_1']... subsetting...\n",
    "    attempting ['103_1', '101_0']... subsetting...\n",
    "    exhausted!\n",
    "    ```\n",
    "\n",
    "1. With that filtered stop_times, recalculate stop-level frequencies as before. Only keep stops meeting the minimum frequency threshold for a major stop or HQ corridor.\n",
    "1. Finally, once again apply the `SHARED_STOP_THRESHOLD` after aggregation (by ensuring at least one route_dir at each stop has >= `SHARED_STOP_THRESHOLD` frequent stops). Exclude stops that don't meet this criteria.\n",
    "\n",
    "### edge cases:\n",
    "\n",
    "[AC Transit 45](https://www.actransit.org/sites/default/files/timetable_files/45-2023_12_03.pdf) _Opposite directions share a same-direction loop._ __Solved__ by preventing the same route from being compared with itself in the opposite direction.\n",
    "\n",
    "[SDMTS 944/945](https://www.sdmts.com/sites/default/files/routes/pdf/944.pdf) _Shared frequent stops are few, and these routes are isolated._ __Solved__ by once again applying the `SHARED_STOP_THRESHOLD` after aggregation (by ensuring at least one route_dir at each stop has >= `SHARED_STOP_THRESHOLD` frequent stops). Complex typology including a loop route, each pair of [944, 945, 945A(946)] has >= threshold... but not actually in the same spots!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc1583ce-b505-4783-9f7b-b46c93151d24",
   "metadata": {},
   "outputs": [],
   "source": [
    "SHARED_STOP_THRESHOLD = 8 #  current rec\n",
    "qualify = {key: share_counts[key] for key in share_counts.keys() if share_counts[key] >= SHARED_STOP_THRESHOLD}\n",
    "\n",
    "#  Yolobus. Separate route_id, but same route in a CW and CCW loop, drop per rule to not compare same rt with itself\n",
    "keys_to_drop = ['3c62ad6ee589d56eca915ce291a5df0a__42A_0__42B_0',\n",
    "               '3c62ad6ee589d56eca915ce291a5df0a__42B_0__42A_0']\n",
    "\n",
    "for key in keys_to_drop: qualify.pop(key) #  will error if key not present, check if situation still present and update key if needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44006dba-4ff0-4519-85bb-b6f6cf5b1392",
   "metadata": {},
   "outputs": [],
   "source": [
    "feeds_to_filter = np.unique([key.split('__')[0] for key in qualify.keys()])\n",
    "feeds_no_qualify = np.unique([key.split('__')[0] for key in share_counts.keys() if key.split('__')[0] not in feeds_to_filter])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d5254e9-33fe-464a-8d85-57d045b9ecee",
   "metadata": {},
   "outputs": [],
   "source": [
    "from calitp_data_analysis.tables import tbls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc909766-9a1b-4682-841d-a7516c186bd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "feeds_no_qualify = tbls.mart_transit_database.dim_gtfs_service_data() >> filter(_.gtfs_dataset_key.isin(feeds_no_qualify)) >> distinct(_.name, _.gtfs_dataset_key) >> collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0045a867-518e-459b-9396-511fac891682",
   "metadata": {},
   "outputs": [],
   "source": [
    "feeds_no_qualify >> filter(_.name.str.contains('Fr'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "264eefd5-2a77-4533-a4b5-df4ad95bde3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "feed_names = (tbls.mart_transit_database.dim_gtfs_service_data() >> filter(_.gtfs_dataset_key.isin(feeds_to_filter))\n",
    " >> distinct(_.name, _.gtfs_dataset_key)\n",
    " >> collect()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f253a870-d7d2-46f1-b766-ce9785ed5fda",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "feed_names_filtered = feed_names >> filter(_.name.str.contains('Sac'))\n",
    "display(feed_names_filtered)\n",
    "gtfs_dataset_key = feed_names_filtered.gtfs_dataset_key.iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f6ea7b6-26ea-4b9a-af4e-cdd507a403c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset_key = '015d67d5b75b5cf2b710bbadadfb75f5' #  Marin\n",
    "# gtfs_dataset_key = '3c62ad6ee589d56eca915ce291a5df0a' #  Yolobus 42A and 42B share 5+ stops so they match, which isn't desirable.\n",
    "# dataset_key = '70c8a8b71c815224299523bf2115924a' #  SacRT\n",
    "# dataset_key = '63029a23cb0e73f2a5d98a345c5e2e40' #  Elk Grove\n",
    "gtfs_dataset_key = 'f1b35a50955aeb498533c1c6fdafbe44' #  LBT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2c3c588-ecb0-4076-b62f-5fa3d74c7da3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def feed_level_filter(\n",
    "gtfs_dataset_key: str,\n",
    "multi_only_explode: pd.DataFrame,\n",
    "qualify_dict: dict,\n",
    "st_prepped: pd.DataFrame,\n",
    "frequency_thresholds: tuple\n",
    ") -> pd.DataFrame:\n",
    "    '''\n",
    "    For a single feed, filter potential stop_times to evaluate based on if their route_dir\n",
    "    appears at all in qualifying route_dir dict, recheck if there's any chance those stops\n",
    "    could qualify. Further shrinks problem space for check_stop lookup step\n",
    "    '''\n",
    "\n",
    "    this_feed_qual = {key.split(gtfs_dataset_key)[1][2:]:qualify_dict[key] for key in qualify_dict.keys() if key.split('__')[0] == gtfs_dataset_key}\n",
    "    qualify_pairs = [tuple(key.split('__')) for key in this_feed_qual.keys()]\n",
    "    arr = np.array(qualify_pairs[0])\n",
    "    for pair in qualify_pairs[1:]: arr = np.append(arr, np.array(pair))\n",
    "    any_appearance = np.unique(arr)\n",
    "\n",
    "    #  only need to check stops that qualify as multi-route only\n",
    "    stops_to_eval = multi_only_explode >> filter(_.schedule_gtfs_dataset_key == gtfs_dataset_key) >> distinct(_.stop_id)\n",
    "    st_prepped = st_prepped >> filter(_.schedule_gtfs_dataset_key == gtfs_dataset_key,\n",
    "                                      _.stop_id.isin(stops_to_eval.stop_id),\n",
    "                                     )\n",
    "    print(f'{st_prepped.shape}')\n",
    "    st_to_eval = st_prepped >> filter(_.route_dir.isin(any_appearance))\n",
    "    print(f'{st_to_eval.shape}')\n",
    "    #  cut down problem space by checking if stops still could qual after filtering for any appearance\n",
    "    min_rows = min(frequency_thresholds) * len(both_peaks_hrs)\n",
    "    st_could_qual = (st_to_eval >> group_by(_.stop_id)\n",
    "     >> mutate(could_qualify = _.shape[0] >= min_rows)\n",
    "     >> ungroup()\n",
    "     >> filter(_.could_qualify)\n",
    "    )\n",
    "    print(f'{st_could_qual.shape}')\n",
    "    return st_could_qual, qualify_pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43a0d51d-be2a-4da0-85e7-627c62659f61",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "st_could_qual, qualify_pairs = feed_level_filter(gtfs_dataset_key, multi_only_explode, qualify,\n",
    "                                                 st_prepped, (HQ_TRANSIT_THRESHOLD, MS_TRANSIT_THRESHOLD))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca76a557-3947-48bc-aa26-ba8a270af4b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_stop(this_stop_route_dirs, qualify_pairs):\n",
    "    #  check if all possible combinations included\n",
    "    this_stop_route_dirs = list(this_stop_route_dirs)\n",
    "    if len(this_stop_route_dirs) == 1:\n",
    "        print('exhausted!')\n",
    "        return []\n",
    "    print(f'attempting {this_stop_route_dirs}... ', end='')\n",
    "    stop_route_dir_pairs = list(itertools.combinations(this_stop_route_dirs, 2))\n",
    "    checks = np.array([True if rt_dir in qualify_pairs else False for rt_dir in stop_route_dir_pairs])\n",
    "    if checks.all():\n",
    "        print(f'matched!')\n",
    "        return this_stop_route_dirs\n",
    "    else:\n",
    "        print('subsetting...')\n",
    "        this_stop_route_dirs.pop(-1)\n",
    "        return check_stop(this_stop_route_dirs, qualify_pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3201f56-6953-42cf-94c5-16f7fcc44585",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check_stop(['no', 'nyet', 'bazz', 'fizz', 'buzz'], qualify_pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7f1cf16-ff3f-4128-927a-b13eed4cf717",
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_qualifying_stops(one_stop_df, qualify_pairs):\n",
    "\n",
    "    one_stop_df = (one_stop_df >> group_by(_.route_dir)\n",
    "                >> mutate(route_dir_count = _.shape[0]) >> ungroup()\n",
    "                >> arrange(-_.route_dir_count)\n",
    "               )\n",
    "    this_stop_route_dirs = (one_stop_df >> distinct(_.route_dir, _.route_dir_count)).route_dir.to_numpy() #  preserves sort order\n",
    "    aggregation_ok_route_dirs = check_stop(this_stop_route_dirs, qualify_pairs)\n",
    "    return one_stop_df >> filter(_.route_dir.isin(aggregation_ok_route_dirs))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ec04c81-1376-4357-b807-193dd27cf6a2",
   "metadata": {},
   "source": [
    "## unify function, try looping over all feeds?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26a97b9c-61e2-4af1-b764-2612bc12b8e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collinear_filter_feed(\n",
    "    gtfs_dataset_key: str,\n",
    "    multi_only_explode: pd.DataFrame,\n",
    "    qualify_dict: dict,\n",
    "    st_prepped: pd.DataFrame,\n",
    "    frequency_thresholds: tuple\n",
    "):\n",
    "    \n",
    "    st_could_qual, qualify_pairs = feed_level_filter(gtfs_dataset_key, multi_only_explode, qualify, st_prepped, frequency_thresholds)\n",
    "    st_qual_filter_1 = st_could_qual.groupby('stop_id').apply(filter_qualifying_stops, qualify_pairs=qualify_pairs)\n",
    "    st_qual_filter_1 = st_qual_filter_1.reset_index(drop=True)\n",
    "    if st_qual_filter_1.empty: return\n",
    "    trips_per_peak_qual_1 = create_aggregate_stop_frequencies.stop_times_aggregation_max_by_stop(st_qual_filter_1, analysis_date, single_route_dir=False)\n",
    "    trips_per_peak_qual_1 = trips_per_peak_qual_1 >> filter(_.am_max_trips_hr >= min(frequency_thresholds), _.pm_max_trips_hr >= min(frequency_thresholds))\n",
    "    short_routes = trips_per_peak_qual_1.explode('route_dir') >> count(_.route_dir) >> filter(_.n < SHARED_STOP_THRESHOLD)\n",
    "    print('short routes, all_short stops:')\n",
    "    display(short_routes)\n",
    "    trips_per_peak_qual_1['all_short'] = trips_per_peak_qual_1.route_dir.map(\n",
    "        lambda x: np.array([True if y in list(short_routes.route_dir) else False for y in x]).all())\n",
    "    display(trips_per_peak_qual_1 >> filter(_.all_short)) #  stops where _every_ shared route has less than SHARED_STOP_THRESHOLD frequent stops (even after aggregation)\n",
    "    trips_per_peak_qual_2 = trips_per_peak_qual_1 >> filter(-_.all_short) >> select(-_.all_short)\n",
    "    \n",
    "    return trips_per_peak_qual_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fefb42f-31a2-4f9a-9642-a66965b494ca",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# yolo = collinear_filter_feed(gtfs_dataset_key, multi_only_explode, qualify, st_prepped, (HQ_TRANSIT_THRESHOLD, MS_TRANSIT_THRESHOLD))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e1c4e80-a1f0-48b1-9eee-d1368f6b3b85",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# %%time 90 seconds (on default user) is not too bad! \n",
    "all_collinear = pd.DataFrame()\n",
    "for gtfs_dataset_key in feeds_to_filter:\n",
    "    df = collinear_filter_feed(gtfs_dataset_key, multi_only_explode, qualify,\n",
    "                               st_prepped, (HQ_TRANSIT_THRESHOLD, MS_TRANSIT_THRESHOLD))\n",
    "    all_collinear = pd.concat([df, all_collinear])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3108b92-f912-433d-a148-3c7cc892e77c",
   "metadata": {},
   "source": [
    "## Map single result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caea4694-0ba2-4bf1-bfbb-1d41dd42805f",
   "metadata": {},
   "outputs": [],
   "source": [
    "stops = helpers.import_scheduled_stops(\n",
    "    analysis_date,\n",
    "    get_pandas = True,\n",
    "    crs = PROJECT_CRS\n",
    ")\n",
    "\n",
    "stops = stops >> inner_join(_, stop_times>>distinct(_.feed_key, _.schedule_gtfs_dataset_key), on='feed_key')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fe6b0cd-e23a-4900-b3c0-128b4ffc32b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# gdf = stops >> inner_join(_, lbt, on = ['feed_key', 'stop_id']) >> distinct(_.stop_id, _.geometry)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b238791-62eb-4dfb-a62f-68ec4cd774a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# gdf.explore()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "612e4a4b-8684-4087-a5ff-6a51716fd717",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Map overall results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0cc20f0-d5c0-4c0a-92e4-d3fd653cce66",
   "metadata": {},
   "outputs": [],
   "source": [
    "single_qual = single_test2 >> filter(_.am_max_trips_hr >= min((HQ_TRANSIT_THRESHOLD, MS_TRANSIT_THRESHOLD)),\n",
    "                                     _.pm_max_trips_hr >= min((HQ_TRANSIT_THRESHOLD, MS_TRANSIT_THRESHOLD)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c35343bf-135a-4d16-beca-5fd288e89471",
   "metadata": {},
   "outputs": [],
   "source": [
    "multi_qual = multi_test2 >> filter(_.am_max_trips_hr >= min((HQ_TRANSIT_THRESHOLD, MS_TRANSIT_THRESHOLD)),\n",
    "                                   _.pm_max_trips_hr >= min((HQ_TRANSIT_THRESHOLD, MS_TRANSIT_THRESHOLD)), _.route_dir_count > 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca587eb6-c949-4777-9e9b-5cb121b1111b",
   "metadata": {},
   "outputs": [],
   "source": [
    "multi_only = multi_qual >> anti_join(_, single_qual, on=['schedule_gtfs_dataset_key', 'stop_id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f767a5f3-3953-40e7-8f70-666a57b1deb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "gdf = (stops >> inner_join(_, multi_only, on = ['stop_id', 'schedule_gtfs_dataset_key'])\n",
    "       >> mutate(route_dir = _.route_dir.astype(str))\n",
    "       >> distinct(_.stop_id, _.route_dir, _.am_max_trips_hr,\n",
    "            _.pm_max_trips_hr, _.geometry, _.schedule_gtfs_dataset_key)\n",
    "       \n",
    "      )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4be83c5-e8cc-419f-a28c-9294896819a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "gdf2 = (stops >> inner_join(_, all_collinear, on = ['stop_id', 'schedule_gtfs_dataset_key'])\n",
    "       >> mutate(route_dir = _.route_dir.astype(str))\n",
    "       >> distinct(_.stop_id, _.route_dir, _.am_max_trips_hr,\n",
    "            _.pm_max_trips_hr, _.geometry, _.schedule_gtfs_dataset_key)\n",
    "       \n",
    "      )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6c66a03-635e-4eb7-9728-644f95980d96",
   "metadata": {},
   "outputs": [],
   "source": [
    "gdf3 = (stops >> inner_join(_, single_qual, on = ['stop_id', 'schedule_gtfs_dataset_key'])\n",
    "       >> mutate(route_dir = _.route_dir.astype(str))\n",
    "       >> distinct(_.stop_id, _.route_dir, _.am_max_trips_hr,\n",
    "            _.pm_max_trips_hr, _.geometry, _.schedule_gtfs_dataset_key)\n",
    "       \n",
    "      )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ef04021-cf82-4b4e-b52b-a96e253ff1f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "m = gdf.explore(color='orange')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40725a20-bc5a-4cc2-b7fb-4c99fd42d893",
   "metadata": {},
   "outputs": [],
   "source": [
    "m = gdf3.explore(color='blue', m=m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55140c86-69f5-4617-a787-395eae644ec1",
   "metadata": {},
   "outputs": [],
   "source": [
    "m = gdf2.explore(m = m, color='red')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87de4b67-03cc-4555-9c8e-345820bee964",
   "metadata": {},
   "outputs": [],
   "source": [
    "folium.LayerControl().add_to(m);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "675f3e4c-b342-4394-bc1a-131efc405ba6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# m #  8 threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b8c2f48-7dc0-482e-92f8-488d61ff72ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "gdf.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2971590e-053f-4952-94f6-680fe7c848b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "gdf2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79d4e638-f597-41b6-89de-5d7e849db7cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "gdf3.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c10c0974-98f2-452f-ad71-88b4d6619d9d",
   "metadata": {},
   "source": [
    "## exporting\n",
    "\n",
    "If a stop appears in processed multiple route aggregation and potentially qualifying single routes, keep multi route row for that stop since the multi-route aggregation should have a higher frequency.\n",
    "\n",
    "So, anti_join potentially qualifying single route stops with processed multi route stops, then concat that with full multi route results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3067beb-4a08-40d1-8998-087ec60e9fd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "single_only_export = single_qual >> anti_join(_, all_collinear, on = ['schedule_gtfs_dataset_key', 'stop_id'])\n",
    "combined_export = pd.concat([single_only_export, all_collinear])\n",
    "combined_export = combined_export.explode('route_dir')\n",
    "combined_export['route_id'] = combined_export['route_dir'].str[:-2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b81da8d-343f-482e-b233-b47c6c9e6850",
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6863011-a654-4211-9696-e545be8f1be6",
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_export.to_parquet(f\"{GCS_FILE_PATH}max_arrivals_by_stop.parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "779c3d8b-77c0-40cf-a6b1-a87c99614ed8",
   "metadata": {},
   "source": [
    "## run pipeline..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f2da246-ee08-4952-a74f-7e8ca6b861d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "areas = gpd.read_parquet(f\"{GCS_FILE_PATH}hqta_areas.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac889a59-e406-4e4e-95e1-bbc0c3599e03",
   "metadata": {},
   "outputs": [],
   "source": [
    "ms = areas >> filter(_.hqta_type == ('major_stop_bus'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3341f7e0-acb5-4d59-9815-8569492e1f9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "ms = ms.dissolve(['agency_primary', 'route_id']).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "526cb669-f168-42f0-a4a3-4d5764d870d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "m2 = ms.explore()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78e94313-ac59-4f7f-aa6d-8a009ed5b2d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "hq = areas >> filter(_.hqta_type == ('hq_corridor_bus'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8ead5e9-bead-4834-adbd-6ebf7961e334",
   "metadata": {},
   "outputs": [],
   "source": [
    "hq = hq.dissolve(['agency_primary', 'route_id']).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e53debc7-f8ab-4de3-af84-780c9bef2a0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "m2 = hq.explore(color='red', m=m2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e25cf3c-bcfe-40cd-910e-11c36afe8719",
   "metadata": {},
   "outputs": [],
   "source": [
    "folium.LayerControl().add_to(m2);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0073691-faf8-4d73-99dd-882c41d8dd57",
   "metadata": {},
   "outputs": [],
   "source": [
    "m2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e231d9e-73ba-4ae5-a128-d9470368a30e",
   "metadata": {},
   "outputs": [],
   "source": [
    "hqta_points = gpd.read_parquet(f\"{GCS_FILE_PATH}hqta_points.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23db2522-3355-4334-ae61-4f2155844a8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "one_pts = hqta_points >> filter(_.agency_primary.str.contains('Sacramento'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d643893d-f0a0-4f7c-b617-f193d9e62dd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# one_pts.explore(column='hqta_type')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b351da3-4825-4caa-8333-33386db868f1",
   "metadata": {},
   "source": [
    "## all bus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55307463-952c-4595-98d7-a5619096aff3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# hqta_segments = gpd.read_parquet(f\"{GCS_FILE_PATH}all_bus.parquet\")\n",
    "hqta_segments = gpd.read_parquet(f\"{GCS_FILE_PATH}hqta_segments.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4043cef0-139f-419f-9915-683f1d85d129",
   "metadata": {},
   "outputs": [],
   "source": [
    "one = hqta_segments >> filter(_.schedule_gtfs_dataset_key == '70c8a8b71c815224299523bf2115924a')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b22b48e3-3b5f-4d72-b7ca-70ee7a27981c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# one.geometry = one.buffer(35)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b9c9200-ebf2-4acf-9f17-71f513695451",
   "metadata": {},
   "outputs": [],
   "source": [
    "one = one >> filter(_.route_id.isin(['228', '227', '062', '061', '056', '106']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3337a03f-33e6-4d1d-be87-c6244496804a",
   "metadata": {},
   "outputs": [],
   "source": [
    "m3 = one.explore(column='segment_direction')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a50e62e3-761a-4251-a69f-888dd2260fbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "folium.LayerControl().add_to(m3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "889c305d-e5ea-4b69-80e7-5bd9c5712e30",
   "metadata": {},
   "outputs": [],
   "source": [
    "m3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "425b28c2-0da4-4feb-be60-5389cb2f77c4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
