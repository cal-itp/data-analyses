{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "91650562-357e-465f-9912-09e8a2e574aa",
   "metadata": {},
   "source": [
    "# Refactor bus corridors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d229973-9381-4177-9a23-7e07b9d041d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CALITP_BQ_MAX_BYTES\"] = str(900_000_000_000) ## 800GB?\n",
    "\n",
    "import datetime as dt\n",
    "import geopandas as gpd\n",
    "import pandas as pd\n",
    "import zlib\n",
    "\n",
    "from siuba import *\n",
    "\n",
    "import utilities\n",
    "import A1_rail_ferry_brt as rail_ferry_brt\n",
    "\n",
    "from shared_utils import rt_utils\n",
    "\n",
    "GCS_FILE_PATH = utilities.GCS_FILE_PATH\n",
    "\n",
    "analysis_date = rail_ferry_brt.analysis_date\n",
    "analysis_date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4d65db0-c8df-4c16-9490-0f0514b12776",
   "metadata": {},
   "outputs": [],
   "source": [
    "# calitp-analytics-data/data-analyses/high_quality_transit_areas/bus_corridors/182_bus.parquet\n",
    "bus_corridor = gpd.read_parquet(f\"{GCS_FILE_PATH}bus_corridors/182_bus.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5aba8372-d0d1-43d0-bcc2-b5dbdd9b34de",
   "metadata": {},
   "outputs": [],
   "source": [
    "itp_id = 182\n",
    "date_str = analysis_date.strftime(rt_utils.FULL_DATE_FMT)\n",
    "\n",
    "# Skip writing geoparquet again for now \n",
    "# TODO: tweak rt_utils to overwrite export? \n",
    "# Overwriting while testing this is not ideal, don't want to mess it up\n",
    "\n",
    "#routelines = rt_utils.get_routelines(itp_id, analysis_date)\n",
    "#routelines = gpd.read_parquet(f\"{rt_utils.GCS_FILE_PATH}\"\n",
    "#                             f\"cached_views/routelines_{itp_id}_{date_str}.parquet\" \n",
    "#                            )\n",
    "\n",
    "#routelines.to_parquet(f\"./data/routelines_{itp_id}_{date_str}.parquet\")\n",
    "\n",
    "## force clear to ensure route type data present\n",
    "#trips = rt_utils.get_trips(itp_id, analysis_date, force_clear=True, route_types = ['3'])\n",
    "#trips.to_parquet(f\"./data/trips_{itp_id}_{date_str}.parquet\")\n",
    "\n",
    "#stop_times = rt_utils.get_stop_times(itp_id, analysis_date)\n",
    "#stop_times.to_parquet(f\"./data/st_{itp_id}_{date_str}.parquet\")\n",
    "\n",
    "#stops = rt_utils.get_stops(itp_id, analysis_date)\n",
    "#stops.to_parquet(f\"./data/stops_{itp_id}_{date_str}.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f924152-711f-4967-96dd-c413a051045b",
   "metadata": {},
   "outputs": [],
   "source": [
    "routelines = gpd.read_parquet(f\"./data/routelines_{itp_id}_{date_str}.parquet\")\n",
    "trips = pd.read_parquet(f\"./data/trips_{itp_id}_{date_str}.parquet\")\n",
    "stop_times = pd.read_parquet(f\"./data/st_{itp_id}_{date_str}.parquet\")\n",
    "stops = gpd.read_parquet(f\"./data/stops_{itp_id}_{date_str}.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a16b882e-5e67-42ed-b937-d179c93be2ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "def single_operator_hqta2():\n",
    "    routelines = gpd.read_parquet(f\"./data/routelines_{itp_id}_{date_str}.parquet\")\n",
    "    trips = pd.read_parquet(f\"./data/trips_{itp_id}_{date_str}.parquet\")\n",
    "    stop_times = pd.read_parquet(f\"./data/st_{itp_id}_{date_str}.parquet\")\n",
    "    stops = gpd.read_parquet(f\"./data/stops_{itp_id}_{date_str}.parquet\")\n",
    "    \n",
    "    distinct_routes = (\n",
    "        trips\n",
    "        >> distinct(_.route_id, _.shape_id, _.direction_id, _keep_all=True)\n",
    "        >> select(_.calitp_itp_id, _.route_id, _.shape_id, _.direction_id, _.trip_id)\n",
    "    )\n",
    "    \n",
    "     route_count_by_stop = (\n",
    "        stop_times\n",
    "        >> select(_.stop_id, _.trip_id)\n",
    "        >> inner_join(_, distinct_routes, on=\"trip_id\")\n",
    "        >> count(_.stop_id)\n",
    "        >> rename(n_routes=_.n)\n",
    "        >> arrange(-_.n_routes)\n",
    "    )\n",
    "        \n",
    "        \n",
    "    trips_shape_sorted = (\n",
    "        trips.groupby(\"shape_id\")\n",
    "        .count()\n",
    "        .sort_values(by=\"trip_id\", ascending=False)\n",
    "        .index\n",
    "    )\n",
    "    \n",
    "    trips_shape_sorted = pd.Series(trips_shape_sorted)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d20142a4-dd20-469c-856f-40571bd1c13a",
   "metadata": {},
   "source": [
    "## Split each shape_id into multiple hqta_segments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44f0817c-1ac1-4298-b79b-173aabbc664d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_segment_id(df):\n",
    "    df = df.reset_index(drop=True)\n",
    "    \n",
    "    ## compute (hopefully unique) hash of segment id that can be used \n",
    "    # across routes/operators\n",
    "    df = df.assign(\n",
    "        segment_sequence = df.index.astype(str))\n",
    "    \n",
    "    df2 = df.assign(\n",
    "        hqta_segment_id = df.apply(lambda x: \n",
    "                                   # this checksum hash always give same value if \n",
    "                                   # the same combination of strings are given\n",
    "                                   zlib.crc32(\n",
    "                                       (str(x.calitp_itp_id) + x.shape_id + x.segment_sequence)\n",
    "                                       .encode(\"utf-8\")), \n",
    "                                       axis=1),\n",
    "        ##generous buffer for street/sidewalk width? \n",
    "        # Required to spatially find stops within each segment\n",
    "        geometry = df.geometry.buffer(50),\n",
    "    )\n",
    "    \n",
    "    return df2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12888ca9-348a-4e42-b88d-6a2fa1a80b43",
   "metadata": {},
   "source": [
    "## Aggregate stop_times by departure hour"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9318c1b1-dcc8-481f-9bb8-c3d37be27e3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_stop_times(stop_times_df):\n",
    "    \n",
    "    df = (stop_times_df.dropna(subset=[\"departure_time\"])\n",
    "            ## filter duplicates for top2 approach\n",
    "          .drop_duplicates(subset=[\"trip_id\", \"stop_id\"])\n",
    "         )\n",
    "    \n",
    "    ## reformat GTFS time to a format datetime can ingest \n",
    "    df[\"departure_time\"] = df.departure_time.apply(utilities.fix_arrival_time)\n",
    "    df[\"departure_hour\"] = df.departure_time.apply(\n",
    "            lambda x: dt.datetime.strptime(x, rt_utils.HOUR_MIN_SEC_FMT).hour)\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "def aggregate_stops_by_hour(df):\n",
    "    df2 = (df  \n",
    "           >> count(_.calitp_itp_id, _.stop_id, _.departure_hour)\n",
    "           >> rename(n_trips = _.n)\n",
    "          )\n",
    "    \n",
    "    return df2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d00c0099-096d-4879-bb93-a63ca3daa2fb",
   "metadata": {},
   "source": [
    "## Join stops to hqta_segment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c12a480-d274-4a19-9dd2-527fcff0a97c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find nearest stop\n",
    "def hqta_segment_to_stops(hqta_segmented, stops):\n",
    "    segment_to_stop = (gpd.sjoin(\n",
    "            stops.drop(columns = [\"stop_lon\", \"stop_lat\"]), \n",
    "            hqta_segmented[[\"hqta_segment_id\", \"geometry\"]],\n",
    "            how = \"inner\",\n",
    "            predicate = \"intersects\"\n",
    "        ).drop(columns = \"index_right\")\n",
    "        .drop_duplicates(subset=[\"calitp_itp_id\", \"stop_id\", \"hqta_segment_id\"])\n",
    "        .reset_index(drop=True)\n",
    "    )\n",
    "    \n",
    "    segment_to_stop2 = pd.merge(\n",
    "        hqta_segmented[[\"hqta_segment_id\", \"geometry\"]],\n",
    "        segment_to_stop.drop(columns = \"geometry\"),\n",
    "        how = \"inner\"\n",
    "    )\n",
    "    \n",
    "    return segment_to_stop2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ab9aedd-1494-48b0-aded-4e5e311d7d3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def max_trips_by_stop(df, new_col):\n",
    "    df2 = (df\n",
    "           >> group_by(_.calitp_itp_id, _.stop_id)\n",
    "           >> summarize(n_trips = _.n_trips.max())\n",
    "    ).rename(columns = {\"n_trips\": new_col})\n",
    "\n",
    "    return df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e0bc240-efb5-42ec-9c15-3602faa0aa6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def hqta_segment_with_max_trips(df):\n",
    "    # Within a hqta_segment_id find\n",
    "    # max am_peak trips and max pm_peak trips\n",
    "    # drop stop_id info (since multiple stops can share the same max)\n",
    "\n",
    "    df2 = (df.assign(\n",
    "            am_max_trips = df.groupby(\"hqta_segment_id\")[\"am_trips\"].transform(\"max\"),\n",
    "            pm_max_trips = df.groupby(\"hqta_segment_id\")[\"pm_trips\"].transform(\"max\"),\n",
    "        )[[\"hqta_segment_id\", \"am_max_trips\", \"pm_max_trips\", \"geometry\"]]\n",
    "           .drop_duplicates()\n",
    "           .reset_index(drop=True)\n",
    "    )\n",
    "    \n",
    "    df2 = df2.assign(\n",
    "        hq_transit_corr = df2.apply(lambda x: \n",
    "                                    x.am_max_trips > 4 and x.pm_max_trips > 4, \n",
    "                                    axis=1)\n",
    "    )\n",
    "    \n",
    "    return df2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25b9165f-3c71-4bcf-9de8-9d3a7c1d6cca",
   "metadata": {},
   "source": [
    "## Put single shape hqta together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e9bc5b5-3b92-4b3a-a1d8-5c85db70f050",
   "metadata": {},
   "outputs": [],
   "source": [
    "def single_shape_hqta2(routelines, stops, stop_times, shape_id):\n",
    "    # Filter to just 1 shape_id\n",
    "    single_line = routelines >> filter(_.shape_id == shape_id)\n",
    "\n",
    "    # Turn 1 row of geometry into hqta_segments, every 1,250 m\n",
    "    segmented = gpd.GeoDataFrame() ##changed to gdf?\n",
    "\n",
    "    for segment in utilities.create_segments(single_line.geometry):\n",
    "        to_append = single_line.drop(columns=[\"geometry\"])\n",
    "        to_append[\"geometry\"] = segment\n",
    "        segmented = pd.concat((segmented, to_append))\n",
    "\n",
    "    segmented = add_segment_id(segmented)\n",
    "\n",
    "    # Fix stop times so it can be parsed\n",
    "    stop_times = clean_stop_times(stop_times)\n",
    "    \n",
    "    # Join hqta_segment_id to stop_id\n",
    "    segment_to_stop = hqta_segment_to_stops(segmented, stops)\n",
    "    \n",
    "    # Aggregate stop_times to departure_hour\n",
    "    stop_times_by_hour = aggregate_stops_by_hour(stop_times)\n",
    "\n",
    "    # Calculate number of trips by AM/PM peak for each hqta_segment_id-stop_id\n",
    "    am_peak = max_trips_by_stop(\n",
    "        stop_times_by_hour[stop_times_by_hour.departure_hour < 12], \n",
    "        \"am_trips\")\n",
    "\n",
    "    pm_peak = max_trips_by_stop(\n",
    "        stop_times_by_hour[stop_times_by_hour.departure_hour >= 12], \n",
    "        \"pm_trips\")\n",
    "\n",
    "    # Merge in AM/PM peak trips on hqta_segment_id-stop_id\n",
    "    stop_cols = [\"calitp_itp_id\", \"stop_id\"]\n",
    "\n",
    "    segment_with_trips = pd.merge(\n",
    "        segment_to_stop, \n",
    "        am_peak,\n",
    "        on = stop_cols,\n",
    "    ).merge(pm_peak, on = stop_cols)\n",
    "\n",
    "    # Only keep max AM/PM peak trip info by hqta_segment_id (no more stop_id)\n",
    "    segment_peak_trips = hqta_segment_with_max_trips(segment_with_trips)\n",
    "    \n",
    "    return segment_peak_trips"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d16cba4-8d76-402d-b16a-b6d84475735b",
   "metadata": {},
   "outputs": [],
   "source": [
    "trips_shape_sorted = (\n",
    "    trips.groupby(\"shape_id\")\n",
    "    .count()\n",
    "    .sort_values(by=\"trip_id\", ascending=False)\n",
    "    .index\n",
    ")\n",
    "\n",
    "trips_shape_sorted = pd.Series(trips_shape_sorted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "181237e0-e598-4a79-8dd6-b50fcf27ee45",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf53d916-1b3c-43c1-9d5d-a36b25fe48ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "start = dt.datetime.now()\n",
    "df = pd.DataFrame()\n",
    "\n",
    "for shape_id in trips_shape_sorted:\n",
    "    \n",
    "    hqta_for_shape = single_shape_hqta2(routelines, stops, stop_times, shape_id)\n",
    "    hqta_for_shape = hqta_for_shape.assign(\n",
    "        shape_id = shape_id\n",
    "    )\n",
    "    \n",
    "    df = pd.concat([df, hqta_for_shape], axis=0, ignore_index=True)\n",
    "\n",
    "end = dt.datetime.now()\n",
    "print(f\"Execution time: {end-start}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8352feed-fb2b-496d-b333-be78599013ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_parquet(\"./data/182_bus2.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "047fc795-e65c-4f0d-8988-cd1d78323a0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape_id.nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f583423-f92b-4353-8392-1b1f74e9ee6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "explore_shape = df.shape_id[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbd7a319-b43f-4716-86a8-351bf5c530f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df.shape_id==explore_shape]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5704a7f2-6d48-4d4e-af05-97eeb11fea18",
   "metadata": {},
   "outputs": [],
   "source": [
    "m1 = pd.merge(df[df.shape_id==explore_shape],\n",
    "         bus_corridor[bus_corridor.shape_id==explore_shape],\n",
    "         on = [\"hqta_segment_id\", \"shape_id\"],\n",
    "         how = \"outer\",\n",
    "         indicator=True\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61d1082e-57ae-48d7-8b08-e7a0acec0aca",
   "metadata": {},
   "outputs": [],
   "source": [
    "m1._merge.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6f0c507-a97e-44fb-bc65-e04fb9c2d78a",
   "metadata": {},
   "outputs": [],
   "source": [
    "m1[[\"hqta_segment_id\", \"am_max_trips_x\", \"am_max_trips_y\", \n",
    "    \"pm_max_trips_x\", \"pm_max_trips_y\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7f7f99a-4142-438b-92bc-fc707fbbbb2a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
