{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8ea6f70a-3368-4d56-a40c-cbf6ef661e5c",
   "metadata": {},
   "source": [
    "# LA Metro 720\n",
    "\n",
    "Pick a route that is pretty long, and compare how the stops are aggregated to AM/PM peak."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6c183037-ccdc-49b1-bd4f-f161f0294688",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/geopandas/_compat.py:111: UserWarning: The Shapely GEOS version (3.10.2-CAPI-1.16.0) is incompatible with the GEOS version PyGEOS was compiled with (3.10.1-CAPI-1.16.0). Conversions between both will be slow.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import datetime as dt\n",
    "import geopandas as gpd\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from siuba import *\n",
    "\n",
    "import utilities\n",
    "import A1_rail_ferry_brt as rail_ferry_brt\n",
    "import B1_bus_corridors as bus_corridors\n",
    "\n",
    "from shared_utils import rt_utils\n",
    "\n",
    "GCS_FILE_PATH = utilities.GCS_FILE_PATH\n",
    "\n",
    "analysis_date = rail_ferry_brt.analysis_date\n",
    "\n",
    "import dask.dataframe as dd\n",
    "import dask_geopandas\n",
    "\n",
    "\n",
    "itp_id = 182\n",
    "date_str = analysis_date.strftime(rt_utils.FULL_DATE_FMT)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b0bd90c-1cbe-42b6-b496-8fa4723e48ad",
   "metadata": {},
   "source": [
    "## Eric's original method by `shape_id`\n",
    "\n",
    "Select just the `shape_id` if it starts with 720, and compile those results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "302fd389-6f32-441a-afaf-88787c9148c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from shared_utils import geography_utils\n",
    "\n",
    "import zlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9b4d17a2-a41c-4b23-a083-f684f4e95658",
   "metadata": {},
   "outputs": [],
   "source": [
    "def single_shape_hqta(\n",
    "    routelines, trips, stop_times, stops, route_count_by_stop, shape_id, hqta\n",
    "):\n",
    "    \"\"\"Starting with a single GTFS shape, split that shape into segments and determine if each segment qualifies\n",
    "    as an HQTA. Existing segments within a shape are dropped for that shape, since their peak frequency and\n",
    "    HQTA status would have already been calculated for a previous shape.\n",
    "    \"\"\"\n",
    "\n",
    "    single_line = routelines >> filter(_.shape_id == shape_id)\n",
    "    if single_line.size == 0 or single_line.geometry.isna().all():\n",
    "        print(f\"no geometry for shape {shape_id}\")\n",
    "        return\n",
    "    \n",
    "    debug_ids = []\n",
    "    if shape_id in debug_ids:\n",
    "        print(f\"***debug shape*** {shape_id}\")\n",
    "        debug_dict[f\"{shape_id}_single_line\"] = single_line\n",
    "        debug_dict[f\"{shape_id}_hqta\"] = hqta\n",
    "\n",
    "    calculated_stops = []\n",
    "    ## TODO any way to make more efficient? a pre-calculated shape overlap? something else?\n",
    "    if hqta.size != 0:\n",
    "        already_calculated = hqta.dissolve(\n",
    "            by=\"calitp_itp_id\"\n",
    "        )  ## get single polygon of HQTA calculation complete area\n",
    "        single_line = single_line.overlay(\n",
    "            already_calculated, how=\"difference\"\n",
    "        )  ## drop calculation complete area from current shape\n",
    "        if single_line.size == 0:\n",
    "            segments_with_max_stop = None\n",
    "            print(f\"already calculated corridor for shape {shape_id}\")\n",
    "            return\n",
    "\n",
    "    segmented = gpd.GeoDataFrame() ##changed to gdf?\n",
    "    for segment in utilities.create_segments(single_line.geometry):\n",
    "        to_append = single_line.drop(columns=[\"geometry\"])\n",
    "        to_append[\"geometry\"] = segment\n",
    "        segmented = pd.concat((segmented, to_append))\n",
    "\n",
    "    segmented = segmented.reset_index()\n",
    "    if shape_id in debug_ids:\n",
    "        debug_dict[f\"{shape_id}_a_segmented\"] = segmented\n",
    "\n",
    "    try:\n",
    "        segmented[\"segment_sequence\"] = segmented.index.astype(str)\n",
    "        assert 'calitp_itp_id' in segmented.columns\n",
    "        # segmented = segmented.astype({\"calitp_itp_id\": str}) ## casting this to string is bad for exports...\n",
    "        ## instead, cast when generating segment id below...\n",
    "    except:\n",
    "        print(f\"segmented shape has no itp_id {shape_id}\")\n",
    "        return\n",
    "\n",
    "    ## compute (hopefully unique) hash of segment id that can be used across routes/operators\n",
    "    segmented[\"hqta_segment_id\"] = segmented.apply(\n",
    "        lambda x: zlib.crc32(\n",
    "            (str(x.calitp_itp_id) + x.shape_id + x.segment_sequence).encode(\"utf-8\")\n",
    "        ),\n",
    "        axis=1,\n",
    "    )\n",
    "\n",
    "    segmented.geometry = segmented.buffer(\n",
    "        50\n",
    "    )  ##generous buffer for street/sidewalk width? Required to spatially find stops within each segment\n",
    "\n",
    "    if shape_id in debug_ids:\n",
    "        debug_dict[f\"{shape_id}_segmented\"] = segmented\n",
    "        debug_dict[f\"{shape_id}_stops\"] = stops\n",
    "        debug_dict[f\"{shape_id}_route_ct_by_stop\"] = route_count_by_stop\n",
    "\n",
    "    segments_with_max_stop = segmented.apply(\n",
    "        utilities.find_stop_with_high_trip_count,\n",
    "        axis=1,\n",
    "        args=(stops, stop_times, 1, calculated_stops),\n",
    "    )\n",
    "\n",
    "    if not \"stop_id\" in segments_with_max_stop.columns:\n",
    "        segments_with_top2_stops = None\n",
    "        print(f\"no stops for shape {shape_id}\")\n",
    "        return  ## no stops within segment\n",
    "\n",
    "    max_stop_times = (\n",
    "        stop_times\n",
    "        >> select(_.stop_id, _.trip_id, _.departure_time)\n",
    "        >> inner_join(_, segments_with_max_stop, on=\"stop_id\")\n",
    "    )  ## filter stop_times to the key stops in each segment\n",
    "    max_stop_times = max_stop_times.dropna(subset=[\"departure_time\"])\n",
    "    max_stop_times = max_stop_times.drop_duplicates(\n",
    "        subset=[\"trip_id\", \"hqta_segment_id\"]\n",
    "    )  ## filter duplicates for top2 approach\n",
    "\n",
    "    if shape_id in debug_ids:\n",
    "        debug_dict[f\"{shape_id}_max_stop0\"] = max_stop_times\n",
    "\n",
    "    max_stop_times[\"departure_time\"] = max_stop_times[\"departure_time\"].apply(\n",
    "        utilities.fix_arrival_time\n",
    "    )  ## reformat GTFS time to a format datetime can ingest\n",
    "    max_stop_times[\"departure_dt\"] = max_stop_times[\"departure_time\"].apply(\n",
    "        lambda x: dt.datetime.strptime(x, \"%H:%M:%S\")\n",
    "    )\n",
    "    max_stop_times[\"departure_hour\"] = max_stop_times[\"departure_dt\"].apply(\n",
    "        lambda x: x.hour\n",
    "    )\n",
    "\n",
    "    if max_stop_times.size == 0:\n",
    "        print(f\"no commute hour trips for shape {shape_id}\")\n",
    "        return\n",
    "\n",
    "    if shape_id in debug_ids:\n",
    "        debug_dict[f\"{shape_id}_max_stop\"] = max_stop_times\n",
    "\n",
    "    ## new flexible peak\n",
    "    segment_am_max = (\n",
    "        max_stop_times\n",
    "        >> count(_.hqta_segment_id, _.departure_hour)\n",
    "        >> filter(_.departure_hour < 12)\n",
    "        >> group_by(_.hqta_segment_id)\n",
    "        >> summarize(am_max_trips=_.n.max())\n",
    "    )\n",
    "\n",
    "    segment_pm_max = (\n",
    "        max_stop_times\n",
    "        >> count(_.hqta_segment_id, _.departure_hour)\n",
    "        >> filter(_.departure_hour >= 12)\n",
    "        >> group_by(_.hqta_segment_id)\n",
    "        >> summarize(pm_max_trips=_.n.max())\n",
    "    )\n",
    "    try:\n",
    "        segment_peak_service = segment_am_max >> inner_join(\n",
    "            _, segment_pm_max, on=\"hqta_segment_id\"\n",
    "        )\n",
    "        segment_peak_service[\"hq_transit_corr\"] = segment_peak_service.apply(\n",
    "            lambda x: x.am_max_trips > 4 and x.pm_max_trips > 4, axis=1\n",
    "        )\n",
    "    except:  ## append when all segments only have am or pm trips, not an hqta by definition\n",
    "        segment_peak_service = pd.concat((segment_am_max, segment_pm_max))\n",
    "        segment_peak_service[\"hq_transit_corr\"] = False\n",
    "\n",
    "    segment_peak_service = segment_peak_service.reset_index(drop=True)\n",
    "\n",
    "    single_hqta = segments_with_max_stop >> inner_join(\n",
    "        _, segment_peak_service, on=\"hqta_segment_id\"\n",
    "    )\n",
    "    single_hqta = single_hqta >> select(-_.calitp_extracted_at, -_.index, -_.n_trips)\n",
    "\n",
    "    if shape_id in debug_ids:\n",
    "        debug_dict[f\"{shape_id}_single_hqta\"] = single_hqta\n",
    "\n",
    "    return single_hqta.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b14c443d-379a-4bc9-94eb-8a91a20ec5cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "FILE_PATH = f\"{rt_utils.GCS_FILE_PATH}cached_views/\"\n",
    "\n",
    "\n",
    "def single_operator_hqta(itp_id, analysis_date):\n",
    "\n",
    "    global debug_dict\n",
    "    debug_dict = {}\n",
    "    \n",
    "    '''\n",
    "    # shapes, trips, stop_times, stops = views\n",
    "    routelines = rt_utils.get_routelines(itp_id, analysis_date)\n",
    "    ## force clear to ensure route type data present\n",
    "    trips = rt_utils.get_trips(itp_id, analysis_date, force_clear=True, route_types = ['3'])\n",
    "    stop_times = rt_utils.get_stop_times(itp_id, analysis_date)\n",
    "    stops = rt_utils.get_stops(itp_id, analysis_date)\n",
    "    '''\n",
    "    date_str = analysis_date.strftime(rt_utils.FULL_DATE_FMT)\n",
    "\n",
    "    routelines = gpd.read_parquet(f\"{FILE_PATH}routelines_{itp_id}_{date_str}.parquet\")\n",
    "    trips = pd.read_parquet(f\"{FILE_PATH}trips_{itp_id}_{date_str}.parquet\")\n",
    "    stop_times = pd.read_parquet(f\"{FILE_PATH}st_{itp_id}_{date_str}.parquet\")\n",
    "    stops = gpd.read_parquet(f\"{FILE_PATH}stops_{itp_id}_{date_str}.parquet\")\n",
    "    \n",
    "    distinct_routes = (\n",
    "        trips\n",
    "        >> distinct(_.route_id, _.shape_id, _.direction_id, _keep_all=True)\n",
    "        >> select(_.calitp_itp_id, _.route_id, _.shape_id, _.direction_id, _.trip_id)\n",
    "    )\n",
    "    \n",
    "    distinct_routes = distinct_routes[\n",
    "        distinct_routes.shape_id.str.startswith(\"720\")].reset_index(drop=True)\n",
    "\n",
    "    route_count_by_stop = (\n",
    "        stop_times\n",
    "        >> select(_.stop_id, _.trip_id)\n",
    "        >> inner_join(_, distinct_routes, on=\"trip_id\")\n",
    "        >> count(_.stop_id)\n",
    "        >> rename(n_routes=_.n)\n",
    "        >> arrange(-_.n_routes)\n",
    "    )\n",
    "\n",
    "    hqta = gpd.GeoDataFrame()\n",
    "    ## start with shapes including the highest number of trips\n",
    "    trips_shape_sorted = (\n",
    "        trips[trips.shape_id.str.startswith(\"720\")].groupby(\"shape_id\")\n",
    "        .count()\n",
    "        .sort_values(by=\"trip_id\", ascending=False)\n",
    "        .index\n",
    "    )\n",
    "    trips_shape_sorted = pd.Series(trips_shape_sorted)\n",
    "    total_shapes = len(trips_shape_sorted)\n",
    "    print(f\"there are {total_shapes} shapes total\")\n",
    "    for ix, shape_id in trips_shape_sorted.items():\n",
    "        print(f\"calculating for shape_id {shape_id}\")\n",
    "        if ix % 25 == 0:\n",
    "            print(\n",
    "                f\"progress: {ix}/{total_shapes} shapes ({round(((ix/total_shapes)*100), 2)}%)\"\n",
    "            )\n",
    "        # try:\n",
    "        result = single_shape_hqta(\n",
    "            routelines, trips, stop_times, stops, route_count_by_stop, shape_id, hqta\n",
    "        )\n",
    "        hqta = pd.concat((hqta, result))\n",
    "        # except:\n",
    "            # print(f\"unable to calculate HQTA for shape_id {shape_id}\")\n",
    "        try:\n",
    "            hqta = hqta.set_crs(geography_utils.CA_NAD83Albers)\n",
    "        except:\n",
    "            continue\n",
    "    return hqta\n",
    "    # return hqta.drop(columns=['n', 'departure_hour']).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "21010441-5429-4d5b-b616-bb479d5244d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "there are 9 shapes total\n",
      "calculating for shape_id 7201271_FEB22\n",
      "progress: 0/9 shapes (0.0%)\n",
      "calculating for shape_id 7201273_FEB22\n",
      "calculating for shape_id 7201277_FEB22\n",
      "calculating for shape_id 7201272_FEB22\n",
      "already calculated corridor for shape 7201272_FEB22\n",
      "calculating for shape_id 7201275_FEB22\n",
      "no stops for shape 7201275_FEB22\n",
      "calculating for shape_id 7201278_FEB22\n",
      "already calculated corridor for shape 7201278_FEB22\n",
      "calculating for shape_id 7201274_FEB22\n",
      "already calculated corridor for shape 7201274_FEB22\n",
      "calculating for shape_id 7201276_FEB22\n",
      "calculating for shape_id 7201279_FEB22\n",
      "already calculated corridor for shape 7201279_FEB22\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1929/2019002867.py:2: UserWarning: this is an initial implementation of Parquet/Feather file support and associated metadata.  This is tracking version 0.1.0 of the metadata specification at https://github.com/geopandas/geo-arrow-spec\n",
      "\n",
      "This metadata specification does not yet make stability promises.  We do not yet recommend using this in a production setting unless you are able to rewrite your Parquet/Feather files.\n",
      "\n",
      "To further ignore this warning, you can do: \n",
      "import warnings; warnings.filterwarnings('ignore', message='.*initial implementation of Parquet.*')\n"
     ]
    }
   ],
   "source": [
    "operator = single_operator_hqta(itp_id, analysis_date)\n",
    "operator.to_parquet(\"./data/eric_720.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "3044195d-301a-4d16-8ef7-845001fffe0a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>hqta_segment_id</th>\n",
       "      <th>trip_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>446356391</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>502734782</td>\n",
       "      <td>27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>918098596</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>945986198</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1057971855</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1102979634</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1208512025</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1332062720</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1839065905</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>2207484445</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>2231258628</td>\n",
       "      <td>45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>2708196140</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>2785414965</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>3506519971</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>3597466554</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>4093214354</td>\n",
       "      <td>45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>4103387787</td>\n",
       "      <td>31</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    hqta_segment_id  trip_id\n",
       "0         446356391       11\n",
       "1         502734782       27\n",
       "2         918098596        5\n",
       "3         945986198       15\n",
       "4        1057971855       15\n",
       "5        1102979634       18\n",
       "6        1208512025       16\n",
       "7        1332062720       15\n",
       "8        1839065905       15\n",
       "9        2207484445       20\n",
       "10       2231258628       45\n",
       "11       2708196140       16\n",
       "12       2785414965       16\n",
       "13       3506519971       16\n",
       "14       3597466554       15\n",
       "15       4093214354       45\n",
       "16       4103387787       31"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "# This is the equivalent calculation \n",
    "# Make sure I'm doing this, otherwise, counts won't match\n",
    "(max_stop_times.groupby([\"hqta_segment_id\", \"departure_hour\"])\n",
    " .agg({\"trip_id\": \"count\"})\n",
    " .reset_index()\n",
    " .groupby([\"hqta_segment_id\"])\n",
    " .agg({\"trip_id\": \"max\"})\n",
    " .reset_index()\n",
    ")\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "009015fa-32f9-4502-910b-762dcb8df527",
   "metadata": {},
   "source": [
    "## Tiff's method, use dask "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f5c88220-7e60-4d78-a739-c5d5fa141e32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import data\n",
    "FILE_PATH = f\"{rt_utils.GCS_FILE_PATH}cached_views/\"\n",
    "\n",
    "routelines = dask_geopandas.read_parquet(\n",
    "    f\"{FILE_PATH}routelines_{itp_id}_{date_str}.parquet\")\n",
    "trips = dd.read_parquet(f\"{FILE_PATH}trips_{itp_id}_{date_str}.parquet\")\n",
    "stop_times = dd.read_parquet(f\"{FILE_PATH}st_{itp_id}_{date_str}.parquet\")\n",
    "stops = dask_geopandas.read_parquet(f\"{FILE_PATH}stops_{itp_id}_{date_str}.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "f1155c4c-e7cb-42e9-b03e-87fe12563126",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1929/525762614.py:3: UserWarning: this is an initial implementation of Parquet/Feather file support and associated metadata.  This is tracking version 0.1.0 of the metadata specification at https://github.com/geopandas/geo-arrow-spec\n",
      "\n",
      "This metadata specification does not yet make stability promises.  We do not yet recommend using this in a production setting unless you are able to rewrite your Parquet/Feather files.\n",
      "\n",
      "To further ignore this warning, you can do: \n",
      "import warnings; warnings.filterwarnings('ignore', message='.*initial implementation of Parquet.*')\n"
     ]
    }
   ],
   "source": [
    "operator2 = bus_corridors.single_operator_hqta(routelines, trips, stop_times, stops)\n",
    "\n",
    "operator2[operator2.calitp_itp_id==itp_id].to_parquet(\"./data/tiff_720.parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25222874-804a-4f51-8391-2859599fee84",
   "metadata": {},
   "source": [
    "## Compare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "6ed43f5e-ea37-4e3e-b709-6e8a190fe950",
   "metadata": {},
   "outputs": [],
   "source": [
    "def stats(df):\n",
    "    df = df[df.shape_id.str.startswith(\"720\")]\n",
    "    print(df.columns)\n",
    "    print(df.dtypes)\n",
    "    print(f\"# obs: {len(df)}\")\n",
    "    print(f\"# unique stops: {df.stop_id.nunique()}\")\n",
    "    print(f\"sum am_max stops: {df.am_max_trips.sum()}\")\n",
    "    print(f\"sum pm_max stops: {df.am_max_trips.sum()}\")\n",
    "    print(f\"# unique hqta_segment_id: {df.hqta_segment_id.nunique()}\")\n",
    "\n",
    "    \n",
    "def compare(eric, tiff):\n",
    "    print(\"***********Eric************\")\n",
    "    stats(eric)\n",
    "    print(\"***********Tiff************\")\n",
    "    stats(tiff)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "cc78a2e4-8280-49bf-b3ee-838ccc6feba5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***********Eric************\n",
      "Index(['calitp_itp_id', 'calitp_url_number', 'shape_id', 'geometry',\n",
      "       'segment_sequence', 'hqta_segment_id', 'stop_id', 'am_max_trips',\n",
      "       'pm_max_trips', 'hq_transit_corr'],\n",
      "      dtype='object')\n",
      "calitp_itp_id           int64\n",
      "calitp_url_number       int64\n",
      "shape_id               object\n",
      "geometry             geometry\n",
      "segment_sequence       object\n",
      "hqta_segment_id         int64\n",
      "stop_id                object\n",
      "am_max_trips            int64\n",
      "pm_max_trips            int64\n",
      "hq_transit_corr          bool\n",
      "dtype: object\n",
      "# obs: 30\n",
      "# unique stops: 30\n",
      "sum am_max stops: 546\n",
      "sum pm_max stops: 546\n",
      "# unique hqta_segment_id: 30\n",
      "***********Tiff************\n",
      "Index(['calitp_itp_id', 'stop_id', 'hqta_segment_id', 'segment_sequence',\n",
      "       'calitp_url_number', 'shape_id', 'geometry', 'am_max_trips',\n",
      "       'pm_max_trips', 'hq_transit_corr'],\n",
      "      dtype='object')\n",
      "calitp_itp_id           int64\n",
      "stop_id                object\n",
      "hqta_segment_id         int64\n",
      "segment_sequence       object\n",
      "calitp_url_number       int64\n",
      "shape_id               object\n",
      "geometry             geometry\n",
      "am_max_trips            int64\n",
      "pm_max_trips            int64\n",
      "hq_transit_corr          bool\n",
      "dtype: object\n",
      "# obs: 25\n",
      "# unique stops: 23\n",
      "sum am_max stops: 429\n",
      "sum pm_max stops: 429\n",
      "# unique hqta_segment_id: 25\n"
     ]
    }
   ],
   "source": [
    "compare(operator, operator2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
