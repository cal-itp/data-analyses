{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8ecdefb3-5554-453d-bdcf-73f087e865a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.9/site-packages/geopandas/_compat.py:111: UserWarning: The Shapely GEOS version (3.9.1-CAPI-1.14.2) is incompatible with the GEOS version PyGEOS was compiled with (3.10.1-CAPI-1.16.0). Conversions between both will be slow.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import datetime as dt\n",
    "import geopandas as gpd\n",
    "import pandas as pd\n",
    "\n",
    "#import create_calenviroscreen_lehd_data\n",
    "import utils\n",
    "import shared_utils\n",
    "\n",
    "from siuba import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "222433ae-d007-4194-b074-7c3fa7c3d3b1",
   "metadata": {},
   "source": [
    "## Compare `shape_frequency`\n",
    "\n",
    "Break down each step and figure out why going into `shape_frequency_funding` the observations differ so much."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55ad9568-a62b-4c37-9474-0b83d8d7c6c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH = \"./data/test/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a6e4e4a-84b5-4ef6-8c40-a0673dd0f2e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_shape_frequency(DATA_PATH):\n",
    "    df = pd.read_parquet(f\"{DATA_PATH}shape_frequency.parquet\")\n",
    "    print(f\"# obs: {len(df)}\")\n",
    "    \n",
    "    group_cols = [\"calitp_itp_id\", \"day_name\"]\n",
    "    df2 = df.groupby(group_cols).agg({\"shape_id\": \"nunique\"}).reset_index()\n",
    "    print(f\"# unique id-shape_id-day-name: {len(df2)}\")\n",
    "    \n",
    "    group_cols = [\"calitp_itp_id\", \"day_name\"]\n",
    "    df3 = df.groupby(group_cols).agg({\"route_id\": \"nunique\"}).reset_index()\n",
    "    print(f\"# unique id-route-id-day-name: {len(df2)}\")\n",
    "\n",
    "    check_ids = [182, 4, 279]\n",
    "    print(f\"check specific ids: {check_ids}\")\n",
    "    print(\"Unique shape_id-day_name\")\n",
    "    \n",
    "    display(df2[df2.calitp_itp_id.isin(check_ids)])\n",
    "    print(\"Unique route_id-day_name\")\n",
    "    display(df3[df3.calitp_itp_id.isin(check_ids)])\n",
    "    \n",
    "    return df2, df3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef29174d-70fc-4d6b-96f4-6fd9397b3e44",
   "metadata": {},
   "outputs": [],
   "source": [
    "tiff_shape, tiff_route = compare_shape_frequency(\"./data/test/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4435307f-1fe9-4a47-87ae-83f350b11567",
   "metadata": {},
   "outputs": [],
   "source": [
    "eric_shape, eric_route = compare_shape_frequency(f\"{utils.GCS_FILE_PATH}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99e224e0-2568-467a-8a09-868aae4cb5b5",
   "metadata": {},
   "source": [
    "Interestingly, Eric pulls same number of route_ids for each day_name, but mine fluctuates.\n",
    "\n",
    "Check which line in the code does that...must be something like `groupby.agg` that includes a `day_name` in there and counts or finds nunique `route_id` or a `drop_duplicates` that gives this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99ccab90-c150-461d-99dd-b6213ca5de97",
   "metadata": {},
   "outputs": [],
   "source": [
    "tiff_shape = pd.read_parquet(f\"{DATA_PATH}shape_frequency.parquet\")\n",
    "eric_shape = pd.read_parquet(f\"{utils.GCS_FILE_PATH}shape_frequency.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c72c5752-1ee0-46ae-9e20-20e136fcdbe9",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(tiff_shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86d8826c-55ee-44bb-a2e2-b7bc24fabea7",
   "metadata": {},
   "outputs": [],
   "source": [
    "tiff_shape[tiff_shape.mean_runtime_min.notna()].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c18ab8ba-e212-4dee-93b8-3760c91046e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(eric_shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b53deda1-4355-417c-9aa1-f4eaaabc5848",
   "metadata": {},
   "outputs": [],
   "source": [
    "eric_shape[eric_shape.mean_runtime_min.notna()].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9f7a343-175a-496c-adfa-1bd2ca99a306",
   "metadata": {},
   "source": [
    "### Go up to time calculations.\n",
    "\n",
    "Look at LA Metro on Thurs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6502adfb-1c73-42f4-a1cd-c39941462e45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the query from warehouse_queries to get just LA Metro on Thurs\n",
    "DATA_PATH = \"./data/test/\"\n",
    "\n",
    "eric = pd.read_parquet(f\"{DATA_PATH}eric_trips_joined_thurs.parquet\")\n",
    "tiff = pd.read_parquet(f\"{DATA_PATH}trips_thurs.parquet\")\n",
    "tiff = tiff[tiff.calitp_itp_id==182].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cbfbd88-5ca5-464c-ab0e-fc00befd088c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if trip keys are the same...they really should be\n",
    "# --> So we have the same trip_keys\n",
    "\n",
    "trip_merge = pd.merge(eric[[\"trip_key\"]].drop_duplicates(),\n",
    "                      tiff[[\"trip_key\"]].drop_duplicates(),\n",
    "                      on = \"trip_key\",\n",
    "                      how = \"outer\",\n",
    "                      validate = \"1:1\",\n",
    "                      indicator=True)\n",
    "\n",
    "trip_merge._merge.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b2a2820-c4d3-44ab-8490-d5f857ef69ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_time_calculations(df):\n",
    "    ## time calculations\n",
    "    df = df.assign(\n",
    "        date = pd.to_datetime(df.date),\n",
    "        departure_time = df.departure_time.dropna().apply(utils.fix_gtfs_time),\n",
    "    )\n",
    "\n",
    "    # Something weird comes up trying to generate departure_dt\n",
    "    # pd.to_datetime() gives today's date\n",
    "    # datetime.strptime gives year 1900\n",
    "    # Either way, we have the service date, and later subsetting between 5am-9pm will address this\n",
    "    df = df.assign(\n",
    "        departure_time = pd.to_datetime(df.departure_time),\n",
    "        departure_hour = pd.to_datetime(df.departure_time).dt.hour,\n",
    "    )\n",
    "    \n",
    "    # Any observation with NaTs for departure time get dropped\n",
    "    # Will create issue later when grouping is done with departure hour\n",
    "    df = df[df.departure_time.notna()].reset_index(drop=True)\n",
    "    \n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3331487d-22e3-464c-ad85-537fc40f8553",
   "metadata": {},
   "outputs": [],
   "source": [
    "eric2 = get_time_calculations(eric)\n",
    "tiff2 = get_time_calculations(tiff)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc5f3577-1986-47cc-ada0-aab45690abb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from shared_utils import geography_utils\n",
    "\n",
    "# Aggregate by departure hour, these results should also be the same\n",
    "eric_hour = geography_utils.aggregate_by_geography(eric2,\n",
    "                                       group_cols = [\"departure_hour\"],\n",
    "                                       nunique_cols = [\"route_id\", \"trip_key\"]\n",
    "                                      )\n",
    "\n",
    "tiff_hour = geography_utils.aggregate_by_geography(tiff2,\n",
    "                                       group_cols = [\"departure_hour\"],\n",
    "                                       nunique_cols = [\"route_id\", \"trip_key\"]\n",
    "                                      )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "597d56f7-8bfc-4022-8463-87695a1ac19b",
   "metadata": {},
   "outputs": [],
   "source": [
    "hour_merge = pd.merge(eric_hour,\n",
    "                      tiff_hour,\n",
    "                      on = \"departure_hour\",\n",
    "                      how = \"outer\",\n",
    "                      validate = \"1:1\",\n",
    "                      indicator=True\n",
    "                     )\n",
    "\n",
    "hour_merge._merge.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e985d282-b5d5-49e7-ab41-8849952579fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# These assert statements pass, as they should\n",
    "assert (hour_merge.route_id_x == hour_merge.route_id_y).all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4651fcb4-5396-4ddb-8c6e-c539de93c4d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert (hour_merge.trip_key_x == hour_merge.trip_key_y).all()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "729d9f90-82c3-45d5-98d6-693ff3ad200f",
   "metadata": {},
   "source": [
    "### Check calculation of runtimes \n",
    "\n",
    "Probably somewhere in this code is where the datasets differ\n",
    "\n",
    "Break out each step and double check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1657fd0f-d6b3-4373-98d0-bbd3bcbb2db8",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Eric's code\n",
    "\n",
    "# Don't think swapping this out will have affected anything\n",
    "# But just leave this intermediate step in anyway,\n",
    "# just to get the find_runtime function to run, then double check results\n",
    "\n",
    "def beginning_time_cleaning(df):\n",
    "    df = df.dropna(subset=['departure_time'])\n",
    "    # _st_trips_joined = st_trips_joined\n",
    "    df.departure_time = df.departure_time.apply(utils.fix_gtfs_time)\n",
    "    df['departure_dt'] = (df['departure_time']\n",
    "                                 .apply(lambda x:\n",
    "                                        dt.datetime.strptime(x, '%H:%M:%S'))\n",
    "                                      )\n",
    "    df['departure_hour'] = df['departure_dt'].apply(lambda x: x.hour)\n",
    "    \n",
    "    return df\n",
    "\n",
    "eric_time = beginning_time_cleaning(eric)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3aa0ca4-69fd-4f36-a04e-8e43ade73bcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_runtime(df):\n",
    "    mindt = df[df.stop_sequence == df.stop_sequence.min()].departure_dt.iloc[0]\n",
    "    maxdt = df[df.stop_sequence == df.stop_sequence.max()].departure_dt.iloc[0]\n",
    "    td = (maxdt - mindt)\n",
    "    df['runtime_seconds'] = td.seconds\n",
    "    return df\n",
    "    \n",
    "# try:\n",
    "st_with_runtimes = eric_time.groupby(['trip_key', 'day_name']).apply(find_runtime)\n",
    "eric3 = st_with_runtimes >> select(_.trip_key, _.day_name, _.runtime_seconds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8e54a65-7b54-4d04-9c7c-374e073f7cee",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b7e6a23-8521-45c1-8b46-2f01269c1e65",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Tiff's code\n",
    "\n",
    "def calculate_runtime_hourlytrips_part1(df):\n",
    "    # Calculate run time for a trip\n",
    "    # Find the median stop (keep that observation)\n",
    "    group_cols = ['trip_key', 'day_name']\n",
    "    df = df.assign(\n",
    "        mindt = df.groupby(group_cols)[\"departure_time\"].transform(\"min\"),\n",
    "        maxdt = df.groupby(group_cols)[\"departure_time\"].transform(\"max\"),\n",
    "        middle_stop = df.groupby([\"trip_key\", \"day_name\"])[\"stop_sequence\"].transform(\"median\"),\n",
    "    ).astype({\"middle_stop\": \"int64\"})\n",
    "\n",
    "    df = df.assign(\n",
    "        runtime_seconds = (df.maxdt - df.mindt).dt.seconds\n",
    "    ).drop(columns = [\"mindt\", \"maxdt\"])\n",
    "    \n",
    "    # Drop any trips with runtime of NaN calculated\n",
    "    df = df[df.runtime_seconds.notna()].reset_index(drop=True)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e2b9474-fb02-4edc-ae7c-d6acaf5dae4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "tiff3 = calculate_runtime_hourlytrips_part1(tiff2)\n",
    "tiff3  = tiff3 >> select(_.trip_key, _.day_name, _.runtime_seconds) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fc9c215-7d39-460c-9eb0-fb24f382bac6",
   "metadata": {},
   "outputs": [],
   "source": [
    "runtime_merge = pd.merge(eric3.drop_duplicates(), \n",
    "                         tiff3.drop_duplicates(), \n",
    "                         on = [\"trip_key\", \"day_name\"],\n",
    "                         how = \"outer\",\n",
    "                         validate = \"1:1\",\n",
    "                         indicator=True\n",
    "                        )\n",
    "\n",
    "runtime_merge._merge.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b766681-2869-418b-85b4-d67a6060a5d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This assert statement does not pass\n",
    "# This would be due to the fact that we have different approaches\n",
    "# Dig into this\n",
    "assert (runtime_merge.runtime_seconds_x == runtime_merge.runtime_seconds_y).all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bcd7f3f-7026-4b3c-bba2-9054c69555ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "runtime_debug = runtime_merge[runtime_merge.runtime_seconds_x != \n",
    "                              runtime_merge.runtime_seconds_y\n",
    "                             ]\n",
    "\n",
    "print(f\"# obs that differ: {len(runtime_debug)}\")\n",
    "print(f\"% obs that differ: {len(runtime_debug) / len(runtime_merge)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a495017-6e28-4c20-bf1b-072c26c3410c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1.2% of the observations differ...that's not a big deal at all\n",
    "# BUT, why are my runtime_seconds SO HUGE\n",
    "# Pull out these trip_keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "765a0f7d-2813-4ca8-9f49-3a859b4b3852",
   "metadata": {},
   "outputs": [],
   "source": [
    "runtime_debug.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b784fc8-7498-445a-bb1c-8445ca0de95b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ahh, this has to do with crossing over the midnight hour\n",
    "# So, it's not correctly deriving the time\n",
    "tiff2[tiff2.trip_key.isin(runtime_debug.trip_key)].departure_hour.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5c92f8b-380d-4565-ad22-1c2437e2a638",
   "metadata": {},
   "outputs": [],
   "source": [
    "keep_trip = [-5414324691387759528]\n",
    "one_trip = tiff2[tiff2.trip_key.isin(keep_trip)]\n",
    "\n",
    "one_trip[(one_trip.stop_sequence==one_trip.stop_sequence.max()) |\n",
    "         (one_trip.stop_sequence==one_trip.stop_sequence.min())\n",
    "        ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9af1f9a5-2abe-44a8-8e51-bab877eea358",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_runtime_modified(df):\n",
    "    mindt = df[df.stop_sequence == df.stop_sequence.min()].departure_time.iloc[0]\n",
    "    maxdt = df[df.stop_sequence == df.stop_sequence.max()].departure_time.iloc[0]\n",
    "    td = (maxdt - mindt)\n",
    "    df['runtime_seconds'] = td.seconds\n",
    "    return df\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fadce43-5c63-4d37-8925-06a039a1f8c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Go back to this method, because this correctly deals with crossing midnight\n",
    "one_trip2 = find_runtime_modified(one_trip)\n",
    "one_trip2[(one_trip2.stop_sequence==one_trip2.stop_sequence.max()) |\n",
    "         (one_trip2.stop_sequence==one_trip2.stop_sequence.min())\n",
    "        ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abeafbe6-aa44-49d6-ba77-e2d34f7167df",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_runtime_hourlytrips_part1_modified(df):\n",
    "    # Calculate run time for a trip\n",
    "    # Find the median stop (keep that observation)\n",
    "    \n",
    "    def find_runtime(df):\n",
    "        mindt = df[df.stop_sequence == df.stop_sequence.min()].departure_time.iloc[0]\n",
    "        maxdt = df[df.stop_sequence == df.stop_sequence.max()].departure_time.iloc[0]\n",
    "        td = (maxdt - mindt)\n",
    "        df['runtime_seconds'] = td.seconds\n",
    "        return df\n",
    "\n",
    "    group_cols = ['trip_key', 'day_name']\n",
    "    df = df.groupby(group_cols).apply(find_runtime)\n",
    "\n",
    "    df = df.assign(\n",
    "        middle_stop = df.groupby(group_cols)[\"stop_sequence\"].transform(\"median\"),\n",
    "    ).astype({\"middle_stop\": \"int64\"})\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8bd3425-d5d4-4a53-852a-f5c0d45477d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "tiff3 = calculate_runtime_hourlytrips_part1_modified(tiff2)\n",
    "tiff3  = tiff3 >> select(_.trip_key, _.day_name, _.runtime_seconds) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19c7638a-fd91-41b9-b7eb-5ad4e925baf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "runtime_merge = pd.merge(eric3.drop_duplicates(), \n",
    "                         tiff3.drop_duplicates(), \n",
    "                         on = [\"trip_key\", \"day_name\"],\n",
    "                         how = \"outer\",\n",
    "                         validate = \"1:1\",\n",
    "                         indicator=True\n",
    "                        )\n",
    "\n",
    "runtime_merge._merge.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "453aa21e-7fe0-4157-85c3-8d2a809db809",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now the assert passes\n",
    "assert (runtime_merge.runtime_seconds_x == runtime_merge.runtime_seconds_y).all()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cad06ee5-a671-432c-86ca-a335c8f9b698",
   "metadata": {},
   "source": [
    "### Go onto middle stops\n",
    "\n",
    "Make sure the `mean(runtime)` is the same"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "d9c43596-2773-4db4-9500-74293c74a8d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Eric's code\n",
    "\n",
    "def eric_part2(st_trips_joined):\n",
    "    st_trips_joined = st_trips_joined.dropna(subset=['departure_time'])\n",
    "    # _st_trips_joined = st_trips_joined\n",
    "    st_trips_joined.departure_time = st_trips_joined.departure_time.apply(utils.fix_gtfs_time)\n",
    "    st_trips_joined['departure_dt'] = (st_trips_joined['departure_time']\n",
    "                                 .apply(lambda x:\n",
    "                                        dt.datetime.strptime(x, '%H:%M:%S'))\n",
    "                                      )\n",
    "    st_trips_joined['departure_hour'] = st_trips_joined['departure_dt'].apply(lambda x: x.hour)\n",
    "    \n",
    "    ## calculate runtimes for each trip, if possible\n",
    "    def find_runtime(df):\n",
    "        mindt = df[df.stop_sequence == df.stop_sequence.min()].departure_dt.iloc[0]\n",
    "        maxdt = df[df.stop_sequence == df.stop_sequence.max()].departure_dt.iloc[0]\n",
    "        td = (maxdt - mindt)\n",
    "        df['runtime_seconds'] = td.seconds\n",
    "        return df\n",
    "    \n",
    "    st_with_runtimes = st_trips_joined.groupby(['trip_key', 'day_name']).apply(find_runtime)\n",
    "    st_with_runtimes = st_with_runtimes >> select(_.trip_key, _.day_name, _.runtime_seconds)\n",
    "    \n",
    "    \n",
    "    ## find middle stop for each trip to calculate frequencies\n",
    "    middle_stops = (st_trips_joined\n",
    "                    >> group_by(_.calitp_itp_id, _.shape_id)\n",
    "                    >> summarize(middle_stop = _.stop_sequence.median())\n",
    "                   )\n",
    "\n",
    "\n",
    "    middle_stops.middle_stop = middle_stops.middle_stop.astype('int64')\n",
    "    \n",
    "    # try:\n",
    "    ## this logic here is confusing, with middle_st and middle_st_runtimes both having inner joins with st_trips_joined\n",
    "    # When this is abstracted to all operators, creates more issues\n",
    "    # probably need more groupby columns\n",
    "    middle_st = (middle_stops\n",
    "                 >> select(_.stop_sequence == _.middle_stop, _.shape_id)\n",
    "                 >> inner_join(_, st_trips_joined, on=['shape_id', 'stop_sequence'])\n",
    "                )\n",
    "    \n",
    "    \n",
    "    ## if multiple trips within the hour, calculate mean runtime\n",
    "    middle_st_runtimes = (middle_stops\n",
    "         >> inner_join(_, st_trips_joined, on=['trip_key', 'day_name'\n",
    "                                              ])\n",
    "            # change the groupby here, just because we'll have cal_itp_id_x\n",
    "         >> group_by(_.calitp_itp_id, _.route_id, _.shape_id, \n",
    "                     _.departure_hour, _.day_name)\n",
    "         >> summarize(mean_runtime_sec = _.runtime_seconds.mean())\n",
    "         )\n",
    "\n",
    "    middle_st_runtimes['mean_runtime_min'] = (middle_st_runtimes.mean_runtime_sec\n",
    "                                              .apply(lambda x: int(round(x) / 60))\n",
    "                                             )\n",
    "    \n",
    "    return middle_st_runtimes\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "acccd812-41ab-4f95-8042-ad3316ce42b4",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'trip_key'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_1272/518650935.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0meric4\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0meric_part2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0meric\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/tmp/ipykernel_1272/1815531854.py\u001b[0m in \u001b[0;36meric_part2\u001b[0;34m(st_trips_joined)\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[0;31m## if multiple trips within the hour, calculate mean runtime\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 43\u001b[0;31m     middle_st_runtimes = (middle_stops\n\u001b[0m\u001b[1;32m     44\u001b[0m          >> inner_join(_, st_trips_joined, on=['trip_key', 'day_name'\n\u001b[1;32m     45\u001b[0m                                                \u001b[0;34m'calitp_itp_id'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'route_id'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.9/site-packages/siuba/dply/verbs.py\u001b[0m in \u001b[0;36m__rrshift__\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     97\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mPipeable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcalls\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcalls\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 99\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    100\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.9/site-packages/siuba/dply/verbs.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    102\u001b[0m         \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    103\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mf\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcalls\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 104\u001b[0;31m             \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mres\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    105\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mres\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    106\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.9/site-packages/siuba/siu.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    201\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0moperator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minst\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mrest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    202\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunc\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__call__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 203\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minst\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mrest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    204\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    205\u001b[0m         \u001b[0;31m# in normal case, get method to call, and then call it\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.9/functools.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kw)\u001b[0m\n\u001b[1;32m    875\u001b[0m                             '1 positional argument')\n\u001b[1;32m    876\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 877\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mdispatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    878\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    879\u001b[0m     \u001b[0mfuncname\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'__name__'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'singledispatch function'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.9/site-packages/siuba/dply/verbs.py\u001b[0m in \u001b[0;36mjoin\u001b[0;34m(left, right, on, how, by, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1081\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mleft\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmerge\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mright\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhow\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhow\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mleft_on\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mleft_on\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mright_on\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mright_on\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1082\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1083\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mleft\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmerge\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mright\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhow\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhow\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mon\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mon\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1084\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1085\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.9/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36mmerge\u001b[0;34m(self, right, how, on, left_on, right_on, left_index, right_index, sort, suffixes, copy, indicator, validate)\u001b[0m\n\u001b[1;32m   9327\u001b[0m         \u001b[0;32mfrom\u001b[0m \u001b[0mpandas\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmerge\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmerge\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   9328\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 9329\u001b[0;31m         return merge(\n\u001b[0m\u001b[1;32m   9330\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   9331\u001b[0m             \u001b[0mright\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.9/site-packages/pandas/core/reshape/merge.py\u001b[0m in \u001b[0;36mmerge\u001b[0;34m(left, right, how, on, left_on, right_on, left_index, right_index, sort, suffixes, copy, indicator, validate)\u001b[0m\n\u001b[1;32m    105\u001b[0m     \u001b[0mvalidate\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    106\u001b[0m ) -> DataFrame:\n\u001b[0;32m--> 107\u001b[0;31m     op = _MergeOperation(\n\u001b[0m\u001b[1;32m    108\u001b[0m         \u001b[0mleft\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    109\u001b[0m         \u001b[0mright\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.9/site-packages/pandas/core/reshape/merge.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, left, right, how, on, left_on, right_on, axis, left_index, right_index, sort, suffixes, copy, indicator, validate)\u001b[0m\n\u001b[1;32m    698\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mright_join_keys\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    699\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin_names\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 700\u001b[0;31m         ) = self._get_merge_keys()\n\u001b[0m\u001b[1;32m    701\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    702\u001b[0m         \u001b[0;31m# validate the merge keys dtypes. We may need to coerce\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.9/site-packages/pandas/core/reshape/merge.py\u001b[0m in \u001b[0;36m_get_merge_keys\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1108\u001b[0m                         \u001b[0mright_keys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1109\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0mlk\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1110\u001b[0;31m                         \u001b[0mleft_keys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mleft\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_label_or_level_values\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1111\u001b[0m                         \u001b[0mjoin_names\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1112\u001b[0m                     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.9/site-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36m_get_label_or_level_values\u001b[0;34m(self, key, axis)\u001b[0m\n\u001b[1;32m   1846\u001b[0m             \u001b[0mvalues\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maxes\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_level_values\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_values\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1847\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1848\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1849\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1850\u001b[0m         \u001b[0;31m# Check for duplicates\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'trip_key'"
     ]
    }
   ],
   "source": [
    "eric4 = eric_part2(eric)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd5489b7-557e-450f-83c9-b38cdfdc6285",
   "metadata": {},
   "outputs": [],
   "source": [
    "eric4.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "090922b7-f1d3-4cf9-bbc4-909ca11417b8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c883026d-cad7-4b45-ac7c-a66b97eff212",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Tiffany's code\n",
    "def calculate_runtime_hourlytrips_part2(df):\n",
    "    \n",
    "    # Incorporate part 1 from above\n",
    "    \n",
    "    # Calculate run time for a trip\n",
    "    # Find the median stop (keep that observation)\n",
    "    \n",
    "    def find_runtime(df):\n",
    "        mindt = df[df.stop_sequence == df.stop_sequence.min()].departure_time.iloc[0]\n",
    "        maxdt = df[df.stop_sequence == df.stop_sequence.max()].departure_time.iloc[0]\n",
    "        td = (maxdt - mindt)\n",
    "        df['runtime_seconds'] = td.seconds\n",
    "        return df\n",
    "\n",
    "    group_cols = ['trip_key', 'day_name']\n",
    "    df = df.groupby(group_cols).apply(find_runtime)\n",
    "\n",
    "    df = df.assign(\n",
    "        middle_stop = df.groupby(group_cols)[\"stop_sequence\"].transform(\"median\"),\n",
    "    ).astype({\"middle_stop\": \"int64\"})\n",
    "    \n",
    "    df = df[df.runtime_seconds.notna()].reset_index(drop=True)\n",
    "\n",
    "    # Still want to use this to merge on the mean runtime info\n",
    "    middle_stops = df >> filter(_.stop_sequence == _.middle_stop)\n",
    "\n",
    "    middle_stops = middle_stops.assign(\n",
    "        mean_runtime_min = (middle_stops.groupby([\"calitp_itp_id\", \n",
    "                                                  \"route_id\", \"shape_id\", \n",
    "                                                  \"departure_hour\", \"day_name\"])\n",
    "                            [\"runtime_seconds\"].transform(\"mean\")\n",
    "                           )\n",
    "    )\n",
    "    \n",
    "    return middle_stops    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01806de9-ce63-498f-829a-301c230a58f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "tiff4 = calculate_runtime_hourlytrips_part2(tiff2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0812fbf8-b753-4442-aa6a-8caa7964df5f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb6643ac-8a6f-40bd-8cb8-d1555f37c3c3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9445f49-045b-498d-8aa2-39aa16892c7e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d32767d9-090d-4a1f-bc92-f1a42673cc35",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "108e3941-d861-408b-a559-d9a91211bae4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modify this as you find errors\n",
    "\n",
    "def calculate_runtime_hourlytrips_NEW(df):\n",
    "    # Calculate run time for a trip\n",
    "    # Find the median stop (keep that observation)\n",
    "    \n",
    "    def find_runtime(df):\n",
    "        mindt = df[df.stop_sequence == df.stop_sequence.min()].departure_time.iloc[0]\n",
    "        maxdt = df[df.stop_sequence == df.stop_sequence.max()].departure_time.iloc[0]\n",
    "        td = (maxdt - mindt)\n",
    "        df['runtime_seconds'] = td.seconds\n",
    "        return df\n",
    "\n",
    "    group_cols = ['trip_key', 'day_name']\n",
    "    df = df.groupby(group_cols).apply(find_runtime)\n",
    "\n",
    "    df = df.assign(\n",
    "        middle_stop = df.groupby(group_cols)[\"stop_sequence\"].transform(\"median\"),\n",
    "    ).astype({\"middle_stop\": \"int64\"})\n",
    "    \n",
    "    ## Left off here\n",
    "    \n",
    "    \n",
    "    # Drop any trips with runtime of NaN calculated\n",
    "    df = df[df.runtime_seconds.notna()].reset_index(drop=True)\n",
    "\n",
    "    # Still want to use this to merge on the mean runtime info\n",
    "    middle_stops = df >> filter(_.stop_sequence == _.middle_stop)\n",
    "    \n",
    "    middle_stops = middle_stops.assign(\n",
    "        mean_runtime_min = (middle_stops.groupby([\"calitp_itp_id\", \n",
    "                                                  \"route_id\", \"shape_id\", \n",
    "                                                  \"departure_hour\", \"day_name\"])\n",
    "                            [\"runtime_seconds\"].transform(\"mean\")\n",
    "                           )\n",
    "    )\n",
    "    \n",
    "    debug_me = middle_stops[middle_stops.mean_runtime_min.isna()][\n",
    "        [\"calitp_itp_id\", \"shape_id\", \"trip_key\"]]\n",
    "    print(\"Debug errors for NaN mean runtimes\")\n",
    "    print(debug_me.head())\n",
    "    # Why are there some NaNs from this, when NaNs were dropped before?\n",
    "    # Some are due to no departure_time (handle it above by dropping NaTs)\n",
    "    \n",
    "    middle_stops = middle_stops.assign(\n",
    "        mean_runtime_min = (middle_stops.mean_runtime_min.dropna()     \n",
    "                            .apply(lambda x: int(round(x) / 60))\n",
    "                           )\n",
    "    )   \n",
    "    \n",
    "    # Add trips per hour column\n",
    "    shape_frequency = (\n",
    "        middle_stops\n",
    "        >> count(_.calitp_itp_id, _.route_id,\n",
    "                 _.shape_id, _.departure_hour, _.day_name, sort = True)\n",
    "        >> rename(trips_per_hour = \"n\")\n",
    "        >> inner_join(_, middle_stops, \n",
    "                      on = [\"calitp_itp_id\", \"day_name\", \n",
    "                            \"shape_id\", \"departure_hour\", \"route_id\"])\n",
    "    )\n",
    "    \n",
    "    # Now, data is at the trip-level (trip_key) still present\n",
    "    # Drop duplicates, but no aggregation because trips_per_hour and mean_runtime \n",
    "    # are already correctly generated at the route-level, across trips in that departure hour\n",
    "    shape_frequency = shape_frequency.drop_duplicates(subset=[\n",
    "        \"calitp_itp_id\", \"shape_id\", \"departure_hour\",\n",
    "        \"day_name\", \"route_id\"])\n",
    "    \n",
    "    # There's an aggregation to deal with multiple route_ids that share same shape_id\n",
    "    # If there are still multiple route_ids, then aggregate and sum / mean\n",
    "    # Modify this to include itp_id into the groupby\n",
    "    shape_frequency2 = (shape_frequency.groupby(\n",
    "        [\"calitp_itp_id\", \"shape_id\", \"departure_hour\", \"day_name\"])\n",
    "                        .agg({\"route_id\": \"max\", \n",
    "                              \"trips_per_hour\": \"sum\", \n",
    "                              \"mean_runtime_min\": \"mean\"\n",
    "                             }).reset_index()\n",
    "                       )\n",
    "    \n",
    "    # Now, drop ITP_ID==200 to use individual operator feeds\n",
    "    shape_frequency3 = shape_frequency2 >> filter(_.calitp_itp_id != 200)\n",
    "    \n",
    "    return shape_frequency3\n",
    "\n",
    "\n",
    "def attach_funding(all_operators_df):\n",
    "    # This is a small query, can leave it here\n",
    "    with_funding = (tbl.views.transitstacks()\n",
    "                    >> select(_.calitp_itp_id == _.itp_id, _.ntd_id, \n",
    "                              _.transit_provider, _._5307_funds, _._5311_funds,\n",
    "                              _.operating_expenses_total_2019)\n",
    "                    >> collect()\n",
    "                    >> right_join(_, all_operators_df, on = 'calitp_itp_id')\n",
    "                   )\n",
    "    \n",
    "    def fix_funds(value):\n",
    "        if type(value) != str:\n",
    "            return None\n",
    "        else:\n",
    "            return int(value.replace('$', '').replace(',', ''))\n",
    "        \n",
    "    funding_cols = [\"_5307_funds\", \"_5311_funds\", \"operating_expenses_total_2019\"] \n",
    "    for c in funding_cols:\n",
    "        with_funding[c] = with_funding[c].apply(fix_funds)\n",
    "    \n",
    "    return with_funding\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a9baabb-8da6-44b6-ba00-d713c5c8dc5f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3f8962f-0c31-4a10-9ee9-225c989193a7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a05c1ab-c339-4153-b06c-8c6acb1a08e1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
