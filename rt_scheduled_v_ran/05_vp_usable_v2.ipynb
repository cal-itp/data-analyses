{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7030d65c-ea3f-4ee7-a6cd-9ed8267bd74e",
   "metadata": {},
   "source": [
    "## Improving on Script\n",
    "* Feedback: https://github.com/cal-itp/data-analyses/pull/961"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5aa8180f-9df6-4dad-a13e-7dfeb60f4fc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "\n",
    "import dask\n",
    "import dask.dataframe as dd\n",
    "import geopandas as gpd\n",
    "import pandas as pd\n",
    "from calitp_data_analysis.geography_utils import WGS84\n",
    "from loguru import logger\n",
    "from scripts import vp_spatial_accuracy\n",
    "from segment_speed_utils import helpers, wrangle_shapes\n",
    "from segment_speed_utils.project_vars import (\n",
    "    GCS_FILE_PATH,\n",
    "    PROJECT_CRS,\n",
    "    SEGMENT_GCS,\n",
    "    analysis_date,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b0f37d0e-12b3-42e7-b4f4-f8cfbc42e7d5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2023-11-15'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "analysis_date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3f056423-3ece-44c1-8c2d-b8ac4d9bd5a2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'gs://calitp-analytics-data/data-analyses/rt_segment_speeds/'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SEGMENT_GCS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cf62db29-f6e9-4d3d-90b5-00f57a05e812",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'gs://calitp-analytics-data/data-analyses/rt_vs_schedule/'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f\"{GCS_FILE_PATH}rt_vs_schedule/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e64d4bd2-73ed-4636-9067-a459d1ead87d",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.options.display.max_columns = 100\n",
    "pd.options.display.float_format = \"{:.2f}\".format\n",
    "pd.set_option(\"display.max_rows\", None)\n",
    "pd.set_option(\"display.max_colwidth\", None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cf1e8503-14f1-46bf-a4ed-9d9a40c0ff95",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scripts import update_vars2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a0e413f6-5de3-4e4d-9d42-2582d1043fbc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['2023-04-12', '2023-10-11', '2023-09-13']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "update_vars2.analysis_date_list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "803ca629-4e90-49fc-a2d6-793a600d1ef4",
   "metadata": {},
   "source": [
    "### Filter columns \n",
    "* ['trip_instance_key', 'location_timestamp_local', 'x','y','vp_idx']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cdc3d25e-b9a0-436e-9556-9fbbc1e0c5cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "operator = \"Bay Area 511 Muni VehiclePositions\"\n",
    "gtfs_key = \"7cc0cb1871dfd558f11a2885c145d144\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4ba298df-5f05-4840-9258-34e78da24468",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_vp_usable(analysis_date):\n",
    "\n",
    "    # Delete schedule_gtfs_dataset_key later\n",
    "    df = dd.read_parquet(\n",
    "        f\"{SEGMENT_GCS}vp_usable_{analysis_date}\",\n",
    "        columns=[\n",
    "            \"schedule_gtfs_dataset_key\",\n",
    "            \"trip_instance_key\",\n",
    "            \"location_timestamp_local\",\n",
    "            \"x\",\n",
    "            \"y\",\n",
    "            \"vp_idx\",\n",
    "        ],\n",
    "    )\n",
    "\n",
    "    # Create a copy of location timestamp for the total_trip_time function\n",
    "    # to avoid type setting\n",
    "    df[\"max_time\"] = df.location_timestamp_local\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1dafdbaa-b4ec-4674-85b5-ca33883db605",
   "metadata": {},
   "outputs": [],
   "source": [
    "vp_usable = load_vp_usable(analysis_date)\n",
    "# Filter for now\n",
    "vp_usable2 = vp_usable.loc[vp_usable.schedule_gtfs_dataset_key == gtfs_key].reset_index(\n",
    "    drop=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "00ef17f9-f0a7-4ff5-836c-ba0c0fd7f8d8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['schedule_gtfs_dataset_key', 'trip_instance_key',\n",
       "       'location_timestamp_local', 'x', 'y', 'vp_idx', 'max_time'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vp_usable.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "539f1010-133e-4d5c-82b7-b5bc1d016aae",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2fd48c34-c5e1-439c-8b6a-4fdd05766250",
   "metadata": {},
   "outputs": [],
   "source": [
    "# vp_usable2 = vp_usable2.compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d6a5c0bd-3afa-4122-a7e2-0e309be30aa0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dask.dataframe.core.DataFrame"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(vp_usable2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "20d060a4-bb22-41a5-897f-c8ebcc7262d5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dask Series Structure:\n",
       "npartitions=1\n",
       "    object\n",
       "       ...\n",
       "Name: schedule_gtfs_dataset_key, dtype: object\n",
       "Dask Name: unique-agg, 10 graph layers"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vp_usable2.schedule_gtfs_dataset_key.unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e66a0806-a9a4-4075-a300-b2d924fda0ed",
   "metadata": {},
   "source": [
    "### Total Trip Time\n",
    "* Addresses \"<i>in this function, min_time, max_time are created on the grouped df (vp_usable grouped by trip and binned minute)...I think to be safer, it should be created on vp_usable grouped by trip.</i>\"\n",
    "* The copy setting is now turned on?? How to get it to go away?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0eec9521-7049-4c91-9371-6198d0362c60",
   "metadata": {},
   "outputs": [],
   "source": [
    "def total_trip_time(vp_usable_df: pd.DataFrame):\n",
    "    \"\"\"\n",
    "    For each trip: find the total service minutes\n",
    "    recorded in real time data so we can compare it with\n",
    "    scheduled service minutes.\n",
    "    \"\"\"\n",
    "    subset = [\"location_timestamp_local\", \"trip_instance_key\", \"max_time\"]\n",
    "    vp_usable_df = vp_usable_df[subset]\n",
    "\n",
    "    # Need an extra copy of the column to find the max\n",
    "\n",
    "    # Find the max and the min time based on location timestamp\n",
    "    df = (\n",
    "        vp_usable_df.groupby([\"trip_instance_key\"])\n",
    "        .agg({\"location_timestamp_local\": \"min\", \"max_time\": \"max\"})\n",
    "        .reset_index()\n",
    "        .rename(columns={\"location_timestamp_local\": \"min_time\"})\n",
    "    )\n",
    "\n",
    "    # Find total rt service mins and add an extra minute\n",
    "    df[\"rt_service_min\"] = (df.max_time - df.min_time) / pd.Timedelta(minutes=1) + 1\n",
    "\n",
    "    # Return only one row per trip with the total trip time\n",
    "    df = df.drop(columns=[\"max_time\", \"min_time\"])\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68084bc8-79e3-4fe0-a409-9610e68d32c6",
   "metadata": {},
   "source": [
    "#### Change in script: remove map partitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "214072aa-3c42-46eb-b835-527c6b47da0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-12-13 11:30:02.714 | INFO     | __main__:<module>:6 - execution time: 0:00:00.060655\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-12-13 11:30:02.654182\n"
     ]
    }
   ],
   "source": [
    "start = datetime.datetime.now()\n",
    "print(start)\n",
    "test = total_trip_time(vp_usable)\n",
    "\n",
    "end = datetime.datetime.now()\n",
    "logger.info(f\"execution time: {end-start}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b442aae1-7650-4f4c-b0e8-d49369e04827",
   "metadata": {},
   "outputs": [],
   "source": [
    "# total_trip_time_df = total_trip_time_df.compute()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6939e3ba-348f-4dbb-af87-c2fbd25aab5c",
   "metadata": {},
   "source": [
    "### Update Completeness\n",
    "#### Break apart?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "74fe91a3-b200-4202-b37f-9eba89375986",
   "metadata": {},
   "outputs": [],
   "source": [
    "def trips_by_one_min(vp_usable_df: pd.DataFrame):\n",
    "    \"\"\"\n",
    "    For each trip: count how many rows are associated with each minute\n",
    "    then tag whether or not a minute has 2+ pings. \n",
    "    \"\"\"\n",
    "    subset = [\"location_timestamp_local\", \"trip_instance_key\", \"vp_idx\"]\n",
    "    vp_usable_df = vp_usable_df[subset]\n",
    "\n",
    "    # Find number of pings each minute\n",
    "    df = (\n",
    "        vp_usable_df.groupby(\n",
    "            [\n",
    "                \"trip_instance_key\",\n",
    "                pd.Grouper(key=\"location_timestamp_local\", freq=\"1Min\"),\n",
    "            ]\n",
    "        )\n",
    "        .vp_idx.count()\n",
    "        .reset_index()\n",
    "        .rename(columns={\"vp_idx\": \"number_of_pings_per_minute\"})\n",
    "    )\n",
    "\n",
    "    # Determine which rows have 2+ pings per minute\n",
    "    df = df.assign(\n",
    "        min_w_atleast2_trip_updates=df.apply(\n",
    "            lambda x: 1 if x.number_of_pings_per_minute >= 2 else 0, axis=1\n",
    "        )\n",
    "    )\n",
    "    \n",
    "    df = df.drop(columns = ['location_timestamp_local'])\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "3061e24d-f061-43e3-add4-ed4c9675eb72",
   "metadata": {},
   "outputs": [],
   "source": [
    "# map partitions here \n",
    "# test = trips_by_min(vp_usable2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ad3a37f3-c6ef-4af2-9c11-bde0cca3c0f4",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-12-13 11:30:02.769780\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-12-13 11:32:17.554 | INFO     | __main__:<module>:14 - execution time: 0:02:14.785107\n"
     ]
    }
   ],
   "source": [
    "start = datetime.datetime.now()\n",
    "print(start)\n",
    "one_min_ping_df = vp_usable.map_partitions(\n",
    "    trips_by_one_min,\n",
    "    meta={\n",
    "        \"trip_instance_key\": \"object\",\n",
    "        \"number_of_pings_per_minute\": \"int64\",\n",
    "        \"min_w_atleast2_trip_updates\":\"int64\"\n",
    "    },\n",
    "    align_dataframes=False,\n",
    ").persist()\n",
    "\n",
    "end = datetime.datetime.now()\n",
    "logger.info(f\"execution time: {end-start}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "fef89a86-b02b-493f-92f5-2fd31a703352",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5554717"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(one_min_ping_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a88b3b7f-3ed1-48dc-b066-d91e55c4d5c2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dask.dataframe.core.DataFrame"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(one_min_ping_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "51c11e77-8b06-4832-87c4-2c7601db9571",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><strong>Dask DataFrame Structure:</strong></div>\n",
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>trip_instance_key</th>\n",
       "      <th>number_of_pings_per_minute</th>\n",
       "      <th>min_w_atleast2_trip_updates</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>npartitions=96</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <td>object</td>\n",
       "      <td>int64</td>\n",
       "      <td>int64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>\n",
       "<div>Dask Name: trips_by_one_min, 1 graph layer</div>"
      ],
      "text/plain": [
       "Dask DataFrame Structure:\n",
       "               trip_instance_key number_of_pings_per_minute min_w_atleast2_trip_updates\n",
       "npartitions=96                                                                         \n",
       "                          object                      int64                       int64\n",
       "                             ...                        ...                         ...\n",
       "...                          ...                        ...                         ...\n",
       "                             ...                        ...                         ...\n",
       "                             ...                        ...                         ...\n",
       "Dask Name: trips_by_one_min, 1 graph layer"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "one_min_ping_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "5227d5df-573e-449a-8fe1-90c8e965446f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_completeness(trips_by_one_min: pd.DataFrame):\n",
    "    \"\"\"\n",
    "    For each trip: find the median GTFS pings per minute,\n",
    "    the total minutes with at least 1 GTFS ping per minute,\n",
    "    and total minutes with at least 2 GTFS pings per minute.\n",
    "    \"\"\"\n",
    "    # Need a copy of numer of pings per minute to count for total minutes w gtfs\n",
    "    df[\"total_min_w_gtfs\"] = df.number_of_pings_per_minute\n",
    "    \n",
    "    # Find the total min with at least 2 pings per min\n",
    "    df = (\n",
    "        df.groupby([\"trip_instance_key\"])\n",
    "        .agg(\n",
    "            {\n",
    "                \"min_w_atleast2_trip_updates\": \"sum\",\n",
    "                \"number_of_pings_per_minute\": \"mean\",\n",
    "                \"total_min_w_gtfs\": \"count\",\n",
    "            }\n",
    "        )\n",
    "        .reset_index()\n",
    "        .rename(\n",
    "            columns={\n",
    "                \"number_of_pings_per_minute\": \"avg_pings_per_min\",\n",
    "            }\n",
    "        )\n",
    "    )\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "d9bd4365-529e-471b-887d-06f37ca649c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-12-13 11:32:17.984788\n"
     ]
    },
    {
     "ename": "UnboundLocalError",
     "evalue": "local variable 'df' referenced before assignment",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUnboundLocalError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[26], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m start \u001b[38;5;241m=\u001b[39m datetime\u001b[38;5;241m.\u001b[39mdatetime\u001b[38;5;241m.\u001b[39mnow()\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(start)\n\u001b[0;32m----> 3\u001b[0m update_df \u001b[38;5;241m=\u001b[39m \u001b[43mupdate_completeness\u001b[49m\u001b[43m(\u001b[49m\u001b[43mone_min_ping_df\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      4\u001b[0m end \u001b[38;5;241m=\u001b[39m datetime\u001b[38;5;241m.\u001b[39mdatetime\u001b[38;5;241m.\u001b[39mnow()\n\u001b[1;32m      5\u001b[0m logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mexecution time: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mend\u001b[38;5;241m-\u001b[39mstart\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[25], line 8\u001b[0m, in \u001b[0;36mupdate_completeness\u001b[0;34m(trips_by_one_min)\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;124;03mFor each trip: find the median GTFS pings per minute,\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;124;03mthe total minutes with at least 1 GTFS ping per minute,\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;124;03mand total minutes with at least 2 GTFS pings per minute.\u001b[39;00m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# Need a copy of numer of pings per minute to count for total minutes w gtfs\u001b[39;00m\n\u001b[0;32m----> 8\u001b[0m df[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtotal_min_w_gtfs\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43mdf\u001b[49m\u001b[38;5;241m.\u001b[39mnumber_of_pings_per_minute\n\u001b[1;32m     10\u001b[0m \u001b[38;5;66;03m# Find the total min with at least 2 pings per min\u001b[39;00m\n\u001b[1;32m     11\u001b[0m df \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m     12\u001b[0m     df\u001b[38;5;241m.\u001b[39mgroupby([\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrip_instance_key\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m     13\u001b[0m     \u001b[38;5;241m.\u001b[39magg(\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     25\u001b[0m     )\n\u001b[1;32m     26\u001b[0m )\n",
      "\u001b[0;31mUnboundLocalError\u001b[0m: local variable 'df' referenced before assignment"
     ]
    }
   ],
   "source": [
    "start = datetime.datetime.now()\n",
    "print(start)\n",
    "update_df = update_completeness(one_min_ping_df)\n",
    "end = datetime.datetime.now()\n",
    "logger.info(f\"execution time: {end-start}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e39d975-ef55-4ea8-b5ff-e5d5c3e8ff08",
   "metadata": {},
   "source": [
    "### Spatial Accuracy\n",
    "* Addresses \"<i>in next draft, work on grouping functions that belong together, such as this one. total_counts and total_counts_by_trip sound basically equivalent, and they are nearly doing the same thing, except total_counts actually creates 2 columns. work on logically grouping or absorbing functions or rewriting functions so the same function can now be used twice.\n",
    "Adapt this function to be used twice\n",
    "Compare it to this to find where they have stuff in common and which part should be removed from the generic function</i>\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "a8096485-4962-4dde-8909-9d1ea9bb127a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def grab_shape_keys_in_vp(vp_usable: dd.DataFrame, analysis_date: str) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Subset raw vp and find unique trip_instance_keys.\n",
    "    Create crosswalk to link trip_instance_key to shape_array_key.\n",
    "    \"\"\"\n",
    "    vp_usable = (\n",
    "        vp_usable[[\"trip_instance_key\"]].drop_duplicates().reset_index(drop=True)\n",
    "    )\n",
    "\n",
    "    trips_with_shape = helpers.import_scheduled_trips(\n",
    "        analysis_date,\n",
    "        columns=[\"trip_instance_key\", \"shape_array_key\"],\n",
    "        get_pandas=True,\n",
    "    )\n",
    "\n",
    "    # Only one row per trip/shape\n",
    "    # trip_instance_key and shape_array_key are the only 2 cols left\n",
    "    m1 = dd.merge(vp_usable, trips_with_shape, on=\"trip_instance_key\", how=\"inner\")\n",
    "\n",
    "    return m1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75b9667a-80e0-4763-ae4e-635ad2f28192",
   "metadata": {},
   "source": [
    "### Why is September not working\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "b84788e3-4f67-479c-82ea-87a0d51a960d",
   "metadata": {},
   "outputs": [],
   "source": [
    "date2= '2023-09-13'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "c4341a40-5d03-45de-8c1d-8d3d0758527b",
   "metadata": {},
   "outputs": [],
   "source": [
    "vp_usable = load_vp_usable(date2)\n",
    "# Filter for now\n",
    "vp_usable2 = vp_usable.loc[vp_usable.schedule_gtfs_dataset_key == gtfs_key].reset_index(\n",
    "    drop=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "a0713387-b6f6-46e6-964a-fcdb91863619",
   "metadata": {},
   "outputs": [],
   "source": [
    "vp_usable2 = vp_usable2.compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "923f4c4b-68ca-4b38-be66-be9c25ed77e5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['7cc0cb1871dfd558f11a2885c145d144'], dtype=object)"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vp_usable2.schedule_gtfs_dataset_key.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "211e46da-ca98-4f7f-9663-056d44be18f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_keys = vp_usable2[['trip_instance_key']].drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "96938d0e-3e82-446d-b480-3397d68fdf9b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(9360, 1)"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_keys.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "6126396b-e7dd-4a34-9987-87674a8113e7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>trip_instance_key</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>cb78e2e2d2bdd74e897574f57629ebf3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>106</th>\n",
       "      <td>ac45dac6886a6c20c532396e674d1f39</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>168</th>\n",
       "      <td>456b7c997b0c63be4e0eeba930ce9460</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>327</th>\n",
       "      <td>68f864dd5831697753c11e29479967e8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>423</th>\n",
       "      <td>edd4e5cfb00cc83cdeb7f15649826bc3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    trip_instance_key\n",
       "0    cb78e2e2d2bdd74e897574f57629ebf3\n",
       "106  ac45dac6886a6c20c532396e674d1f39\n",
       "168  456b7c997b0c63be4e0eeba930ce9460\n",
       "327  68f864dd5831697753c11e29479967e8\n",
       "423  edd4e5cfb00cc83cdeb7f15649826bc3"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_keys.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "ae81fdab-38ce-4037-9405-c289fb31caf6",
   "metadata": {},
   "outputs": [],
   "source": [
    " trips_with_shape = helpers.import_scheduled_trips(\n",
    "        date2,\n",
    "        columns=[\"trip_instance_key\", \"shape_array_key\"],\n",
    "        get_pandas=True,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "be10d42b-de3a-475a-a7b9-71641371b11f",
   "metadata": {},
   "outputs": [],
   "source": [
    "m1 = pd.merge(test_keys, trips_with_shape, on=\"trip_instance_key\", how=\"inner\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "b93e99be-dc23-47b2-a60f-c3204f50bc6c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0, 2)"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "75c62e87-a298-405e-bf8e-f362bab9c0b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "shapes_df = grab_shape_keys_in_vp(vp_usable2, date2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "d4691c03-37fa-44ff-9a84-2f33d4b670b6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pandas.core.frame.DataFrame"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(shapes_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "88d9d527-bc41-4eda-8444-2e36f61c3527",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0, 2)"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "shapes_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "ed6ac6d9-3ddd-4b21-b13f-a3d4ebe30f76",
   "metadata": {},
   "outputs": [],
   "source": [
    "subset = shapes_df.shape_array_key.unique().tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "b8390b24-0bf0-404b-afa1-d58b720b4148",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "subset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "54ef16ff-03a2-402e-a67b-a510dea4f0f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def buffer_shapes(\n",
    "    trips_with_shape: pd.DataFrame,\n",
    "    analysis_date: str,\n",
    "    buffer_meters: int = 35,\n",
    "):\n",
    "    \"\"\"\n",
    "    Filter scheduled shapes down to the shapes that appear in vp.\n",
    "    Buffer these.\n",
    "\n",
    "    Attach the shape geometry for a subset of shapes or trips.\n",
    "    \"\"\"\n",
    "    subset = trips_with_shape.shape_array_key.unique().compute().tolist()\n",
    "\n",
    "    shapes = helpers.import_scheduled_shapes(\n",
    "        analysis_date,\n",
    "        columns=[\"shape_array_key\", \"geometry\"],\n",
    "        filters=[[(\"shape_array_key\", \"in\", subset)]],\n",
    "        crs=PROJECT_CRS,\n",
    "        get_pandas=False,\n",
    "    ).pipe(helpers.remove_shapes_outside_ca)\n",
    "\n",
    "    # to_crs takes awhile, so do a filtering on only shapes we need\n",
    "    shapes = shapes.assign(geometry=shapes.geometry.buffer(buffer_meters))\n",
    "\n",
    "    trips_with_shape_geom = dd.merge(\n",
    "        shapes, trips_with_shape, on=\"shape_array_key\", how=\"inner\"\n",
    "    )\n",
    "\n",
    "    trips_with_shape_geom = trips_with_shape_geom.compute()\n",
    "    return trips_with_shape_geom"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "43a4df1f-3e2b-430e-8310-4b6d2308c6f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from segment_speed_utils.project_vars import COMPILED_CACHED_VIEWS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "8ca1431d-0881-4d2e-b6ab-b8b4e2b04b6f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'gs://calitp-analytics-data/data-analyses/rt_delay/compiled_cached_views/'"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "COMPILED_CACHED_VIEWS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "4d26dda9-6053-4f9e-bc0f-86210c4ae463",
   "metadata": {},
   "outputs": [],
   "source": [
    "FILE = f\"{COMPILED_CACHED_VIEWS}routelines_{date2}.parquet\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "9f08825e-3c3c-411c-aca1-3614bacdca5f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'gs://calitp-analytics-data/data-analyses/rt_delay/compiled_cached_views/routelines_2023-09-13.parquet'"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "FILE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "40eee091-f35e-4d81-99e6-ec50f2d3305d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# gs://calitp-analytics-data/data-analyses/rt_delay/compiled_cached_views/routelines_2023-09-13.parquet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "02bb0f09-7a39-4bed-9791-ea6279be50c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "shapes = helpers.import_scheduled_shapes(\n",
    "        date2,\n",
    "        columns=[\"shape_array_key\", \"geometry\"],\n",
    "        crs=PROJECT_CRS,\n",
    "        get_pandas=False,\n",
    "    ).pipe(helpers.remove_shapes_outside_ca)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "977233d2-81bf-4d1f-802a-354dada280e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import dask_geopandas as dg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "e35842ea-be00-4ca8-a7d9-556fb647f729",
   "metadata": {},
   "outputs": [],
   "source": [
    "test = dg.read_parquet(FILE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "863e11f7-325d-40cb-82b4-fe66de3b97d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "test2 = gpd.read_parquet(FILE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "371a5a82-c4f8-4799-8992-34630ed5600c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['feed_key', 'feed_timezone', 'service_date',\n",
       "       'shape_first_departure_datetime_pacific',\n",
       "       'shape_last_arrival_datetime_pacific', 'shape_id', 'shape_array_key',\n",
       "       'n_trips', 'geometry'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test2.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "68b33291-c057-46b5-9cea-5ad1de2cdd42",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "ename": "ArrowTypeError",
     "evalue": "Array type doesn't match type of values set: string vs null",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mArrowTypeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[33], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m buffer_df \u001b[38;5;241m=\u001b[39m \u001b[43mbuffer_shapes\u001b[49m\u001b[43m(\u001b[49m\u001b[43mshapes_df\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdate2\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m35\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[29], line 14\u001b[0m, in \u001b[0;36mbuffer_shapes\u001b[0;34m(trips_with_shape, analysis_date, buffer_meters)\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;124;03mFilter scheduled shapes down to the shapes that appear in vp.\u001b[39;00m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;124;03mBuffer these.\u001b[39;00m\n\u001b[1;32m      9\u001b[0m \n\u001b[1;32m     10\u001b[0m \u001b[38;5;124;03mAttach the shape geometry for a subset of shapes or trips.\u001b[39;00m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     12\u001b[0m subset \u001b[38;5;241m=\u001b[39m trips_with_shape\u001b[38;5;241m.\u001b[39mshape_array_key\u001b[38;5;241m.\u001b[39munique()\u001b[38;5;241m.\u001b[39mcompute()\u001b[38;5;241m.\u001b[39mtolist()\n\u001b[0;32m---> 14\u001b[0m shapes \u001b[38;5;241m=\u001b[39m \u001b[43mhelpers\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mimport_scheduled_shapes\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     15\u001b[0m \u001b[43m    \u001b[49m\u001b[43manalysis_date\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     16\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mshape_array_key\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mgeometry\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     17\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfilters\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43m[\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mshape_array_key\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43min\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msubset\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     18\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcrs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mPROJECT_CRS\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     19\u001b[0m \u001b[43m    \u001b[49m\u001b[43mget_pandas\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     20\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mpipe(helpers\u001b[38;5;241m.\u001b[39mremove_shapes_outside_ca)\n\u001b[1;32m     22\u001b[0m \u001b[38;5;66;03m# to_crs takes awhile, so do a filtering on only shapes we need\u001b[39;00m\n\u001b[1;32m     23\u001b[0m shapes \u001b[38;5;241m=\u001b[39m shapes\u001b[38;5;241m.\u001b[39massign(geometry\u001b[38;5;241m=\u001b[39mshapes\u001b[38;5;241m.\u001b[39mgeometry\u001b[38;5;241m.\u001b[39mbuffer(buffer_meters))\n",
      "File \u001b[0;32m~/data-analyses/rt_segment_speeds/segment_speed_utils/helpers.py:152\u001b[0m, in \u001b[0;36mimport_scheduled_shapes\u001b[0;34m(analysis_date, filters, columns, get_pandas, crs)\u001b[0m\n\u001b[1;32m    149\u001b[0m     shapes \u001b[38;5;241m=\u001b[39m gpd\u001b[38;5;241m.\u001b[39mread_parquet(FILE, filters \u001b[38;5;241m=\u001b[39m filters, \n\u001b[1;32m    150\u001b[0m                               columns \u001b[38;5;241m=\u001b[39m columns)\n\u001b[1;32m    151\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 152\u001b[0m     shapes \u001b[38;5;241m=\u001b[39m \u001b[43mdg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_parquet\u001b[49m\u001b[43m(\u001b[49m\u001b[43mFILE\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfilters\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mfilters\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    153\u001b[0m \u001b[43m                             \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    155\u001b[0m \u001b[38;5;66;03m# Don't do this expensive operation unless we have to\u001b[39;00m\n\u001b[1;32m    156\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m crs \u001b[38;5;241m!=\u001b[39m shapes\u001b[38;5;241m.\u001b[39mcrs:\n",
      "File \u001b[0;32m/opt/conda/lib/python3.9/site-packages/dask_geopandas/io/parquet.py:102\u001b[0m, in \u001b[0;36mread_parquet\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    101\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mread_parquet\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m--> 102\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43mdd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_parquet\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mengine\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mGeoArrowEngine\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    103\u001b[0m     \u001b[38;5;66;03m# check if spatial partitioning information was stored\u001b[39;00m\n\u001b[1;32m    104\u001b[0m     spatial_partitions \u001b[38;5;241m=\u001b[39m result\u001b[38;5;241m.\u001b[39m_meta\u001b[38;5;241m.\u001b[39mattrs\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mspatial_partitions\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.9/site-packages/dask/dataframe/io/parquet/core.py:471\u001b[0m, in \u001b[0;36mread_parquet\u001b[0;34m(path, columns, filters, categories, index, storage_options, engine, calculate_divisions, ignore_metadata_file, metadata_task_size, split_row_groups, chunksize, aggregate_files, parquet_file_extension, **kwargs)\u001b[0m\n\u001b[1;32m    468\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m index \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(index, \u001b[38;5;28mstr\u001b[39m):\n\u001b[1;32m    469\u001b[0m     index \u001b[38;5;241m=\u001b[39m [index]\n\u001b[0;32m--> 471\u001b[0m read_metadata_result \u001b[38;5;241m=\u001b[39m \u001b[43mengine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_metadata\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    472\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    473\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpaths\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    474\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcategories\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcategories\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    475\u001b[0m \u001b[43m    \u001b[49m\u001b[43mindex\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mindex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    476\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgather_statistics\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcalculate_divisions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    477\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfilters\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfilters\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    478\u001b[0m \u001b[43m    \u001b[49m\u001b[43msplit_row_groups\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msplit_row_groups\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    479\u001b[0m \u001b[43m    \u001b[49m\u001b[43mchunksize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchunksize\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    480\u001b[0m \u001b[43m    \u001b[49m\u001b[43maggregate_files\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maggregate_files\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    481\u001b[0m \u001b[43m    \u001b[49m\u001b[43mignore_metadata_file\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_metadata_file\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    482\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmetadata_task_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmetadata_task_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    483\u001b[0m \u001b[43m    \u001b[49m\u001b[43mparquet_file_extension\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparquet_file_extension\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    484\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    485\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    487\u001b[0m \u001b[38;5;66;03m# In the future, we may want to give the engine the\u001b[39;00m\n\u001b[1;32m    488\u001b[0m \u001b[38;5;66;03m# option to return a dedicated element for `common_kwargs`.\u001b[39;00m\n\u001b[1;32m    489\u001b[0m \u001b[38;5;66;03m# However, to avoid breaking the API, we just embed this\u001b[39;00m\n\u001b[1;32m    490\u001b[0m \u001b[38;5;66;03m# data in the first element of `parts` for now.\u001b[39;00m\n\u001b[1;32m    491\u001b[0m \u001b[38;5;66;03m# The logic below is inteded to handle backward and forward\u001b[39;00m\n\u001b[1;32m    492\u001b[0m \u001b[38;5;66;03m# compatibility with a user-defined engine.\u001b[39;00m\n\u001b[1;32m    493\u001b[0m meta, statistics, parts, index \u001b[38;5;241m=\u001b[39m read_metadata_result[:\u001b[38;5;241m4\u001b[39m]\n",
      "File \u001b[0;32m/opt/conda/lib/python3.9/site-packages/dask_geopandas/io/parquet.py:51\u001b[0m, in \u001b[0;36mGeoArrowEngine.read_metadata\u001b[0;34m(cls, fs, paths, **kwargs)\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[38;5;129m@classmethod\u001b[39m\n\u001b[1;32m     50\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mread_metadata\u001b[39m(\u001b[38;5;28mcls\u001b[39m, fs, paths, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m---> 51\u001b[0m     meta, stats, parts, index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_metadata\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpaths\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     53\u001b[0m     gather_spatial_partitions \u001b[38;5;241m=\u001b[39m kwargs\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgather_spatial_partitions\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m     55\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m gather_spatial_partitions:\n",
      "File \u001b[0;32m/opt/conda/lib/python3.9/site-packages/dask/dataframe/io/parquet/arrow.py:361\u001b[0m, in \u001b[0;36mArrowDatasetEngine.read_metadata\u001b[0;34m(cls, fs, paths, categories, index, gather_statistics, filters, split_row_groups, chunksize, aggregate_files, ignore_metadata_file, metadata_task_size, parquet_file_extension, **kwargs)\u001b[0m\n\u001b[1;32m    358\u001b[0m meta \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_create_dd_meta(dataset_info)\n\u001b[1;32m    360\u001b[0m \u001b[38;5;66;03m# Stage 3: Generate parts and stats\u001b[39;00m\n\u001b[0;32m--> 361\u001b[0m parts, stats, common_kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_construct_collection_plan\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset_info\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    363\u001b[0m \u001b[38;5;66;03m# Add `common_kwargs` and `aggregation_depth` to the first\u001b[39;00m\n\u001b[1;32m    364\u001b[0m \u001b[38;5;66;03m# element of `parts`. We can return as a separate element\u001b[39;00m\n\u001b[1;32m    365\u001b[0m \u001b[38;5;66;03m# in the future, but should avoid breaking the API for now.\u001b[39;00m\n\u001b[1;32m    366\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(parts):\n",
      "File \u001b[0;32m/opt/conda/lib/python3.9/site-packages/dask/dataframe/io/parquet/arrow.py:1203\u001b[0m, in \u001b[0;36mArrowDatasetEngine._construct_collection_plan\u001b[0;34m(cls, dataset_info)\u001b[0m\n\u001b[1;32m   1193\u001b[0m \u001b[38;5;66;03m# Main parts/stats-construction\u001b[39;00m\n\u001b[1;32m   1194\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m   1195\u001b[0m     has_metadata_file\n\u001b[1;32m   1196\u001b[0m     \u001b[38;5;129;01mor\u001b[39;00m metadata_task_size \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1201\u001b[0m \n\u001b[1;32m   1202\u001b[0m     \u001b[38;5;66;03m# Start with sorted (by path) list of file-based fragments\u001b[39;00m\n\u001b[0;32m-> 1203\u001b[0m     file_frags \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43msorted\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1204\u001b[0m \u001b[43m        \u001b[49m\u001b[43m(\u001b[49m\u001b[43mfrag\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mfrag\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mds\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_fragments\u001b[49m\u001b[43m(\u001b[49m\u001b[43mds_filters\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1205\u001b[0m \u001b[43m        \u001b[49m\u001b[43mkey\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mnatural_sort_key\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1206\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1207\u001b[0m     parts, stats \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_collect_file_parts(file_frags, dataset_info_kwargs)\n\u001b[1;32m   1208\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1209\u001b[0m     \u001b[38;5;66;03m# We DON'T have a global _metadata file to work with.\u001b[39;00m\n\u001b[1;32m   1210\u001b[0m     \u001b[38;5;66;03m# We should loop over files in parallel\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1215\u001b[0m     \u001b[38;5;66;03m# the _metadata file to generate our dataset object , we need\u001b[39;00m\n\u001b[1;32m   1216\u001b[0m     \u001b[38;5;66;03m# to ignore any file fragments that are not in the list.\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.9/site-packages/dask/dataframe/io/parquet/arrow.py:1204\u001b[0m, in \u001b[0;36m<genexpr>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m   1193\u001b[0m \u001b[38;5;66;03m# Main parts/stats-construction\u001b[39;00m\n\u001b[1;32m   1194\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m   1195\u001b[0m     has_metadata_file\n\u001b[1;32m   1196\u001b[0m     \u001b[38;5;129;01mor\u001b[39;00m metadata_task_size \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1201\u001b[0m \n\u001b[1;32m   1202\u001b[0m     \u001b[38;5;66;03m# Start with sorted (by path) list of file-based fragments\u001b[39;00m\n\u001b[1;32m   1203\u001b[0m     file_frags \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msorted\u001b[39m(\n\u001b[0;32m-> 1204\u001b[0m         (frag \u001b[38;5;28;01mfor\u001b[39;00m frag \u001b[38;5;129;01min\u001b[39;00m ds\u001b[38;5;241m.\u001b[39mget_fragments(ds_filters)),\n\u001b[1;32m   1205\u001b[0m         key\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mlambda\u001b[39;00m x: natural_sort_key(x\u001b[38;5;241m.\u001b[39mpath),\n\u001b[1;32m   1206\u001b[0m     )\n\u001b[1;32m   1207\u001b[0m     parts, stats \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_collect_file_parts(file_frags, dataset_info_kwargs)\n\u001b[1;32m   1208\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1209\u001b[0m     \u001b[38;5;66;03m# We DON'T have a global _metadata file to work with.\u001b[39;00m\n\u001b[1;32m   1210\u001b[0m     \u001b[38;5;66;03m# We should loop over files in parallel\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1215\u001b[0m     \u001b[38;5;66;03m# the _metadata file to generate our dataset object , we need\u001b[39;00m\n\u001b[1;32m   1216\u001b[0m     \u001b[38;5;66;03m# to ignore any file fragments that are not in the list.\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.9/site-packages/pyarrow/_dataset.pyx:252\u001b[0m, in \u001b[0;36m_get_fragments\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.9/site-packages/pyarrow/_compute.pyx:2697\u001b[0m, in \u001b[0;36mpyarrow._compute._bind\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.9/site-packages/pyarrow/error.pxi:154\u001b[0m, in \u001b[0;36mpyarrow.lib.pyarrow_internal_check_status\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.9/site-packages/pyarrow/error.pxi:91\u001b[0m, in \u001b[0;36mpyarrow.lib.check_status\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mArrowTypeError\u001b[0m: Array type doesn't match type of values set: string vs null"
     ]
    }
   ],
   "source": [
    "buffer_df = buffer_shapes(shapes_df, date2, 35)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb56d65d-471a-414b-8fe2-d8aade972ddc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def vp_in_shape(\n",
    "    vp_usable: dd.DataFrame, trips_with_buffered_shape: gpd.GeoDataFrame\n",
    ") -> gpd.GeoDataFrame:\n",
    "\n",
    "    keep = [\"trip_instance_key\", \"x\", \"y\", \"location_timestamp_local\"]\n",
    "    vp_usable = vp_usable[keep]\n",
    "\n",
    "    vp_gdf = wrangle_shapes.vp_as_gdf(vp_usable)\n",
    "\n",
    "    gdf = pd.merge(\n",
    "        vp_gdf, trips_with_buffered_shape, on=\"trip_instance_key\", how=\"inner\"\n",
    "    )\n",
    "\n",
    "    gdf = gdf.assign(is_within=gdf.geometry_x.within(gdf.geometry_y))\n",
    "    gdf = gdf[[\"trip_instance_key\", \"location_timestamp_local\", \"is_within\"]]\n",
    "\n",
    "    return gdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd996bc1-ccf5-4bcf-9ce9-807e936913e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "start = datetime.datetime.now()\n",
    "print(start)\n",
    "spatial_accuracy_df1 = vp_usable.map_partitions(\n",
    "    vp_in_shape,\n",
    "    buffer_df,\n",
    "    meta={\n",
    "        \"trip_instance_key\": \"object\",\n",
    "        \"location_timestamp_local\": \"datetime64[ns]\",\n",
    "        \"is_within\": \"bool\",\n",
    "    },\n",
    "    align_dataframes=False,\n",
    ").persist()\n",
    "\n",
    "end = datetime.datetime.now()\n",
    "logger.info(f\"execution time: {end-start}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8c1051e-af7b-490d-8b09-b5044bd7fa7a",
   "metadata": {},
   "source": [
    "#### Adapt this to be used in multiple places"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8736a406-abc1-4331-a350-598cda6c5e0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def total_vp_counts_by_trip(vp: gpd.GeoDataFrame, new_col_title: str) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Get a count of vp for each trip, whether or not those fall\n",
    "    within buffered shape or not\n",
    "    \"\"\"\n",
    "    count_vp = (\n",
    "        vp.groupby(\"trip_instance_key\", observed=True, group_keys=False)\n",
    "        .agg({\"location_timestamp_local\": \"count\"})\n",
    "        .reset_index()\n",
    "        .rename(columns={\"location_timestamp_local\": new_col_title})\n",
    "    )\n",
    "\n",
    "    return count_vp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f7ecadb-ef13-4035-a7b7-fed2fc9ab15c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def total_counts(result: dd.DataFrame):\n",
    "\n",
    "    # Find the total number of vps for each route\n",
    "    total_vp_df = total_vp_counts_by_trip(result, \"total_vp\")\n",
    "\n",
    "    # Find the total number of vps that actually fall within the  route shape\n",
    "    subset = [\"trip_instance_key\", \"location_timestamp_local\"]\n",
    "    result2 = result.loc[result.is_within == True].reset_index(drop=True)[subset]\n",
    "\n",
    "    vps_in_shape = total_vp_counts_by_trip(result2, \"vp_in_shape\")\n",
    "\n",
    "    # Count total vps for the trip\n",
    "    count_df = pd.merge(total_vp_df, vps_in_shape, on=\"trip_instance_key\", how=\"left\")\n",
    "\n",
    "    count_df = count_df.assign(\n",
    "        vp_in_shape=count_df.vp_in_shape.fillna(0).astype(\"int32\"),\n",
    "        total_vp=count_df.total_vp.fillna(0).astype(\"int32\"),\n",
    "    )\n",
    "\n",
    "    return count_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64895df8-ab55-4251-a245-26b2f3e1808c",
   "metadata": {},
   "outputs": [],
   "source": [
    "start = datetime.datetime.now()\n",
    "print(start)\n",
    "spatial_accuracy_df2 = spatial_accuracy_df1.map_partitions(\n",
    "    total_counts,\n",
    "    meta={\"trip_instance_key\": \"object\", \"total_vp\": \"int32\", \"vp_in_shape\": \"int32\"},\n",
    "    align_dataframes=False,\n",
    ").persist()\n",
    "\n",
    "end = datetime.datetime.now()\n",
    "logger.info(f\"execution time: {end-start}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f3cbda0-4685-4568-8e60-4b3703ef5eeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "spatial_accuracy_df2 = spatial_accuracy_df2.compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6cfa291-414a-4df7-a26e-321bec0e03cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "spatial_accuracy_df2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e538088a-0b33-4414-9ed8-666b45661844",
   "metadata": {},
   "outputs": [],
   "source": [
    "type(update)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f208f48c-d677-4343-9359-44c5d1559847",
   "metadata": {},
   "source": [
    "test_m = (total_trip_time_df.merge(update, on = \"trip_instance_key\", how = \"outer\")\n",
    "         .merge(spatial_accuracy_df2, on =\"trip_instance_key\", how = \"outer\")) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4e5ad18-bb8d-4259-bf58-8c2c35ee3b64",
   "metadata": {},
   "source": [
    "### Read back in the file 12/11"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e92b3ec-1069-4729-84fa-24412cbc155c",
   "metadata": {},
   "outputs": [],
   "source": [
    "full_df = pd.read_parquet(\"./scripts/rt_v_schedule_trip_metrics.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82c7d45b-266d-433d-a054-085dbcaa1e73",
   "metadata": {},
   "outputs": [],
   "source": [
    "full_df.sample()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7f23536-0bce-4f49-a32e-6e51c5647031",
   "metadata": {},
   "outputs": [],
   "source": [
    "full_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cdf7d96-95fd-4c52-a391-e31b48bdff8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "full_df.trip_instance_key.nunique(), len(full_df)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
