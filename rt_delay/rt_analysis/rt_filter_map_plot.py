import shared_utils
from shared_utils.geography_utils import WGS84, CA_NAD83Albers
from shared_utils.rt_utils import *
import branca
import mapclassify

from siuba import *

import pandas as pd
import geopandas as gpd
import shapely
import folium

import datetime as dt
from tqdm import tqdm

import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt

from IPython.display import display, Markdown, IFrame

import gzip
import base64
import json
from calitp_data.storage import get_fs

class RtFilterMapper:
    '''
    Collects filtering and mapping functions, init with stop segment speed view and rt_trips
    '''
    def __init__(self, rt_trips, stop_delay_view,
                 shapes, pbar = None):
        self.debug_dict = {}
        self.pbar = pbar
        self.rt_trips = rt_trips
        self.calitp_itp_id = self.rt_trips.calitp_itp_id.iloc[0]
        self.stop_delay_view = stop_delay_view
        # # below line fixes interpolated segment mapping for 10/12 data, fixed upstream for future dates
        # self.stop_delay_view = self.stop_delay_view >> group_by(_.shape_id) >> mutate(direction_id = _.direction_id.ffill()) >> ungroup()
        self.shapes = shapes
        self.using_v2 = 'organization_name' in rt_trips.columns
        if self.using_v2:
            if 'activity_date' in rt_trips.columns:
                # raise Exception('rerun intermediate data')
                # temporarily accomodate 3/15/22, 4/12/22 intermediates generated with activity_date
                self.analysis_date = rt_trips.activity_date.iloc[0] #v2 warehouse
            else:
                self.analysis_date = rt_trips.service_date.iloc[0]
            self.organization_name = rt_trips.organization_name.iloc[0]
            self.caltrans_district = self.rt_trips.caltrans_district.iloc[0]
            self.ct_dist_numeric = int(self.caltrans_district.split(' ')[0])
            self.shn = (gpd.read_parquet(SHN_PATH)
                        >> select(_.Route, _.County, _.District,
                                  _.RouteType, _.geometry)
                        >> filter(_.District == self.ct_dist_numeric)
                       )
        else: # v1 compatibility
            self.analysis_date = rt_trips.service_date.iloc[0]
            self.organization_name = rt_trips.calitp_agency_name.iloc[0]
        self.display_date = self.analysis_date.strftime('%b %d, %Y (%a)')
        self.transit_priority_target_mph = 16
        self.endpoint_delay_view = (self.stop_delay_view
                      >> group_by(_.trip_id)
                      >> filter(_.stop_sequence == _.stop_sequence.max())
                      >> ungroup()
                      >> mutate(arrival_hour = _.arrival_time.apply(lambda x: x.hour))
                      >> inner_join(_, self.rt_trips >> select(_.trip_id, _.mean_speed_mph), on = 'trip_id')
                     )
        self.endpoint_delay_summary = (self.endpoint_delay_view
                      >> group_by(_.direction_id, _.route_id, _.arrival_hour)
                      >> summarize(n_trips = _.route_id.size, mean_end_delay_seconds = _.delay_seconds.mean())
                     )
        self.reset_filter()

    def set_filter(self, start_time = None, end_time = None, route_names = None,
                   shape_ids = None, direction_id = None, direction = None, trip_ids = None,
                   route_types = None
                  ):
        '''
        start_time, end_time: string %H:%M, for example '11:00' and '14:00'
        route_names: list or pd.Series of route_names (GTFS route_short_name)
        direction_id: '0' or '1'
        direction: string 'Northbound', 'Eastbound', 'Southbound', 'Westbound' (experimental)
        trip_ids: list or pd.Series of trip_ids (GTFS trip_ids)
        route_types: list or pd.Series of route_type
        '''
        route_names = list(route_names) if isinstance(route_names, pd.Series) else route_names
        route_types = list(route_types) if isinstance(route_types, pd.Series) else route_types
        trip_ids = list(trip_ids) if isinstance(trip_ids, pd.Series) else trip_ids
        args = [start_time, end_time, route_names, direction_id, direction, shape_ids, trip_ids, route_types]
        assert any(args), 'must supply at least 1 argument to filter'
        assert not start_time or type(dt.datetime.strptime(start_time, '%H:%M') == type(dt.datetime)), 'invalid time string'
        assert not end_time or type(dt.datetime.strptime(end_time, '%H:%M') == type(dt.datetime)), 'invalid time string'
        if route_types:
            assert pd.Series(route_types).isin(self.rt_trips.route_type).all(), 'at least 1 route type not found in self.rt_trips'
        if route_names:
            assert pd.Series(route_names).isin(self.rt_trips.route_short_name).all(), 'at least 1 route not found in self.rt_trips'
        assert not direction_id or direction_id in ['0', '1', 0, 1, 0.0, 1.0]
        if direction_id:
            direction_id = float(direction_id)
        self.filter = {}
        if start_time:
            self.filter['start_time'] = dt.datetime.strptime(start_time, '%H:%M').time()
        else:
            self.filter['start_time'] = None
        if end_time:
            self.filter['end_time'] = dt.datetime.strptime(end_time, '%H:%M').time()
        else:
            self.filter['end_time'] = None
        self.filter['route_names'] = route_names
        self.filter['route_types'] = route_types
        self.filter['shape_ids'] = shape_ids
        self.filter['trip_ids'] = trip_ids
        if shape_ids:
            shape_trips = (self.rt_trips >> filter(_.shape_id.isin(shape_ids))
                           >> distinct(_.route_short_name)
                          )
            if not shape_trips.route_short_name.isnull().all():
                self.filter['route_names'] = list(shape_trips.route_short_name)
            if len(shape_ids) == 1:
                direction = (self.rt_trips >> filter(_.shape_id == shape_ids[0])).direction.iloc[0]
        if trip_ids:
            trips = (self.rt_trips >> filter(_.trip_id.isin(trip_ids))
                           >> distinct(_.route_short_name)
                          )
            if not trips.route_short_name.isnull().all():
                self.filter['route_names'] = list(trips.route_short_name)
            if len(trip_ids) == 1:
                direction = (self.rt_trips >> filter(_.trip_id == trip_ids[0])).direction.iloc[0]
        self.filter['direction_id'] = direction_id
        self.filter['direction'] = direction
        if start_time and end_time:
            self.hr_duration_in_filter = (dt.datetime.combine(self.analysis_date, self.filter['end_time'])
                                 - dt.datetime.combine(self.analysis_date, self.filter['start_time'])
                                ).seconds / 60**2
        else:
            self.hr_duration_in_filter = (self.stop_delay_view.actual_time.max() - 
                                         self.stop_delay_view.actual_time.min()).seconds / 60**2
        
        if self.filter['route_names'] and len(self.filter['route_names']) <= 5:
            rts = 'Route(s) ' + ', '.join(self.filter['route_names'])
        elif self.filter['route_names'] and len(self.filter['route_names']) > 5:
            rts = 'Multiple Routes'
        elif not self.filter['route_names']:
            rts = 'All Routes'
                
        if self.filter['route_types'] and len(self.filter['route_types']) <= 3:
            typename = [route_type_names[rttyp] for rttyp in self.filter['route_types']]
            typename_print = ' & '.join(typename)
        elif self.filter['route_types'] and len(self.filter['route_types']) > 3:
            typename_print = 'Multiple Route Types'
        elif not self.filter['route_types']:
            typename_print = 'All Route Types'
            
        if start_time and end_time and start_time == '15:00' and end_time == '19:00':
            self.filter_period = 'PM_Peak' # underscores enable use in filenames
        elif start_time and end_time and start_time == '06:00' and end_time == '09:00':
            self.filter_period = 'AM_Peak'
        elif start_time and end_time and start_time == '10:00' and end_time == '14:00':
            self.filter_period = 'Midday'
        elif not start_time and not end_time:
            self.filter_period = 'All_Day'
        else:
            self.filter_period = f'{start_time}â€“{end_time}'
        elements_ordered = [typename_print, rts, direction, self.filter_period, self.display_date]
        self.filter_formatted = ', ' + ', '.join([str(x).replace('_', ' ') for x in elements_ordered if x])
        
        self._time_only_filter = not route_types and not route_names and not shape_ids and not direction_id and not direction and not trip_ids
            
    def reset_filter(self):
        '''
        Clear filter.
        '''
        self.filter = None
        self._time_only_filter = True
        self.hr_duration_in_filter = (self.stop_delay_view.actual_time.max() - 
                                         self.stop_delay_view.actual_time.min()).seconds / 60**2
        self.filter_period = 'All_Day'
        rts = 'All Routes'
        elements_ordered = [rts, self.filter_period, self.display_date]
        self.filter_formatted = ', ' + ', '.join([str(x).replace('_', ' ') for x in elements_ordered if x])
    
    def _filter(self, df):
        '''Filters a df (containing trip_id) on trip_id based on any set filter.
        '''
        if not self.filter:
            return df
        else:
            trips = self.rt_trips.copy()
            # print(f'view filter: {self.filter}')
        if self.filter['start_time']:
            trips = trips >> filter(_.median_time > self.filter['start_time'])
        if self.filter['end_time']:
            trips = trips >> filter(_.median_time < self.filter['end_time'])
        if self.filter['route_names']:
            trips = trips >> filter(_.route_short_name.isin(self.filter['route_names']))
        if self.filter['route_types']:
            trips = trips >> filter(_.route_type.isin(self.filter['route_types']))
        if self.filter['shape_ids']:
            trips = trips >> filter(_.shape_id.isin(self.filter['shape_ids']))
        if self.filter['trip_ids']:
            trips = trips >> filter(_.trip_id.isin(self.filter['trip_ids']))
        if self.filter['direction_id']:
            trips = trips >> filter(_.direction_id == self.filter['direction_id'])
        if self.filter['direction']:
            trips = trips >> filter(_.direction == self.filter['direction'])
        return df.copy() >> inner_join(_, trips >> select(_.trip_id), on = 'trip_id')
    
    def add_corridor(self, corridor_gdf, manual_exclude = {}):
        '''
        Add ability to filter stop_delay_view and subsequently generated stop_segment_speed_views
        to trips running within a corridor, as specified by a polygon bounding box (corridor gdf).
        Enables calculation of metrics for SCCP, LPP.
        
        manual_exclude: dict in {'shape': {'min': int, 'max': int}, ...} format (values are stop sequence)
        '''
        corridor_gdf = corridor_gdf.to_crs(CA_NAD83Albers)
        self.corridor = corridor_gdf >> select(_.geometry)
        to_clip = self.stop_delay_view.drop_duplicates(subset=['shape_id', 'stop_sequence']).dropna(subset=['stop_id'])
        clipped = to_clip.clip(corridor_gdf)
        shape_sequences = (self.stop_delay_view.dropna(subset=['stop_id'])
                          >> distinct(_.shape_id, _.stop_sequence)
                          >> arrange(_.shape_id, _.stop_sequence)
                         )
        stops_per_shape = shape_sequences >> group_by(_.shape_id) >> summarize(n_stops = _.shape_id.size)
        # can use this as # of stops in corr...
        stops_in_corr = clipped >> group_by(_.shape_id) >> summarize(in_corr = _.shape_id.size)
        stop_comparison = stops_per_shape >> inner_join(_, stops_in_corr, on='shape_id')
        # # only keep shapes with at least 10% of stops in corridor
        # stop_comparison = stop_comparison >> mutate(corr_shape = _.in_corr > _.n_stops / 10)
        # corr_shapes = stop_comparison >> filter(_.corr_shape)
        corr_shapes = stop_comparison # actually keep all

        self._shape_sequence_filter = {}
        for shape_id in corr_shapes.shape_id.unique():
            self._shape_sequence_filter[shape_id] = {}
            clipped_min = (clipped >> filter(_.shape_id == shape_id)).stop_sequence.min()
            clipped_max = (clipped >> filter(_.shape_id == shape_id)).stop_sequence.max()
            this_shape = shape_sequences >> filter(_.shape_id == shape_id)
            filter_min = (this_shape >> filter(_.stop_sequence < clipped_min)).stop_sequence.max()
            filter_max = (this_shape >> filter(_.stop_sequence > clipped_max)).stop_sequence.min()
            self._shape_sequence_filter[shape_id]['min'] = max(0, filter_min)
            self._shape_sequence_filter[shape_id]['max'] = filter_max if not np.isnan(filter_max) else clipped_max
            if manual_exclude and shape_id in manual_exclude.keys():
                if 'min' in manual_exclude[shape_id].keys():
                    self._shape_sequence_filter[shape_id]['min'] = manual_exclude[shape_id]['min']
                if 'max' in manual_exclude[shape_id].keys():
                    self._shape_sequence_filter[shape_id]['max'] = manual_exclude[shape_id]['max']
            
        fn = (lambda x: x.shape_id in (self._shape_sequence_filter.keys())
              and x.stop_sequence >= self._shape_sequence_filter[x.shape_id]['min']
              and x.stop_sequence <= self._shape_sequence_filter[x.shape_id]['max']
             )
        self.stop_delay_view['corridor'] = self.stop_delay_view.apply(fn, axis=1)
        corridor_filtered = self.stop_delay_view >> filter(_.corridor)
        assert not corridor_filtered.empty, 'no stops in corridor!'
        first_stops = corridor_filtered >> group_by(_.trip_id) >> summarize(stop_sequence = _.stop_sequence.min())
        entry_delays = (first_stops
                >> inner_join(_, corridor_filtered, on = ['trip_id', 'stop_sequence'])
                >> select(_.trip_id, _.delay_seconds)
                >> rename(entry_delay_seconds = _.delay_seconds)
               )
        with_entry_delay = corridor_filtered >> inner_join(_, entry_delays, on='trip_id')
        with_entry_delay = with_entry_delay >> mutate(corridor_delay_seconds = _.delay_seconds - _.entry_delay_seconds)
        self.corridor_stop_delays = with_entry_delay
    
    def autocorridor(self, shape_id: str, stop_seq_range: list, manual_exclude = {}):
        '''
        Defines a corridor for delay analysis using existing GTFS Shapes data,
        without the need for externally defining the corridor bounding box.
        Designed to be used alongside segment_speed_map in a notebook, i.e.
        by viewing speeds to decide on bounds and inputting the corresponding
        shape_id and stop sequence range. Attaches corridor for corridor maps/metrics.
        
        shape_id: string, GTFS shape_id
        stop_seq_range: list, containing 2 ints/floats, any order
        exclude_stops: stop_id to exclude manually (optional)
        
        example:
        shape_id = '27_3_87'
        stop_range = [16, 31]
        
        NOTE: last generated speedmap must include shape_id of interest for this to work
        
        manual_exclude: dict in {'shape': {'min': int, 'max': int}, ...} format (values are stop sequence)
        '''
        corridor = (self.stop_segment_speed_view
         >> distinct(_.shape_id, _.stop_sequence, _keep_all=True)
         >> filter(_.shape_id == shape_id,
                   _.stop_sequence >= min(stop_seq_range),
                   _.stop_sequence <= max(stop_seq_range))
        )
        corridor.geometry = corridor.buffer(100)
        corridor = corridor.dissolve()
        self.add_corridor(corridor, manual_exclude)
        return
    
    def segment_speed_map(self, segments: str='stops', how: str='low_speeds',
                          colorscale = ZERO_THIRTY_COLORSCALE, size: list=[900, 550],
                         no_title = False, corridor = False, shn = False,
                         no_render = False):
        ''' Generate a map of segment speeds aggregated across all trips for each shape, either as medians
        or 20th percentile speeds.
        
        segments: 'stops' or 'detailed' (detailed not yet implemented)
        how: 'median', 'low_speeds' (20%ile)
        colorscale: branca.colormap
        size: [x, y]
        no_title: don't show title in folium map
        corridor: if corridor set, only map segments relevant to that corridor
        shn: show state highway network
        no_render: don't remder map using folium, only store as self.detailed_map_view
        '''
        assert segments in ['stops', 'detailed']
        assert how in ['median', 'low_speeds']
        
        gcs_filename = f'{self.calitp_itp_id}_{self.analysis_date.isoformat()}_{self.filter_period}'
        subfolder = 'v2_segment_speed_views/' if self.using_v2 else 'segment_speed_views/'
        cached_periods = ['PM_Peak', 'AM_Peak', 'Midday', 'All_Day']
        if (check_cached(filename = f'{gcs_filename}.parquet', subfolder = subfolder) and self.filter_period in cached_periods
            and self._time_only_filter and not corridor):
            self.stop_segment_speed_view = gpd.read_parquet(f'{GCS_FILE_PATH}{subfolder}{gcs_filename}.parquet')
        else:
            gdf = self._filter(self.stop_delay_view)
            if corridor:
                assert hasattr(self, 'corridor'), 'must add corridor before generating corridor map'
                gdf = gdf >> filter(_.corridor)
            all_stop_speeds = gpd.GeoDataFrame()
            # for shape_id in tqdm(gdf.shape_id.unique()): trying self.pbar.update...
            if type(self.pbar) != type(None):
                self.pbar.reset(total=len(gdf.shape_id.unique()))
                self.pbar.desc = f'Generating segment speeds itp_id: {self.calitp_itp_id}'
            for shape_id in gdf.shape_id.unique():
                try:
                    this_shape = (gdf >> filter((_.shape_id == shape_id))).copy()
                    # self.debug_dict[f'{shape_id}_segments'] = this_shape
                    # Siuba errors unless you do this twice? TODO make upstream issue...
                    this_shape = this_shape >> group_by(_.trip_id) >> arrange(_.stop_sequence) >> ungroup()
                    stop_speeds = (this_shape
                                 >> group_by(_.trip_id)
                                 >> arrange(_.stop_sequence)
                                 >> mutate(seconds_from_last = (_.actual_time - _.actual_time.shift(1)).apply(lambda x: x.seconds))
                                 >> mutate(last_loc = _.shape_meters.shift(1))
                                 >> mutate(meters_from_last = (_.shape_meters - _.last_loc))
                                 >> mutate(speed_from_last = _.meters_from_last / _.seconds_from_last)
                                 >> mutate(delay_chg_sec = (_.delay_seconds - _.delay_seconds.shift(1)))
                                 >> ungroup()
                                )
                    # self.debug_dict[f'{shape_id}_{direction_id}_st_spd'] = stop_speeds
                    stop_speeds = stop_speeds.dropna(subset=['last_loc']).set_crs(shared_utils.geography_utils.CA_NAD83Albers)
                    stop_speeds.geometry = stop_speeds.apply(
                        lambda x: shapely.ops.substring(
                                    (self.shapes >> filter(_.shape_id == x.shape_id)).geometry.iloc[0],
                                    x.last_loc,
                                    x.shape_meters),
                                                    axis = 1)
                    # self.debug_dict[f'{shape_id}_st_spd1'] = stop_speeds
                    stop_speeds = (stop_speeds
                         >> mutate(speed_mph = _.speed_from_last * MPH_PER_MPS)
                         >> filter(_.speed_mph < 80, _.speed_mph > 0) ## drop impossible speeds, TODO logging?
                         >> group_by(_.stop_sequence)
                         >> mutate(n_trips_shp = _.stop_sequence.size, # filtered to shape
                                    p50_mph = _.speed_mph.median(),
                                    p20_mph = _.speed_mph.quantile(.2),
                                    p80_mph = _.speed_mph.quantile(.8),
                                    fast_slow_ratio = _.p80_mph / _.p20_mph # new intuitive variation measure
                                    # var_mph = _.speed_mph.var() # old statistical variance
                                  )
                         >> ungroup()
                         >> select(-_.arrival_time, -_.actual_time, -_.delay, -_.last_delay)
                        )
                    # self.debug_dict[f'{shape_id}_st_spd2'] = stop_speeds
                    assert not stop_speeds.empty, 'stop speeds gdf is empty!'
                except Exception as e:
                    print(f'stop_speeds shape: {stop_speeds.shape}, shape_id: {shape_id}')
                    print(e)
                    continue
                
                all_stop_speeds = pd.concat((all_stop_speeds, stop_speeds))

                if type(self.pbar) != type(None):
                    self.pbar.update()
                if type(self.pbar) != type(None):
                    self.pbar.refresh
            # count all trips in direction with RT data for a better gauge of frequency
            direction_counts = (self._filter(self.rt_trips)
                 >> count(_.route_id, _.direction_id)
                 >> mutate(trips_per_hour = _.n / self.hr_duration_in_filter)
                 >> select(-_.n)
                )
            all_stop_speeds = all_stop_speeds >> inner_join(_, direction_counts, on = ['route_id', 'direction_id'])
            self.stop_segment_speed_view = all_stop_speeds
            export_path = f'{GCS_FILE_PATH}{subfolder}'
            if self._time_only_filter and self.filter_period in cached_periods:
                shared_utils.utils.geoparquet_gcs_export(all_stop_speeds, export_path, gcs_filename)
        self.speed_map_params = (how, colorscale, size, no_title, corridor, shn, no_render)
        return self._show_speed_map()
    
    def _show_speed_map(self):
        '''
        Final formatting for speed map, saves to self.detailed_map_view and
        renders via folium unless self.segment_speed_map called with no_render = True
        
        Don't call directly; use self.segment_speed_map
        '''
        how, colorscale, size, no_title, corridor, shn, no_render = self.speed_map_params
        gdf = self.stop_segment_speed_view.copy()
        # essential here for reasonable map size!
        gdf = gdf >> distinct(_.shape_id, _.stop_sequence, _keep_all=True)
        gdf['miles_from_last'] = gdf.meters_from_last / 1609
        # Further reduce map size
        gdf = gdf >> select(-_.speed_mph, -_.speed_from_last, -_.trip_id,
                            -_.trip_key, -_.delay_seconds, -_.seconds_from_last,
                           -_.delay_chg_sec, -_.activity_date, -_.service_date,
                            -_.last_loc, -_.shape_meters,
                           -_.meters_from_last, -_.n_trips_shp)
        orig_rows = gdf.shape[0]
        gdf = gdf.round({'p50_mph': 1, 'p20_mph': 1, 'p80_mph': 1,
                         'miles_from_last': 1, 'trips_per_hour': 1, 'avg_sec': 0,
                         '_20p_sec': 0, 'fast_slow_ratio': 1}) ##round for display
        
        how_speed_col = {'median': 'p50_mph', 'low_speeds': 'p20_mph'}
        how_formatted = {'median': 'Median', 'low_speeds': '20th Percentile'}
        
        # filter out 0's in distance, speed-- issue arises from rounding error
        gdf = (gdf >> filter(_.miles_from_last > 0, _.p20_mph > 0, _.p50_mph > 0))
        
        gdf['time_formatted'] = (gdf.miles_from_last / gdf[how_speed_col[how]]) * 60**2 #seconds
        gdf['time_formatted'] = gdf['time_formatted'].apply(lambda x: f'{int(x)//60}' + f':{int(x)%60:02}')

        gdf = gdf >> arrange(_.trips_per_hour)
        gdf = gdf.set_crs(CA_NAD83Albers)
        
        ## shift to right side of road to display direction
        gdf.geometry = gdf.geometry.apply(try_parallel)
        gdf = gdf.apply(arrowize_by_frequency, axis=1)
        gdf.geometry = gdf.geometry.simplify(tolerance=5) # 5 is max for good-looking arrows?
        gdf = gdf >> filter(gdf.geometry.is_valid)
        gdf = gdf >> filter(-gdf.geometry.is_empty)
        
        assert gdf.shape[0] >= orig_rows*.975, \
            f'over 2.5% of geometries invalid after buffer+simplify ({gdf.shape[0]} / {orig_rows})'
        gdf = gdf.to_crs(WGS84)
        self.current_centroid = (gdf.geometry.centroid.y.mean(), gdf.geometry.centroid.x.mean())
        self.detailed_map_view = gdf.copy()
        if no_render:
            return  # ready but don't show map here, export later           
        display_cols = [how_speed_col[how], 'time_formatted', 'miles_from_last',
                       'route_short_name', 'trips_per_hour', 'shape_id',
                       'stop_sequence', 'fast_slow_ratio']
        display_aliases = ['Speed (miles per hour)', 'Travel time', 'Segment distance (miles)',
                          'Route', 'Frequency (trips per hour)', 'Shape ID',
                          'Stop Sequence', '80th %ile to 20th %ile speed ratio (variation in speeds)']
        tooltip_dict = {'aliases': display_aliases}
        if no_title:
            title = ''
        else:
            title = f"{self.organization_name} {how_formatted[how]} Vehicle Speeds Between Stops{self.filter_formatted}"
        colorscale.caption = "Speed (miles per hour)"
        style_dict = {'opacity': 0, 'fillOpacity': 0.8}
        if shn:
            m = self.shn.explore(tiles = 'CartoDB positron', color = 'gray',
                                 width = size[0], height = size[1], zoom_start = 13,
                                 location = self.current_centroid)
        else:
            m = None
        g = gdf.explore(column=how_speed_col[how],
                        cmap = colorscale,
                        tiles = 'CartoDB positron',
                        style_kwds = style_dict,
                        tooltip = display_cols, popup = display_cols,
                        tooltip_kwds = tooltip_dict, popup_kwds = tooltip_dict,
                        highlight_kwds = {'fillColor': '#DD1C77',"fillOpacity": 0.6},
                        width = size[0], height = size[1], zoom_start = 13,
                        location = self.current_centroid, m = m)
        
        title_html = f"""
         <h3 align="center" style="font-size:20px"><b>{title}</b></h3>
         """
        g.get_root().html.add_child(folium.Element(title_html)) # might still want a util for this...
        return g   
    
    def map_variance(self, no_title = False, no_render = False):
        '''
        A quick map of relative speed variance across stop segments, measured as
        the ratio between the 80th percentile and 20th percentile speeds
        '''
        assert hasattr(self, 'detailed_map_view'), 'must generate a speedmap first with self.segment_speed_map'
        gdf = self.detailed_map_view.copy().dropna(subset=['fast_slow_ratio']) >> arrange(_.trips_per_hour)
        display_cols = ['fast_slow_ratio', 'p20_mph', 'p80_mph', 'miles_from_last',
                       'route_short_name', 'trips_per_hour', 'shape_id',
                       'stop_sequence']
        display_aliases = ['80th %ile to 20th %ile speed ratio (variation in speeds)',
                           '20th %ile Speed (miles per hour)', ' 80th %ile Speed (miles per hour)', 'Segment distance (miles)',
                          'Route', 'Frequency (trips per hour)', 'Shape ID',
                          'Stop Sequence']
        tooltip_dict = {'aliases': display_aliases}
        cmap = VARIANCE_FIXED_COLORSCALE
        self.variance_cmap = cmap
        gdf = gdf[display_cols + ['geometry']]
        assert isinstance(gdf, gpd.GeoDataFrame)
        self._variance_map_view = gdf
        if no_render:
            return
        if no_title:
            title = ''
        else:
            title = f"{self.organization_name} Variation in Vehicle Speeds Between Stops{self.filter_formatted}"

        style_dict = {'opacity': 0.8, 'fillOpacity': 0.8}
        g = gdf.explore(column='fast_slow_ratio', cmap = self.variance_cmap,
                        tooltip = display_cols, popup = display_cols,
                        tooltip_kwds = tooltip_dict, popup_kwds = tooltip_dict,
                        tiles = 'CartoDB positron',
                   style_kwds = style_dict)
        title_html = f"""
         <h3 align="center" style="font-size:20px"><b>{title}</b></h3>
         """
        g.get_root().html.add_child(folium.Element(title_html)) # might still want a util for this...
        return g
    
    def map_gz_export(self, map_type: str='_20p_speeds'):
        '''
        Test exporting speed data to gcs bucket for iframe render
        Will always put state highway network in state['layers'][0] 
        map_type: '_20p_speeds', 'variance', or 'shn'
        '''
        self.spa_map_state = {"name": "null", "layers": [], "lat_lon": (),
                             "zoom": 13}
        fs = get_fs()
        prefix = f'calitp-map-tiles/speeds_{self.analysis_date.isoformat()}'
        
        def _export(gdf, path):
        ## TODO generalize and --> rt_utils
            geojson_str = gdf.to_json()
            geojson_bytes = geojson_str.encode('utf-8')
            print(f'writing to {path}')
            with fs.open(path, 'wb') as writer:  # write out to public-facing GCS?
                with gzip.GzipFile(fileobj=writer, mode="w") as gz:
                    gz.write(geojson_bytes)            
            return
        
        if map_type == '_20p_speeds':
            assert hasattr(self, 'detailed_map_view'), 'must generate a speedmap first with self.segment_speed_map'
            if len(self.spa_map_state["layers"]) != 1:  # re-initialize to SHN only
                self.map_gz_export(map_type = 'shn')
            
            path = f'{prefix}/{self.calitp_itp_id}_{self.filter_period}_speeds.geojson.gz'
            gdf = self.detailed_map_view.copy()
            gdf['organization_name'] = self.organization_name
            cmap = self.speed_map_params[1]
            gdf['color'] = gdf.p20_mph.apply(lambda x: cmap.rgb_bytes_tuple(x))
            gdf = gdf.round({'stop_sequence': 2}) # round for map display, interpolated segs are long floats
            self.spa_map_state["layers"] += [{
                "name": f"{self.organization_name} Vehicle Speeds {self.display_date}",
                "url": f"https://storage.googleapis.com/{path}", "type": "speedmap",
                'properties': {'stroked': False, 'highlight_saturation_multiplier': 0.5, 'tooltip_speed_key': 'p20_mph',
                              'opacity': 0.7}
                }]
            self.spa_map_state["lat_lon"] = self.current_centroid
            self.spa_map_state["zoom"] = 13
            self.spa_map_state['legend_url'] = 'https://storage.googleapis.com/calitp-map-tiles/speeds_legend.svg'
            return _export(gdf, path)
        
        elif map_type == 'variance':
            assert hasattr(self, 'detailed_map_view'), 'must generate a variance map first with self.map_variance'
            if len(self.spa_map_state["layers"]) != 1:  # re-initialize to SHN only
                self.map_gz_export(map_type = 'shn')
            
            path = f'{prefix}/{self.calitp_itp_id}_{self.filter_period}_variance.geojson.gz'
            cmap = self.variance_cmap
            gdf = self._variance_map_view
            gdf = gdf.round({'stop_sequence': 2}) # round for map display, interpolated segs are long floats
            gdf['color'] = gdf.fast_slow_ratio.apply(lambda x: cmap.rgb_bytes_tuple(x))
            self.spa_map_state["layers"] += [{"name": f"{self.organization_name} Variation in Speeds {self.display_date}",
             "url": f"https://storage.googleapis.com/{path}",
             'properties': {'stroked': False, 'highlight_saturation_multiplier': 0.5, 'opacity': 0.8}
                                             }]
            self.spa_map_state["lat_lon"] = self.current_centroid
            self.spa_map_state["zoom"] = 13
            self.spa_map_state['legend_url'] = 'https://storage.googleapis.com/calitp-map-tiles/variance_legend.svg'
            return _export(gdf, path)
        
        elif map_type == 'shn':
            dist = self.caltrans_district[:2]
            path = f'{prefix}/{dist}_SHN.geojson.gz'
            shn_gdf = self.shn.copy().to_crs(WGS84)
            self.spa_map_state["layers"] = [{"name": f"D{dist} State Highway Network",
             "url": f"https://storage.googleapis.com/{path}", "type": "state_highway_network"}]
            return _export(shn_gdf, path)
    
    def display_spa_map(self, width: int=1100, height: int=650):
        '''
        Display map from external simple web app in the notebook/JupyterBook context via an IFrame.
        Will show most recent map set using self.map_gz_export
        Width/height defaults are current best option for JupyterBook, don't change for portfolio use
        width, height: int (pixels)
        '''
        assert hasattr(self, 'spa_map_state'), 'must export map and set state first using self.map_gz_export'
        base64state = base64.urlsafe_b64encode(json.dumps(self.spa_map_state).encode()).decode()
        i = IFrame(f'https://leaflet-speedmaps--cal-itp-data-analyses.netlify.app/?state={base64state}', width=width, height=height)
        display(i)
        return

    
    def chart_speeds(self, no_title = False):
        '''
        A bar chart showing delays grouped by arrival hour for current filtered selection.
        Currently hardcoded to 0600-2200.
        '''
        filtered_trips = (self._filter(self.rt_trips)
                             >> mutate(median_hour = _.median_time.apply(lambda x: x.hour))
                             >> filter(_.median_hour > 5)
                             >> filter(_.median_hour < 23)
                            )
        grouped = (filtered_trips >> group_by(_.median_hour)
                   >> summarize(median_trip_mph = _.mean_speed_mph.median())
                  )
        grouped['Median Trip Speed (mph)'] = grouped.median_trip_mph.apply(lambda x: round(x, 1))
        grouped['Hour'] = grouped.median_hour
        if no_title:
            title = ''
        else:
            title = f"{self.organization_name} Median Trip Speeds by Arrival Hour{self.filter_formatted}"
            
        sns_plot = (sns.barplot(x=grouped['Hour'], y=grouped['Median Trip Speed (mph)'], ci=None, 
                       palette=[shared_utils.calitp_color_palette.CALITP_CATEGORY_BOLD_COLORS[1]])
            .set_title(title)
           )
        chart = sns_plot.get_figure()
        chart.tight_layout()
        return chart
    
    def chart_variability(self, min_stop_seq = None, max_stop_seq = None, num_segments = None,
                         no_title = False):
        '''
        Chart trip speed variability, as speed between each stop segments.
        stop_sequence_range: (min_stop, max_stop)
        '''
        sns.set(rc = {'figure.figsize':(13,6)})
        assert (self.filter['shape_ids']
                and len(self.filter['shape_ids']) == 1), 'must filter to a single shape_id'
        _map = self.segment_speed_map()
        to_chart = self.stop_segment_speed_view.copy()
        to_chart = to_chart.dropna(subset=['stop_id'])
        if num_segments:
            unique_stops = list(to_chart.stop_sequence.unique())[:num_segments]
            min_stop_seq = min(unique_stops)
            max_stop_seq = max(unique_stops)
        if min_stop_seq:
            to_chart = to_chart >> filter(_.stop_sequence >= min_stop_seq)
        if max_stop_seq:
            to_chart = to_chart >> filter(_.stop_sequence <= max_stop_seq)
        to_chart.stop_name = to_chart.stop_name.str.split('&').map(lambda x: x[-1])
        to_chart = to_chart.rename(columns={'speed_mph': 'Segement Speed (mph)',
                                      'delay_chg_sec': 'Increase in Delay (seconds)',
                                      'stop_sequence': 'Stop Segment ID',
                                      'stop_name': 'Segment Cross Street'})
        plt.xticks(rotation=65)
        title = f"{self.organization_name} Speed Variability by Stop Segment{self.filter_formatted}"
        if no_title:
            title = None
        variability_plt = sns.swarmplot(x = to_chart['Segment Cross Street'], y=to_chart['Segement Speed (mph)'],
              palette=shared_utils.calitp_color_palette.CALITP_CATEGORY_BRIGHT_COLORS,
             ).set_title(title)
        return variability_plt
    
    def describe_slow_routes(self):
        try:
            slowest = (self._filter(self.rt_trips)
                >> group_by(_.shape_id)
                >> summarize(num_trips = _.shape_id.count(), median_trip_mph = _.mean_speed_mph.median())
                >> arrange(_.median_trip_mph)
                >> inner_join(_, self._filter(self.rt_trips) >> distinct(_.shape_id, _.route_id, _.route_short_name,
                            _.route_long_name, _.route_desc, _.direction), on = 'shape_id')
                >> head(7)
            )
            slowest = slowest.apply(describe_slowest, axis=1)
            display_list = slowest.full_description.to_list()
            with_newlines = '\n * ' + '\n * '.join(display_list)
            period_formatted = self.filter_period.replace('_', ' ')
            # display(slowest)
            display(Markdown(f'{period_formatted} slowest routes: {with_newlines}'))
        except:
            # print('describe slow routes failed!')
            pass
        return
    
    def quick_map_corridor(self):
        '''
        Maps currently attached corridor, with stops just before/
        in/just after corridor for all routes in current filter, if any
        '''
        
        assert hasattr(self, 'corridor'), 'Must attach a corridor first'
        filtered_stops = self._filter(self.stop_delay_view)
        mappable_stops = (filtered_stops.dropna(subset=['stop_id'])
                  >> distinct(_.shape_id, _.stop_sequence, _keep_all=True)
                  >> filter(_.corridor)
                  >> select(_.stop_id, _.geometry, _.stop_sequence, _.shape_id)
                 )
        gdf = (self.corridor >> select(_.geometry, _.total_speed_delay, _.total_schedule_delay)).iloc[:1,:]
        m = pd.concat([mappable_stops, gdf]).explore(tiles = "CartoDB positron")
        return m
    
    def runtime_metrics(self):
        '''
        Measure observed runtimes for routes within filter. Based on full data extent of each trip,
        currently bidirectional.
        
        TODO document rest!
        '''
        df = (self._filter(self.stop_delay_view)
             >> filter(_.actual_time > dt.datetime.combine(rt_day.analysis_date, dt.time(6)),
                       _.actual_time < dt.datetime.combine(rt_day.analysis_date, dt.time(21))
                      )
             >> group_by(_.shape_id, _.route_id, _.route_short_name, _.trip_id)
             >> summarize(runtime = _.actual_time.max() - _.actual_time.min(),
                         start = _.actual_time.min())
             >> group_by(_.route_id, _.route_short_name)
             >> summarize(p50_runtime_minutes =_.runtime.quantile(.5).seconds / 60,
                          p20_runtime_minutes =_.runtime.quantile(.2).seconds / 60,
                          p80_runtime_minutes =_.runtime.quantile(.8).seconds / 60,
                         first_start = _.start.min(), last_start = _.start.max(),
                         n_trips = _.shape[0])
             >> mutate(span = _.last_start - _.first_start, span_minutes = _.span.map(lambda x: x.seconds) / 60,
                      daily_avg_headway = (_.span_minutes / _.n_trips) * 2) # create avg per direction by multiplying 2x
        ).round(1)
        return df
    def corridor_metrics(self):
        '''
        Generate schedule based and speed based metrics for SCCP/LPP programs.
        
        Default assumption is that
        
        Note that trips_added assumes that service hour savings based on corridor speed improvements
        are reinvested into additonal runs of the *entire* route, at the existing median runtime.
        '''
        assert hasattr(self, 'corridor'), 'must add corridor before generating corridor metrics'
        assert not self._filter(self.corridor_stop_delays).empty, 'filter does not include any corridor trips'
        if self.filter:
            print('warning: filter set -- for SCCP/LPP reset filter first')
            
        # median delay within segment for each trip, then aggregate to route
        schedule_metric_df = (self._filter(self.corridor_stop_delays)
             >> group_by(_.trip_id, _.route_id, _.route_short_name)
             >> summarize(median_corridor_delay_seconds = _.corridor_delay_seconds.median())
             >> group_by(_.route_id, _.route_short_name)
             >> summarize(schedule_delay_minutes = _.median_corridor_delay_seconds.sum() / 60)
            )
        
        self.stop_delay_view = self.stop_delay_view >> group_by(_.trip_id) >> arrange(_.stop_sequence) >> ungroup()
        self.corridor_trip_speeds = (self._filter(self.stop_delay_view)
             >> filter(_.corridor)
             >> group_by(_.trip_id)
             >> mutate(entry_time = _.actual_time.min())
             >> mutate(exit_time = _.actual_time.max())
             >> mutate(entry_loc = _.shape_meters.min())
             >> mutate(exit_loc = _.shape_meters.max())
             >> mutate(meters_from_entry = (_.exit_loc - _.entry_loc))
             >> mutate(seconds_from_entry = (_.exit_time - _.entry_time).apply(lambda x: x.seconds))
             >> mutate(speed_from_entry = _.meters_from_entry / _.seconds_from_entry)
             >> ungroup()
             >> distinct(_.trip_id, _keep_all=True)
            )
        target_speed_mps = self.transit_priority_target_mph / MPH_PER_MPS
        
        speed_int_df = (self.corridor_trip_speeds
              >> mutate(corridor_speed_mph = _.speed_from_entry * MPH_PER_MPS)
              >> mutate(target_seconds = _.meters_from_entry / target_speed_mps)
              >> mutate(target_delay_seconds = _.seconds_from_entry - _.target_seconds)
             )
        speed_metric_df = (speed_int_df
              >> mutate(organization = self.organization_name)
              >> group_by(_.route_id, _.route_short_name, _.organization)
              >> summarize(p20_corr_mph = _.corridor_speed_mph.quantile(.2),
                           speed_delay_minutes = _.target_delay_seconds.sum() / 60)
             )
        both_metrics_df = (speed_metric_df >> inner_join(_, schedule_metric_df, on = ['route_id', 'route_short_name'])
                              >> mutate(total_speed_delay = _.speed_delay_minutes.sum(),
                                       total_schedule_delay = _.schedule_delay_minutes.sum())
                          )
        runtimes = self.runtime_metrics() >> select(_.route_id, _.p50_runtime_minutes)
        both_metrics_df = (both_metrics_df >> inner_join(_, runtimes, on = 'route_id')
                              >> mutate(trips_added = _.speed_delay_minutes / _.p50_runtime_minutes)
                          )
        gdf = gpd.GeoDataFrame(both_metrics_df, geometry=self.corridor.geometry, crs=self.corridor.crs)
        # ffill kinda broken in geopandas?
        gdf.geometry = gdf.geometry.fillna(value=gdf.geometry.iloc[0])
        self.corridor = gdf.round(1)
        print('metrics attached to self.corridor: ')
        return self.corridor
    
def from_gcs(itp_id, analysis_date, pbar = None):
    ''' Generates RtFilterMapper from cached artifacts in GCS. Generate using rt_parser.OperatorDayAnalysis.export_views_gcs()
    '''
    date_iso = analysis_date.isoformat()
    
    if analysis_date <= warehouse_cutoff_date:
        shapes = get_routelines(itp_id, analysis_date)
        trips = (pd.read_parquet(f'{GCS_FILE_PATH}rt_trips/{itp_id}_{date_iso}.parquet')
            .reset_index(drop=True))
        stop_delay = (gpd.read_parquet(f'{GCS_FILE_PATH}stop_delay_views/{itp_id}_{date_iso}.parquet')
                 .reset_index(drop=True))
    else:
        index_df = get_speedmaps_ix_df(analysis_date = analysis_date, itp_id = itp_id)
        trips = (pd.read_parquet(f'{GCS_FILE_PATH}v2_rt_trips/{itp_id}_{date_iso}.parquet')
            .reset_index(drop=True))
        stop_delay = (gpd.read_parquet(f'{GCS_FILE_PATH}v2_stop_delay_views/{itp_id}_{date_iso}.parquet')
                 .reset_index(drop=True))
        shapes = get_shapes(index_df)
    
    stop_delay['arrival_time'] = stop_delay.arrival_time.map(lambda x: np.datetime64(x))
    stop_delay['actual_time'] = stop_delay.actual_time.map(lambda x: np.datetime64(x))
    
    rt_day = RtFilterMapper(trips, stop_delay, shapes, pbar)
    return rt_day