{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8eb5db7-eb2d-4846-8a6e-dbd9e9ab0a6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CALITP_BQ_MAX_BYTES\"] = str(1_000_000_000_000) ## 1TB?\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import geopandas as gpd\n",
    "from siuba import *\n",
    "import json\n",
    "\n",
    "import shared_utils\n",
    "import warnings\n",
    "from path_example_vars import GCS_PATH\n",
    "\n",
    "CONVEYAL_GCS_PATH = 'gs://calitp-analytics-data/data-analyses/conveyal_update/'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b97c5ba-55c7-41b0-a6c4-a05dc81234bd",
   "metadata": {},
   "source": [
    "# Conveyal Transit Paths\n",
    "\n",
    "* GH Issue: https://github.com/cal-itp/data-analyses/issues/1098\n",
    "\n",
    "## Conveyal SOP\n",
    "\n",
    "* prepare a csv with lat, lon, and od column with 0 for origin and 1 for destination\n",
    "    * allow freeform, use od col as id in upload\n",
    "* run Conveyal Analysis: 8-10am, standard transit parameters, add JSON feed_id param\n",
    "* run Regional Analysis: 120min max time, 5, 50, 95 %ile, get paths and travel times\n",
    "\n",
    "## Metrics\n",
    "\n",
    "* modal trip by n_iterations (most available trip) (how fast is it, how many xfers, fares?)\n",
    "* fastest trip (how available?, fares?, xfer count?)\n",
    "* fewest xfer trip (how fast, how available, fares?)\n",
    "\n",
    "## Visuals??"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dbd5682-18f7-4336-82c1-d5f15e9baa7d",
   "metadata": {},
   "source": [
    "# Basic Paths Parse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0ed37e1-8c3b-4370-827b-182baa0b376c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_conveyal_path_df(path):\n",
    "\n",
    "    array_cols = ['routes', 'boardStops', 'alightStops',\n",
    "           'rideTimes', 'waitTimes', 'feedIds']\n",
    "\n",
    "    def unpack_conveyal_path_df(df, array_cols = array_cols):\n",
    "\n",
    "        for col in array_cols:\n",
    "            df.loc[:,col] = df[col].map(lambda x: x.split('|'))\n",
    "        return df\n",
    "\n",
    "    df = pd.read_csv(path)\n",
    "    df.index.rename('trip_group_id', inplace=True)\n",
    "    df.reset_index(inplace=True)\n",
    "    df['total_iterations'] = (df >> filter(_.origin == 0, _.destination == 0)).nIterations.iloc[0]\n",
    "    df = (df >> filter(_.origin == 0, _.destination == 1)\n",
    "             >> select(-_.group)\n",
    "         )\n",
    "    df = unpack_conveyal_path_df(df)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d14e526-e60f-4c81-8dd7-0a04d3e20cf1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df = read_conveyal_path_df('./la/6643b1ec46ade8368e2cb698_PATHS.csv')\n",
    "df >> head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fe3b537-f981-48b3-b53c-019d8a317926",
   "metadata": {},
   "source": [
    "# Conveyal bundle-feed matching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfffd61b-35cf-49a1-8ba5-e24e545acbff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_warehouse_identifiers(conveyal_df):\n",
    "    \n",
    "    warehouse_conveyal_joined = pd.read_parquet(f'{CONVEYAL_GCS_PATH}warehouse_conveyal_simple_2023-10-18')\n",
    "    analysis_date = warehouse_conveyal_joined.date.iloc[0].date()\n",
    "\n",
    "    # all example feeds present?\n",
    "    unique_feeds = conveyal_df.feedIds.explode().unique()\n",
    "    assert np.isin(unique_feeds, warehouse_conveyal_joined.feedId).all()\n",
    "\n",
    "    warehouse_conveyal_joined = warehouse_conveyal_joined >> select(_.feedId, _.feed_key, _.gtfs_dataset_name, _.base64_url, _.date)\n",
    "\n",
    "    as_dict = warehouse_conveyal_joined.set_index('feedId').to_dict()\n",
    "\n",
    "    conveyal_df['feed_keys'] = conveyal_df.feedIds.apply(lambda x: [as_dict['feed_key'][item] for item in x])\n",
    "    conveyal_df['gtfs_dataset_names'] = conveyal_df.feedIds.apply(lambda x: [as_dict['gtfs_dataset_name'][item] for item in x])\n",
    "    conveyal_df['date'] = analysis_date\n",
    "    \n",
    "    return conveyal_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5015d1f-e653-4833-ad39-77b2dbb36b89",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = add_warehouse_identifiers(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddddadf0-fe46-4528-ac67-5724214ade1d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# with pd.option_context('display.max_colwidth', 100):\n",
    "#     display(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29da9c69-133b-46a8-9aaa-cf4772c31337",
   "metadata": {},
   "outputs": [],
   "source": [
    "# works, but is it useful?\n",
    "# df_exploded = df.explode(array_cols).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e816ea93-3407-4a42-94f7-a413b5d3bc55",
   "metadata": {},
   "source": [
    "## Metric Development"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9f43ea4-1833-4cbb-9db5-fc2b559986fb",
   "metadata": {},
   "source": [
    "### Get warehouse data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5252884-df74-4247-a6b9-ec8faffc3cdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from shapely.ops import split, substring, LineString\n",
    "from calitp_data_analysis import geography_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4c009b5-0771-4bbd-9be9-f2e582095a3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_warehouse_data(path_df):\n",
    "    '''\n",
    "    get relevant data from warehouse for all trips in Conveyal path output\n",
    "    '''\n",
    "    analysis_date = path_df.date.iloc[0]\n",
    "    all_feed_keys = list(df.feed_keys.explode().unique())\n",
    "    all_route_ids = list(df.routes.explode().unique())\n",
    "    all_stops = list(df.boardStops.explode().unique()) + list(df.alightStops.explode().unique())\n",
    "\n",
    "    warehouse_data = {}\n",
    "    warehouse_data['shapes'] = shared_utils.gtfs_utils_v2.get_shapes(selected_date=analysis_date, operator_feeds=all_feed_keys,\n",
    "                                                      shape_cols = ['feed_key', 'shape_id'])\n",
    "    warehouse_data['shapes'] = warehouse_data['shapes'].to_crs(geography_utils.CA_NAD83Albers)\n",
    "    warehouse_data['trips'] = shared_utils.gtfs_utils_v2.get_trips(selected_date=analysis_date,\n",
    "                                                                   operator_feeds=all_feed_keys,\n",
    "                                                                   trip_cols = ['feed_key', 'trip_id', 'route_id',\n",
    "                                                                                'shape_id', 'trip_first_departure_ts']\n",
    "                                                                  )\n",
    "    warehouse_data['trips'] = warehouse_data['trips'] >> filter(_.route_id.isin(all_route_ids))\n",
    "    warehouse_data['st'] = shared_utils.gtfs_utils_v2.get_stop_times(selected_date=analysis_date, operator_feeds=all_feed_keys, trip_df=warehouse_data['trips'])\n",
    "    warehouse_data['st'] = warehouse_data['st'] >> filter(_.stop_id.isin(all_stops)) >> collect()\n",
    "    warehouse_data['stops'] = shared_utils.gtfs_utils_v2.get_stops(selected_date=analysis_date, operator_feeds=all_feed_keys, custom_filtering={'stop_id': all_stops})\n",
    "    warehouse_data['stops'] = warehouse_data['stops'].to_crs(geography_utils.CA_NAD83Albers)\n",
    "    \n",
    "    return warehouse_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d602732-84db-40a9-9e80-1e98b78244b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "warehouse_data = get_warehouse_data(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c9e4fff-45f1-4849-95c9-fe14c1f29b80",
   "metadata": {},
   "source": [
    "### Map?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b266e461-11cd-421c-a955-6fb51c387bd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def shape_segments_from_row(row, warehouse_data):\n",
    "\n",
    "    stop_pairs = list(zip(row.boardStops, row.alightStops))\n",
    "    \n",
    "    row_shape_segments = []\n",
    "    for stop_pair in stop_pairs:\n",
    "        # print(stop_pair)\n",
    "        first_filter = warehouse_data['st'] >> filter(_.stop_id.isin(stop_pair))\n",
    "        # display(first_filter)\n",
    "        good_trips = first_filter >> count(_.trip_id) >> filter(_.n > 1)\n",
    "        assert good_trips.shape[0] > 0\n",
    "        trip_with_pair = first_filter >> filter(_.trip_id == good_trips.trip_id.iloc[0]) >> arrange(_.stop_sequence)\n",
    "        trip_with_pair = trip_with_pair >> select(_.feed_key, _.trip_id, _.stop_id, _.stop_sequence)\n",
    "        trip_with_pair = trip_with_pair >> inner_join(_, warehouse_data['stops'] >> select(_.feed_key, _.stop_id, _.geometry),\n",
    "                                                      on = ['feed_key', 'stop_id'])\n",
    "        trip_with_pair = trip_with_pair >> inner_join(_, warehouse_data['trips'] >> select(_.feed_key, _.trip_id, _.shape_id),\n",
    "                                                      on = ['feed_key', 'trip_id'])\n",
    "        paired_shape = warehouse_data['shapes'] >> filter(_.feed_key == trip_with_pair.feed_key.iloc[0], _.shape_id == trip_with_pair.shape_id.iloc[0])\n",
    "            \n",
    "        if not trip_with_pair.stop_id.is_unique:\n",
    "            print('warning, trip has duplicate stops at a single stop')\n",
    "            trip_with_pair = trip_with_pair >> distinct(_.stop_id, _keep_all=True)\n",
    "        stop0 =  (trip_with_pair >> filter(_.stop_sequence == _.stop_sequence.min())).geometry.iloc[0]\n",
    "        stop1 =  (trip_with_pair >> filter(_.stop_sequence == _.stop_sequence.max())).geometry.iloc[0]\n",
    "        if paired_shape.empty:\n",
    "            print('warning, trip has no shape')\n",
    "            trip_with_pair = trip_with_pair >> distinct(_.stop_id, _keep_all=True)\n",
    "            paired_segment = LineString([stop0, stop1])\n",
    "        # stop0_proj = shape_geom.project(stop0)\n",
    "        # stop1_proj = shape_geom.project(stop1)\n",
    "        else:\n",
    "            shape_geom = paired_shape.geometry.iloc[0]\n",
    "            stops_proj = [shape_geom.project(stop0), shape_geom.project(stop1)] #  be resillient to looping\n",
    "            paired_segment = substring(shape_geom, min(stops_proj), max(stops_proj))\n",
    "        \n",
    "        trip_with_pair['segment_geom'] = paired_segment\n",
    "        trip_with_pair.set_geometry('segment_geom')\n",
    "        trip_with_pair = trip_with_pair >> rename(stop_geom = _.geometry)\n",
    "        # display(stop_pair)\n",
    "        trip_with_pair = trip_with_pair >> distinct(_.shape_id, _keep_all=True)\n",
    "        trip_with_pair['stop_pair'] = [stop_pair]\n",
    "        trip_with_pair['trip_group_id'] = row.trip_group_id\n",
    "        trip_with_pair['nIterations'] = row.nIterations\n",
    "        trip_with_pair['totalTime'] = row.totalTime\n",
    "        trip_with_pair['xfer_count'] = len(stop_pairs) - 1\n",
    "        trip_with_seg = gpd.GeoDataFrame(trip_with_pair, geometry='segment_geom', crs=geography_utils.CA_NAD83Albers)\n",
    "        row_shape_segments += [trip_with_seg]\n",
    "    return pd.concat(row_shape_segments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1133763f-8c1c-4990-9def-19ce8250e572",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compile_all_spatial_routes(df):\n",
    "    spatial_routes = []\n",
    "    for _ix, row in df.iterrows():\n",
    "        try:\n",
    "            spatial_routes += [shape_segments_from_row(row, warehouse_data)]\n",
    "        except:\n",
    "            print(f'failed for row {row}')\n",
    "    spatial_routes = pd.concat(spatial_routes)\n",
    "    return spatial_routes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf581e00-2b1e-4979-8730-b4d426dc90a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "with warnings.catch_warnings():\n",
    "    warnings.simplefilter(\"ignore\")\n",
    "    spatial_routes = compile_all_spatial_routes(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b8cc561-2f4e-4051-a0ae-5489b3ed5423",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "spatial_routes >> head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9d4f1a0-7f08-4b8f-9870-176152cf6d1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "map_routes = spatial_routes.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66844985-fe10-407b-87a7-1493e10e725a",
   "metadata": {},
   "outputs": [],
   "source": [
    "map_routes.segment_geom = map_routes.apply(lambda x: x.segment_geom.buffer(x.nIterations * 2), axis=1)\n",
    "map_routes = map_routes >> arrange(-_.nIterations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b548ce8-0fb3-4bff-9402-2b4a19104e92",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "map_routes.explore(column = 'trip_group_id', cmap='tab20', tiles=\"CartoDB positron\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "804f65d7-3a1c-4226-a708-a0498aa6ae07",
   "metadata": {},
   "outputs": [],
   "source": [
    "# map_routes.explore(column = 'shape_id', cmap='Accent', tiles=\"CartoDB positron\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d63c10bb-a5cd-4f9b-aaf6-67d29514fd25",
   "metadata": {},
   "outputs": [],
   "source": [
    "map_routes.explore(column = 'xfer_count', cmap='coolwarm', tiles=\"CartoDB positron\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6845fb90-0b40-4344-9c25-f6da646cd808",
   "metadata": {},
   "outputs": [],
   "source": [
    "map_routes['car_p50_ratio'] = map_routes['totalTime'] / 46"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3938cea-8a24-40e6-903c-7038e1e82be8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# map_routes.explore(column = 'totalTime', cmap='coolwarm', tiles=\"CartoDB positron\")\n",
    "#  TODO fixed scale @1.5, 2, 2.5, 3\n",
    "map_routes.explore(column = 'car_p50_ratio', cmap='coolwarm', tiles=\"CartoDB positron\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7f020c9-682a-4d91-89d5-4cbc44ecf9e1",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Storytelling - infrequent regional rail\n",
    "\n",
    "* only two usable trips for 8-10am departures from origin, at 10:41 (Metrolink) and 11:01 (Amtrak)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54d76186-9091-4843-949d-9b205fc2eea4",
   "metadata": {},
   "outputs": [],
   "source": [
    "(warehouse_data['trips'] >> filter(_.route_id == 'Antelope Valley Line') >> arrange(_.trip_first_departure_ts))[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eacd81d8-edfd-4c19-9bcc-bdf581d52a81",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "(warehouse_data['trips'] >> filter(_.route_id == '78') >> arrange(_.trip_first_departure_ts))[:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30f2a7e2-a534-45da-b978-be7d6ba5b5fd",
   "metadata": {},
   "source": [
    "## full shape map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "244afbca-6a47-46a9-a603-f21bf355e5da",
   "metadata": {},
   "outputs": [],
   "source": [
    "shape_grouped = spatial_routes >> group_by(_.shape_id, _.segment_geom) >> summarize(total_iterations = _.nIterations.sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3de332b3-139d-4d41-a918-a099884a93f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "shape_grouped.segment_geom = shape_grouped.apply(lambda x: x.segment_geom.buffer(x.total_iterations * 2), axis=1)\n",
    "shape_grouped = shape_grouped >> arrange(-_.total_iterations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57697a91-f816-4651-b9f4-698837ba86c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "shape_grouped = gpd.GeoDataFrame(shape_grouped, geometry='segment_geom', crs=geography_utils.CA_NAD83Albers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34ca4df1-36ce-4e74-994c-819f25e5e63d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "shape_grouped.explore(column = 'shape_id', cmap='Accent', tiles=\"CartoDB positron\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
